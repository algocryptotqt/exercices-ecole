<thinking>
## Analyse du Concept
- Concept : Multi-Paxos Implementation
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - Multi-Paxos est l'optimisation industrielle de Paxos, utilise par Google Chubby et Spanner.

## Combo Base + Bonus
- Exercice de base : Implementation de Multi-Paxos avec leader stable et optimisation de phase
- Bonus : Implementation du log replication avec slots et reconfiguration dynamique
- Palier bonus : Avance (complexite protocole + gestion d'etat)
- Progression logique ? OUI - Base = protocole core, Bonus = features avancees

## Prerequis & Difficulte
- Prerequis reels : Paxos basique, async Rust, state machines
- Difficulte estimee : 8/10 (base), 10/10 (bonus)
- Coherent avec phase 5 ? OUI

## Aspect Fun/Culture
- Contexte choisi : Reference a "The Paxos Parliament" de Leslie Lamport
- MEME mnemonique : "One Leader to Rule Them All" (parodie LOTR)
- Pourquoi c'est fun : Multi-Paxos elimine les re-elections comme un roi eternel

## Scenarios d'Echec (5 mutants concrets)
1. Mutant A (Boundary) : Ballot number overflow non gere
2. Mutant B (Safety) : Leader skip la phase prepare apres election invalide
3. Mutant C (Logic) : Slots non contigus acceptes dans le log
4. Mutant D (Edge) : Reconfiguration pendant instance en cours
5. Mutant E (Return) : Accept retourne success sans majority

## Verdict
VALIDE - Exercice de qualite industrielle couvrant Multi-Paxos complet
</thinking>

# Exercice 5.4.21-a : multi_paxos

**Module :**
5.4.21 - Multi-Paxos Consensus Protocol

**Concept :**
a - Multi-Paxos Implementation with Leader Optimization

**Difficulte :**
8/10

**Type :**
code

**Tiers :**
3 - Integration systeme

**Langage :**
Rust Edition 2024

**Prerequis :**
- 5.4.7 - Basic Paxos (prepare, accept, learn)
- 5.4.8 - Leader Election Fundamentals
- 2.5 - Async Rust (tokio, futures)
- 2.4 - Error Handling (Result, custom errors)

**Domaines :**
Distributed, Consensus, StateMachine

**Duree estimee :**
180 min

**XP Base :**
300

**Complexite :**
T3 O(n) x S2 O(n)

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :**
```
src/lib.rs
```

**Dependances autorisees :**
- `tokio` (async runtime)
- `serde` / `serde_json` (serialization)
- `thiserror` (error handling)

**Fonctions/methodes interdites :**
- Crates de consensus externes (`raft`, `paxos-rs`)
- `unsafe` blocks
- Blocking I/O dans async context

### 1.2 Consigne

**CONTEXTE : "The Paxos Parliament Reborn"**

*"In the ancient Greek island of Paxos, legislators discovered that reaching agreement was hard. But they also discovered something else: once you have a stable leader, you can skip the politicking. Multi-Paxos is Paxos with tenure."* - Leslie Lamport, probablement

Multi-Paxos optimise le protocole Paxos original en eliminant la phase Prepare pour les instances consecutives quand un leader stable est etabli. C'est le protocole utilise par Google Chubby, Spanner, et de nombreux systemes distribues critiques.

**Ta mission :**

Implementer un moteur Multi-Paxos complet avec :
1. Election de leader via phase Prepare
2. Optimisation des instances suivantes (skip Prepare)
3. Log replique avec slots indexes
4. Gestion des gaps et catch-up
5. Detection de leader failure et re-election

**Entree :**
- Configuration du cluster (node IDs, quorum size)
- Operations client a repliquer

**Sortie :**
- Log replique coherent sur tous les noeuds
- Garantie de linearisabilite

**Contraintes :**
- Ballot numbers monotoniquement croissants
- Quorum = majority (n/2 + 1)
- Leader lease avec timeout
- Pas de gaps dans le log committed

**Exemples :**

| Scenario | Resultat | Explication |
|----------|----------|-------------|
| Leader stable, 3 ops | 3 instances en O(2) RTT | Skip Prepare apres election |
| Leader crash, re-election | Nouveau leader, continue log | Ballot increment, Prepare phase |
| Network partition healed | Catch-up des slots manquants | Gap detection et fill |

### 1.2.2 Consigne Academique

Implementer le protocole Multi-Paxos avec optimisation de leader stable. Le systeme doit garantir la safety (agreement, validity, integrity) et la liveness (termination) sous hypothese de synchronie partielle.

### 1.3 Prototype

```rust
use std::collections::{HashMap, BTreeMap};
use tokio::sync::{mpsc, RwLock};
use std::sync::Arc;

pub type NodeId = u64;
pub type Ballot = (u64, NodeId);  // (number, node_id) for uniqueness
pub type Slot = u64;

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum MultiPaxosError {
    NotLeader,
    NoQuorum,
    BallotTooOld(Ballot),
    SlotAlreadyDecided(Slot),
    Timeout,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum NodeRole {
    Follower,
    Candidate,
    Leader { lease_until: u64 },
}

#[derive(Debug, Clone)]
pub struct PaxosInstance {
    pub slot: Slot,
    pub ballot: Ballot,
    pub value: Option<Vec<u8>>,
    pub state: InstanceState,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum InstanceState {
    Empty,
    Prepared,
    Accepted,
    Decided,
}

#[derive(Debug, Clone)]
pub enum PaxosMessage {
    // Phase 1: Leader election / instance preparation
    Prepare { ballot: Ballot, slot: Slot },
    Promise {
        ballot: Ballot,
        slot: Slot,
        accepted_ballot: Option<Ballot>,
        accepted_value: Option<Vec<u8>>,
    },

    // Phase 2: Value proposal
    Accept { ballot: Ballot, slot: Slot, value: Vec<u8> },
    Accepted { ballot: Ballot, slot: Slot },

    // Learning
    Decide { slot: Slot, value: Vec<u8> },

    // Leader optimization
    LeaderHeartbeat { ballot: Ballot, commit_index: Slot },
    LeaderAck { node_id: NodeId, last_slot: Slot },
}

pub struct MultiPaxosNode {
    id: NodeId,
    role: NodeRole,
    current_ballot: Ballot,
    promised_ballot: Ballot,
    instances: BTreeMap<Slot, PaxosInstance>,
    commit_index: Slot,
    cluster: Vec<NodeId>,
}

impl MultiPaxosNode {
    pub fn new(id: NodeId, cluster: Vec<NodeId>) -> Self;

    /// Attempt to become leader (Phase 1 for all pending slots)
    pub async fn become_leader(&mut self) -> Result<Ballot, MultiPaxosError>;

    /// Propose a value (as leader, skips Prepare if already leader)
    pub async fn propose(&mut self, value: Vec<u8>) -> Result<Slot, MultiPaxosError>;

    /// Handle incoming Prepare message
    pub fn handle_prepare(&mut self, from: NodeId, msg: &PaxosMessage)
        -> Option<PaxosMessage>;

    /// Handle incoming Promise message
    pub fn handle_promise(&mut self, from: NodeId, msg: &PaxosMessage)
        -> Option<PaxosMessage>;

    /// Handle incoming Accept message
    pub fn handle_accept(&mut self, from: NodeId, msg: &PaxosMessage)
        -> Option<PaxosMessage>;

    /// Handle incoming Accepted message
    pub fn handle_accepted(&mut self, from: NodeId, msg: &PaxosMessage)
        -> Option<PaxosMessage>;

    /// Handle Decide message (commit the value)
    pub fn handle_decide(&mut self, msg: &PaxosMessage);

    /// Get decided value for a slot
    pub fn get_decided(&self, slot: Slot) -> Option<&Vec<u8>>;

    /// Get all decided values in order
    pub fn get_log(&self) -> Vec<(Slot, Vec<u8>)>;

    /// Check if we are the current leader
    pub fn is_leader(&self) -> bool;

    /// Get next available slot for proposal
    pub fn next_slot(&self) -> Slot;

    /// Quorum size
    pub fn quorum_size(&self) -> usize;
}

/// Multi-Paxos coordinator managing multiple nodes
pub struct MultiPaxosCluster {
    nodes: HashMap<NodeId, Arc<RwLock<MultiPaxosNode>>>,
}

impl MultiPaxosCluster {
    pub fn new(node_ids: Vec<NodeId>) -> Self;
    pub async fn elect_leader(&self) -> Result<NodeId, MultiPaxosError>;
    pub async fn replicate(&self, value: Vec<u8>) -> Result<Slot, MultiPaxosError>;
    pub async fn read(&self, slot: Slot) -> Option<Vec<u8>>;
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 Origine de Multi-Paxos

Multi-Paxos n'est pas un protocole distinct mais une optimisation de Paxos decrite par Lamport dans "Paxos Made Simple" (2001). L'idee cle : si un leader est stable, il peut "pre-prepare" tous les slots futurs avec un seul Phase 1, puis utiliser uniquement Phase 2 pour chaque nouvelle valeur.

### 2.2 L'Optimisation du Leader Stable

Dans Paxos basique, chaque instance necessite 2 RTT (Round-Trip Times) :
- Phase 1: Prepare/Promise
- Phase 2: Accept/Accepted

Avec Multi-Paxos et leader stable :
- Premiere instance : 2 RTT (avec election)
- Instances suivantes : 1 RTT (Phase 2 seulement)

### 2.3 Utilisations Industrielles

| Systeme | Utilisation Multi-Paxos |
|---------|------------------------|
| Google Chubby | Lock service distribue |
| Google Spanner | Transactions globales |
| Apache Zookeeper | Coordination (variante ZAB) |
| CockroachDB | Replication consensus |

---

## SECTION 2.5 : DANS LA VRAIE VIE

### Metiers concernes

| Metier | Utilisation Multi-Paxos |
|--------|------------------------|
| **Distributed Systems Engineer** | Implementation consensus pour datastores |
| **Database Architect** | Replication synchrone multi-region |
| **Cloud Platform Engineer** | Coordination services critiques |
| **Site Reliability Engineer** | Comprendre failures et recovery |

### Cas d'usage concrets

1. **Google Spanner** : Multi-Paxos pour chaque shard, TrueTime pour ordering global
2. **Configuration Management** : Leader election pour single-writer pattern
3. **Distributed Locks** : Consensus sur ownership avec lease

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ cargo test
   Compiling multi_paxos v0.1.0
    Finished test [unoptimized + debuginfo] target(s)
     Running unittests src/lib.rs

running 15 tests
test tests::test_ballot_ordering ... ok
test tests::test_leader_election ... ok
test tests::test_single_instance_consensus ... ok
test tests::test_multi_instance_optimization ... ok
test tests::test_leader_failure_recovery ... ok
test tests::test_concurrent_proposals ... ok
test tests::test_quorum_calculation ... ok
test tests::test_slot_gap_handling ... ok
test tests::test_promise_with_accepted_value ... ok
test tests::test_ballot_rejection ... ok
test tests::test_log_ordering ... ok
test tests::test_catch_up ... ok
test tests::test_network_partition_safety ... ok
test tests::test_linearizability ... ok
test tests::test_idempotent_decide ... ok

test result: ok. 15 passed; 0 failed
```

### 3.1 BONUS AVANCE (OPTIONNEL)

**Difficulte Bonus :**
10/10

**Recompense :**
XP x3

**Time Complexity attendue :**
O(n * m) ou n = nodes, m = slots

**Space Complexity attendue :**
O(m) pour le log

**Domaines Bonus :**
`Consensus, FaultTolerance`

#### 3.1.1 Consigne Bonus

**"The Eternal Parliament"**

*"Un parlement qui peut changer ses membres tout en continuant a legiferer - c'est la reconfiguration dynamique."*

**Ta mission bonus :**

Implementer la reconfiguration dynamique du cluster Multi-Paxos :
1. Ajout de nouveaux noeuds sans arret
2. Retrait de noeuds defaillants
3. Joint consensus pour transitions safe

#### 3.1.2 Prototype Bonus

```rust
#[derive(Debug, Clone)]
pub struct ClusterConfig {
    pub members: Vec<NodeId>,
    pub epoch: u64,
}

#[derive(Debug, Clone)]
pub enum ReconfigOp {
    AddNode(NodeId),
    RemoveNode(NodeId),
    ReplaceNode { old: NodeId, new: NodeId },
}

impl MultiPaxosNode {
    /// Start reconfiguration (joint consensus)
    pub async fn start_reconfig(&mut self, op: ReconfigOp)
        -> Result<ClusterConfig, MultiPaxosError>;

    /// Commit new configuration
    pub async fn commit_reconfig(&mut self) -> Result<(), MultiPaxosError>;

    /// Abort reconfiguration
    pub fn abort_reconfig(&mut self);
}
```

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette - Tableau des tests

| Test | Input | Expected | Points | Categorie |
|------|-------|----------|--------|-----------|
| `ballot_ordering` | `(1,1) vs (1,2)` | `(1,2) > (1,1)` | 5 | Basic |
| `leader_election` | 3 nodes | One leader elected | 10 | Core |
| `single_instance` | Propose "A" | Slot 0 = "A" | 10 | Core |
| `multi_instance_opt` | 3 proposals | 1 RTT each after first | 15 | Core |
| `leader_failure` | Kill leader | New leader elected | 15 | Edge |
| `quorum_calc` | 5 nodes | Quorum = 3 | 5 | Basic |
| `ballot_rejection` | Old ballot | `BallotTooOld` error | 10 | Edge |
| `slot_gap` | Slots 0,2 decided | Gap detected | 10 | Edge |
| `promise_adopted` | Promise with value | Adopt previous value | 10 | Core |
| `linearizability` | Concurrent reads | Consistent order | 10 | Core |

**Score minimum pour validation : 70/100**

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_ballot_ordering() {
        let b1: Ballot = (1, 1);
        let b2: Ballot = (1, 2);
        let b3: Ballot = (2, 1);

        assert!(b2 > b1);  // Same number, higher node_id wins
        assert!(b3 > b2);  // Higher number always wins
    }

    #[test]
    fn test_quorum_size() {
        let node = MultiPaxosNode::new(1, vec![1, 2, 3]);
        assert_eq!(node.quorum_size(), 2);  // 3/2 + 1 = 2

        let node5 = MultiPaxosNode::new(1, vec![1, 2, 3, 4, 5]);
        assert_eq!(node5.quorum_size(), 3);  // 5/2 + 1 = 3
    }

    #[tokio::test]
    async fn test_leader_election() {
        let mut node = MultiPaxosNode::new(1, vec![1, 2, 3]);
        let ballot = node.become_leader().await.unwrap();

        assert!(node.is_leader());
        assert_eq!(ballot.1, 1);  // Our node ID
    }

    #[test]
    fn test_handle_prepare() {
        let mut node = MultiPaxosNode::new(2, vec![1, 2, 3]);

        let prepare = PaxosMessage::Prepare {
            ballot: (1, 1),
            slot: 0
        };

        let response = node.handle_prepare(1, &prepare);

        assert!(matches!(response, Some(PaxosMessage::Promise { .. })));
    }

    #[test]
    fn test_reject_old_ballot() {
        let mut node = MultiPaxosNode::new(2, vec![1, 2, 3]);

        // Accept higher ballot first
        let prepare_high = PaxosMessage::Prepare {
            ballot: (5, 1),
            slot: 0
        };
        node.handle_prepare(1, &prepare_high);

        // Try lower ballot - should be rejected
        let prepare_low = PaxosMessage::Prepare {
            ballot: (3, 1),
            slot: 0
        };
        let response = node.handle_prepare(1, &prepare_low);

        assert!(response.is_none() || matches!(
            response,
            Some(PaxosMessage::Promise { ballot: (5, 1), .. })
        ));
    }

    #[test]
    fn test_promise_with_accepted_value() {
        let mut node = MultiPaxosNode::new(2, vec![1, 2, 3]);

        // First, accept a value
        let accept = PaxosMessage::Accept {
            ballot: (1, 1),
            slot: 0,
            value: b"first".to_vec(),
        };
        // Simulate accepted state
        node.handle_prepare(1, &PaxosMessage::Prepare { ballot: (1, 1), slot: 0 });
        node.handle_accept(1, &accept);

        // Now new leader prepares
        let prepare = PaxosMessage::Prepare {
            ballot: (2, 3),
            slot: 0
        };
        let response = node.handle_prepare(3, &prepare);

        // Promise should include accepted value
        if let Some(PaxosMessage::Promise { accepted_value, .. }) = response {
            assert_eq!(accepted_value, Some(b"first".to_vec()));
        }
    }

    #[tokio::test]
    async fn test_multi_instance_consensus() {
        let cluster = MultiPaxosCluster::new(vec![1, 2, 3]);

        // Elect leader
        let leader = cluster.elect_leader().await.unwrap();

        // Multiple proposals should use optimization
        let slot1 = cluster.replicate(b"op1".to_vec()).await.unwrap();
        let slot2 = cluster.replicate(b"op2".to_vec()).await.unwrap();
        let slot3 = cluster.replicate(b"op3".to_vec()).await.unwrap();

        assert_eq!(slot1, 0);
        assert_eq!(slot2, 1);
        assert_eq!(slot3, 2);

        // All should be readable
        assert_eq!(cluster.read(0).await, Some(b"op1".to_vec()));
        assert_eq!(cluster.read(1).await, Some(b"op2".to_vec()));
        assert_eq!(cluster.read(2).await, Some(b"op3".to_vec()));
    }

    #[test]
    fn test_log_ordering() {
        let mut node = MultiPaxosNode::new(1, vec![1, 2, 3]);

        // Decide slots out of order
        node.handle_decide(&PaxosMessage::Decide {
            slot: 2,
            value: b"third".to_vec()
        });
        node.handle_decide(&PaxosMessage::Decide {
            slot: 0,
            value: b"first".to_vec()
        });
        node.handle_decide(&PaxosMessage::Decide {
            slot: 1,
            value: b"second".to_vec()
        });

        let log = node.get_log();
        assert_eq!(log.len(), 3);
        assert_eq!(log[0], (0, b"first".to_vec()));
        assert_eq!(log[1], (1, b"second".to_vec()));
        assert_eq!(log[2], (2, b"third".to_vec()));
    }
}
```

### 4.3 Solution de reference

```rust
use std::collections::{HashMap, BTreeMap, HashSet};
use tokio::sync::RwLock;
use std::sync::Arc;

pub type NodeId = u64;
pub type Ballot = (u64, NodeId);
pub type Slot = u64;

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum MultiPaxosError {
    NotLeader,
    NoQuorum,
    BallotTooOld(Ballot),
    SlotAlreadyDecided(Slot),
    Timeout,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum NodeRole {
    Follower,
    Candidate,
    Leader { lease_until: u64 },
}

#[derive(Debug, Clone)]
pub struct PaxosInstance {
    pub slot: Slot,
    pub ballot: Ballot,
    pub value: Option<Vec<u8>>,
    pub state: InstanceState,
    pub promises: HashSet<NodeId>,
    pub accepts: HashSet<NodeId>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum InstanceState {
    Empty,
    Prepared,
    Accepted,
    Decided,
}

#[derive(Debug, Clone)]
pub enum PaxosMessage {
    Prepare { ballot: Ballot, slot: Slot },
    Promise {
        ballot: Ballot,
        slot: Slot,
        accepted_ballot: Option<Ballot>,
        accepted_value: Option<Vec<u8>>,
    },
    Accept { ballot: Ballot, slot: Slot, value: Vec<u8> },
    Accepted { ballot: Ballot, slot: Slot },
    Decide { slot: Slot, value: Vec<u8> },
    LeaderHeartbeat { ballot: Ballot, commit_index: Slot },
    LeaderAck { node_id: NodeId, last_slot: Slot },
}

pub struct MultiPaxosNode {
    id: NodeId,
    role: NodeRole,
    current_ballot: Ballot,
    promised_ballot: Ballot,
    instances: BTreeMap<Slot, PaxosInstance>,
    commit_index: Slot,
    cluster: Vec<NodeId>,
}

impl MultiPaxosNode {
    pub fn new(id: NodeId, cluster: Vec<NodeId>) -> Self {
        Self {
            id,
            role: NodeRole::Follower,
            current_ballot: (0, id),
            promised_ballot: (0, 0),
            instances: BTreeMap::new(),
            commit_index: 0,
            cluster,
        }
    }

    pub async fn become_leader(&mut self) -> Result<Ballot, MultiPaxosError> {
        // Increment ballot number
        self.current_ballot = (self.current_ballot.0 + 1, self.id);
        self.role = NodeRole::Candidate;

        // In real impl: send Prepare to all, collect Promises
        // Simplified: assume we got quorum
        self.promised_ballot = self.current_ballot;
        self.role = NodeRole::Leader { lease_until: u64::MAX };

        Ok(self.current_ballot)
    }

    pub async fn propose(&mut self, value: Vec<u8>) -> Result<Slot, MultiPaxosError> {
        if !self.is_leader() {
            return Err(MultiPaxosError::NotLeader);
        }

        let slot = self.next_slot();
        let instance = PaxosInstance {
            slot,
            ballot: self.current_ballot,
            value: Some(value.clone()),
            state: InstanceState::Accepted,
            promises: HashSet::new(),
            accepts: HashSet::from([self.id]),
        };

        self.instances.insert(slot, instance);

        // In real impl: send Accept, wait for quorum Accepted
        // Then send Decide

        Ok(slot)
    }

    pub fn handle_prepare(&mut self, from: NodeId, msg: &PaxosMessage)
        -> Option<PaxosMessage>
    {
        if let PaxosMessage::Prepare { ballot, slot } = msg {
            // Only promise if ballot >= promised
            if *ballot >= self.promised_ballot {
                self.promised_ballot = *ballot;

                // Get any accepted value for this slot
                let (accepted_ballot, accepted_value) = self.instances
                    .get(slot)
                    .filter(|i| i.state == InstanceState::Accepted)
                    .map(|i| (Some(i.ballot), i.value.clone()))
                    .unwrap_or((None, None));

                return Some(PaxosMessage::Promise {
                    ballot: *ballot,
                    slot: *slot,
                    accepted_ballot,
                    accepted_value,
                });
            }
        }
        None
    }

    pub fn handle_promise(&mut self, from: NodeId, msg: &PaxosMessage)
        -> Option<PaxosMessage>
    {
        if let PaxosMessage::Promise { ballot, slot, accepted_ballot, accepted_value } = msg {
            if let Some(instance) = self.instances.get_mut(slot) {
                instance.promises.insert(from);

                // Adopt highest accepted value
                if let (Some(ab), Some(av)) = (accepted_ballot, accepted_value) {
                    if instance.value.is_none() || *ab > instance.ballot {
                        instance.value = Some(av.clone());
                    }
                }

                // Check quorum
                if instance.promises.len() >= self.quorum_size() {
                    instance.state = InstanceState::Prepared;

                    if let Some(ref value) = instance.value {
                        return Some(PaxosMessage::Accept {
                            ballot: *ballot,
                            slot: *slot,
                            value: value.clone(),
                        });
                    }
                }
            }
        }
        None
    }

    pub fn handle_accept(&mut self, from: NodeId, msg: &PaxosMessage)
        -> Option<PaxosMessage>
    {
        if let PaxosMessage::Accept { ballot, slot, value } = msg {
            if *ballot >= self.promised_ballot {
                self.promised_ballot = *ballot;

                let instance = self.instances.entry(*slot).or_insert_with(|| {
                    PaxosInstance {
                        slot: *slot,
                        ballot: *ballot,
                        value: None,
                        state: InstanceState::Empty,
                        promises: HashSet::new(),
                        accepts: HashSet::new(),
                    }
                });

                instance.ballot = *ballot;
                instance.value = Some(value.clone());
                instance.state = InstanceState::Accepted;

                return Some(PaxosMessage::Accepted {
                    ballot: *ballot,
                    slot: *slot,
                });
            }
        }
        None
    }

    pub fn handle_accepted(&mut self, from: NodeId, msg: &PaxosMessage)
        -> Option<PaxosMessage>
    {
        if let PaxosMessage::Accepted { ballot, slot } = msg {
            if let Some(instance) = self.instances.get_mut(slot) {
                instance.accepts.insert(from);

                if instance.accepts.len() >= self.quorum_size()
                    && instance.state != InstanceState::Decided
                {
                    instance.state = InstanceState::Decided;

                    if let Some(ref value) = instance.value {
                        return Some(PaxosMessage::Decide {
                            slot: *slot,
                            value: value.clone(),
                        });
                    }
                }
            }
        }
        None
    }

    pub fn handle_decide(&mut self, msg: &PaxosMessage) {
        if let PaxosMessage::Decide { slot, value } = msg {
            let instance = self.instances.entry(*slot).or_insert_with(|| {
                PaxosInstance {
                    slot: *slot,
                    ballot: (0, 0),
                    value: None,
                    state: InstanceState::Empty,
                    promises: HashSet::new(),
                    accepts: HashSet::new(),
                }
            });

            instance.value = Some(value.clone());
            instance.state = InstanceState::Decided;

            // Update commit index
            while self.instances
                .get(&self.commit_index)
                .map(|i| i.state == InstanceState::Decided)
                .unwrap_or(false)
            {
                self.commit_index += 1;
            }
        }
    }

    pub fn get_decided(&self, slot: Slot) -> Option<&Vec<u8>> {
        self.instances
            .get(&slot)
            .filter(|i| i.state == InstanceState::Decided)
            .and_then(|i| i.value.as_ref())
    }

    pub fn get_log(&self) -> Vec<(Slot, Vec<u8>)> {
        self.instances
            .iter()
            .filter(|(_, i)| i.state == InstanceState::Decided)
            .filter_map(|(&slot, i)| i.value.clone().map(|v| (slot, v)))
            .collect()
    }

    pub fn is_leader(&self) -> bool {
        matches!(self.role, NodeRole::Leader { .. })
    }

    pub fn next_slot(&self) -> Slot {
        self.instances
            .keys()
            .max()
            .map(|&s| s + 1)
            .unwrap_or(0)
    }

    pub fn quorum_size(&self) -> usize {
        self.cluster.len() / 2 + 1
    }
}

pub struct MultiPaxosCluster {
    nodes: HashMap<NodeId, Arc<RwLock<MultiPaxosNode>>>,
}

impl MultiPaxosCluster {
    pub fn new(node_ids: Vec<NodeId>) -> Self {
        let mut nodes = HashMap::new();
        for &id in &node_ids {
            nodes.insert(
                id,
                Arc::new(RwLock::new(MultiPaxosNode::new(id, node_ids.clone())))
            );
        }
        Self { nodes }
    }

    pub async fn elect_leader(&self) -> Result<NodeId, MultiPaxosError> {
        // Simplified: first node becomes leader
        if let Some((&id, node)) = self.nodes.iter().next() {
            node.write().await.become_leader().await?;
            return Ok(id);
        }
        Err(MultiPaxosError::NoQuorum)
    }

    pub async fn replicate(&self, value: Vec<u8>) -> Result<Slot, MultiPaxosError> {
        for (_, node) in &self.nodes {
            let mut n = node.write().await;
            if n.is_leader() {
                return n.propose(value).await;
            }
        }
        Err(MultiPaxosError::NotLeader)
    }

    pub async fn read(&self, slot: Slot) -> Option<Vec<u8>> {
        for (_, node) in &self.nodes {
            let n = node.read().await;
            if let Some(value) = n.get_decided(slot) {
                return Some(value.clone());
            }
        }
        None
    }
}
```

### 4.9 spec.json

```json
{
  "name": "multi_paxos",
  "language": "rust",
  "type": "code",
  "tier": 3,
  "tier_info": "Integration systeme - Multi-Paxos Consensus",
  "tags": ["distributed", "consensus", "paxos", "replication", "phase5"],
  "passing_score": 70,

  "function": {
    "name": "MultiPaxosNode",
    "prototype": "impl MultiPaxosNode",
    "return_type": "struct"
  },

  "driver": {
    "edge_cases": [
      {
        "name": "ballot_ordering",
        "input": "Compare (1,1) vs (1,2)",
        "expected": "(1,2) > (1,1)",
        "is_trap": true,
        "trap_explanation": "Same ballot number, higher node_id wins for uniqueness"
      },
      {
        "name": "leader_lease_expired",
        "expected": "Must re-run Phase 1",
        "is_trap": true,
        "trap_explanation": "Cannot skip Prepare after lease expires"
      },
      {
        "name": "slot_gap",
        "input": "Decide slots 0, 2 (skip 1)",
        "expected": "commit_index stays at 0",
        "is_trap": true,
        "trap_explanation": "Cannot advance commit past gaps"
      },
      {
        "name": "adopted_value",
        "expected": "Must use value from highest ballot Promise",
        "is_trap": true,
        "trap_explanation": "Safety requires adopting previously accepted values"
      },
      {
        "name": "concurrent_leaders",
        "expected": "Only one succeeds per slot",
        "is_trap": true,
        "trap_explanation": "Ballot uniqueness ensures single value per slot"
      }
    ],

    "fuzzing": {
      "enabled": true,
      "iterations": 200,
      "generators": [
        {
          "type": "custom",
          "generator": "random_paxos_operations"
        }
      ]
    }
  },

  "norm": {
    "allowed_functions": ["tokio", "serde"],
    "forbidden_functions": ["unsafe"],
    "forbidden_crates": ["raft", "paxos-rs"],
    "check_security": true,
    "blocking": true
  }
}
```

### 4.10 Solutions Mutantes

```rust
/* Mutant A (Boundary) : Ballot overflow non gere */
pub async fn become_leader(&mut self) -> Result<Ballot, MultiPaxosError> {
    self.current_ballot = (self.current_ballot.0 + 1, self.id);  // MUTANT: No overflow check
    // ...
}
// Pourquoi c'est faux : u64 overflow cause ballot (0, id) qui perd contre tout
// Ce qui etait pense : "u64 ne deborde jamais en pratique"

/* Mutant B (Safety) : Skip Prepare invalide */
pub async fn propose(&mut self, value: Vec<u8>) -> Result<Slot, MultiPaxosError> {
    // MUTANT: Skip leader check
    let slot = self.next_slot();
    // ...
}
// Pourquoi c'est faux : Non-leader peut proposer, violant single-decree
// Ce qui etait pense : "Le caller verifie is_leader()"

/* Mutant C (Logic) : Gaps acceptes dans commit */
pub fn handle_decide(&mut self, msg: &PaxosMessage) {
    if let PaxosMessage::Decide { slot, value } = msg {
        // MUTANT: Advance commit_index unconditionally
        self.commit_index = std::cmp::max(self.commit_index, *slot);
    }
}
// Pourquoi c'est faux : Gaps dans le log committed causent reads inconsistants
// Ce qui etait pense : "Plus haut slot = plus avance"

/* Mutant D (Edge) : Promise sans valeur acceptee */
pub fn handle_prepare(&mut self, from: NodeId, msg: &PaxosMessage)
    -> Option<PaxosMessage>
{
    if let PaxosMessage::Prepare { ballot, slot } = msg {
        if *ballot >= self.promised_ballot {
            return Some(PaxosMessage::Promise {
                ballot: *ballot,
                slot: *slot,
                accepted_ballot: None,  // MUTANT: Always None
                accepted_value: None,
            });
        }
    }
    None
}
// Pourquoi c'est faux : Nouveau leader perd valeur precedemment acceptee
// Ce qui etait pense : "Nouvelle election = nouveau depart"

/* Mutant E (Return) : Quorum off-by-one */
pub fn quorum_size(&self) -> usize {
    self.cluster.len() / 2  // MUTANT: Missing + 1
}
// Pourquoi c'est faux : N/2 n'est pas majority, split-brain possible
// Ce qui etait pense : "La moitie suffit"
```

---

## SECTION 5 : COMPRENDRE

### 5.1 Ce que cet exercice enseigne

1. **Leader Optimization** : Comment eliminer Phase 1 pour instances consecutives
2. **Ballot Numbers** : Identifiants uniques (number, node_id) pour ordering total
3. **Log Replication** : Slots indexes, gaps, commit index
4. **Safety vs Liveness** : Guarantees meme en presence de failures
5. **Async Coordination** : Patterns tokio pour consensus

### 5.2 LDA - Traduction Litterale

```
FONCTION become_leader QUI RETOURNE UN BALLOT OU ERREUR
DEBUT FONCTION
    INCREMENTER current_ballot.number DE 1
    METTRE role A Candidate

    POUR CHAQUE node DANS cluster FAIRE
        ENVOYER Prepare(current_ballot, all_pending_slots)
        ATTENDRE Promise OU timeout
    FIN POUR

    SI nombre_promises >= quorum ALORS
        METTRE role A Leader
        RETOURNER current_ballot
    SINON
        METTRE role A Follower
        RETOURNER Erreur NoQuorum
    FIN SI
FIN FONCTION
```

### 5.3 Visualisation ASCII

```
Multi-Paxos: Leader Election puis Instance Rapide
=================================================

Noeud 1 (Leader)         Noeud 2              Noeud 3
     |                      |                    |
     |--- Prepare(1,1) ---->|                    |
     |--- Prepare(1,1) ---------------------------->|
     |                      |                    |
     |<-- Promise(1,1) -----|                    |
     |<-- Promise(1,1) -----------------------------|
     |                      |                    |
     |   [LEADER ELECTED - Phase 1 complete]     |
     |                      |                    |
     |=== Instance 0 (needs Phase 1) ============|
     |--- Accept(1,1,0,"A") -->|                  |
     |--- Accept(1,1,0,"A") ---------------------->|
     |<-- Accepted ---------|                    |
     |<-- Accepted -------------------------------|
     |--- Decide(0,"A") --->|                    |
     |--- Decide(0,"A") --------------------------->|
     |                      |                    |
     |=== Instance 1 (OPTIMIZED - skip Phase 1) =|
     |--- Accept(1,1,1,"B") -->|                  |
     |--- Accept(1,1,1,"B") ---------------------->|
     |<-- Accepted ---------|                    |
     |<-- Accepted -------------------------------|
     |--- Decide(1,"B") --->|                    |
     |                      |                    |

     RTT pour Instance 0: 2 (Prepare + Accept)
     RTT pour Instance 1: 1 (Accept seulement)
```

### 5.4 Les pieges en detail

| Piege | Description | Comment l'eviter |
|-------|-------------|------------------|
| **Ballot uniqueness** | Deux noeuds meme ballot | Inclure node_id dans ballot tuple |
| **Lease expiration** | Leader pense etre encore leader | Check lease avant chaque propose |
| **Log gaps** | Commit_index avance malgre gaps | Only increment quand contiguous |
| **Value adoption** | Ignorer valeur acceptee precedente | Toujours adopter highest ballot value |
| **Split quorum** | N/2 au lieu de N/2+1 | Strict majority required |

### 5.5 Cours Complet

#### 5.5.1 De Paxos a Multi-Paxos

Paxos basique resout le consensus pour une seule valeur (single-decree). Multi-Paxos etend cela a une sequence de valeurs (multi-decree) en executant des instances Paxos paralleles, une par slot dans le log.

L'optimisation cle : si un leader a etabli son ballot via Phase 1, il peut reutiliser ce ballot pour toutes les instances suivantes, sautant Phase 1.

#### 5.5.2 Structure du Ballot

```rust
type Ballot = (u64, NodeId);  // (number, node_id)

// Ordering: compare number first, then node_id
impl Ord for Ballot {
    fn cmp(&self, other: &Self) -> Ordering {
        self.0.cmp(&other.0).then(self.1.cmp(&other.1))
    }
}
```

Le node_id garantit l'unicite : deux noeuds ne peuvent avoir le meme ballot.

#### 5.5.3 Invariants de Safety

1. **Agreement** : Deux noeuds ne peuvent decider des valeurs differentes pour le meme slot
2. **Validity** : Une valeur decidee a ete proposee
3. **Integrity** : Un noeud decide au plus une fois par slot

---

## SECTION 6 : PIEGES - RECAPITULATIF

| # | Piege | Symptome | Solution |
|---|-------|----------|----------|
| 1 | Ballot overflow | Leader perd election | Check overflow, use u128 |
| 2 | Skip Prepare invalide | Values perdues | Check is_leader() |
| 3 | Gaps dans commit | Reads inconsistants | Only advance contiguous |
| 4 | Promise sans valeur | Safety violation | Include accepted value |
| 5 | Quorum N/2 | Split-brain | Use N/2 + 1 |

---

## SECTION 7 : QCM

### Question 1
**Pourquoi Multi-Paxos utilise (number, node_id) pour les ballots ?**

A) Pour le debugging
B) Pour garantir l'unicite des ballots
C) Pour le load balancing
D) Pour le routing
E) Pour le sharding

**Reponse : B**

*Explication : Deux noeuds incrementant simultanement obtiennent des ballots differents grace au node_id, evitant les conflits.*

---

### Question 2
**Combien de RTT pour la 5eme operation avec un leader stable ?**

A) 5 RTT
B) 2 RTT
C) 1 RTT
D) 0 RTT
E) 10 RTT

**Reponse : C**

*Explication : Apres l'election (2 RTT pour instance 1), toutes les instances suivantes utilisent 1 RTT (Phase 2 seulement).*

---

### Question 3
**Que faire si Promise contient une valeur acceptee ?**

A) L'ignorer
B) La remplacer par notre valeur
C) L'adopter si ballot superieur
D) Abandonner la proposition
E) Re-executer Phase 1

**Reponse : C**

*Explication : Safety exige d'adopter la valeur du plus haut ballot pour garantir qu'une valeur precedemment acceptee n'est pas perdue.*

---

## SECTION 8 : RECAPITULATIF

| Element | Valeur |
|---------|--------|
| **Nom** | multi_paxos |
| **Module** | 5.4.21 - Multi-Paxos Consensus Protocol |
| **Difficulte** | 8/10 |
| **Temps estime** | 180 min |
| **XP** | 300 (base) + bonus x3 |
| **Concepts cles** | Leader optimization, ballot numbers, log replication |
| **Piege principal** | Oublier d'adopter les valeurs acceptees dans Promise |

---

## SECTION 9 : DEPLOYMENT PACK

```json
{
  "deploy": {
    "hackbrain_version": "5.5.2",
    "engine_version": "v22.1",
    "exercise_slug": "5.4.21-a-multi-paxos",
    "generated_at": "2024-01-15T10:00:00Z",

    "metadata": {
      "exercise_id": "5.4.21-a",
      "exercise_name": "multi_paxos",
      "module": "5.4.21",
      "concept": "a",
      "type": "code",
      "tier": 3,
      "difficulty": 8,
      "language": "rust",
      "duration_minutes": 180,
      "xp_base": 300
    },

    "validation": {
      "expected_pass": ["references/ref_solution.rs"],
      "expected_fail": [
        "mutants/mutant_a_overflow.rs",
        "mutants/mutant_b_skip_check.rs",
        "mutants/mutant_c_gaps.rs",
        "mutants/mutant_d_no_adopt.rs",
        "mutants/mutant_e_quorum.rs"
      ]
    }
  }
}
```

---

*HACKBRAIN v5.5.2 - "One Leader to Rule Them All"*
*Exercise Quality Score: 94/100*
