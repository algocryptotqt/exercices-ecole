<thinking>
## Analyse du Concept
- Concept : Load Balancer HTTP Layer 7 avec Health Checks
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - Le load balancing est essentiel pour les architectures distribuees. L'exercice combine async networking, algorithmes de distribution, et monitoring.

## Combo Base + Bonus
- Exercice de base : Load balancer HTTP avec round-robin, least-connections, IP-hash, health checks, retrait/reintegration automatique
- Bonus : Implementation de session persistence (sticky sessions) avec connection draining et API de gestion hot-reload
- Palier bonus : EXPERT (architecture production-ready)
- Progression logique ? OUI - Base = distribution + health, Bonus = persistence + graceful ops

## Prerequis & Difficulte
- Prerequis reels : tokio (async runtime, TcpListener, TcpStream), hyper (HTTP client/server), serde (config parsing), std::sync (Arc, RwLock)
- Difficulte estimee : 8/10 (base), 10/10 (bonus)
- Coherent avec phase 5 ? OUI

## Aspect Fun/Culture
- Contexte choisi : Reference a "The Matrix" - Les Agents Smith (backends) et l'Oracle (health checker)
- MEME mnemonique : "Dodge this" (routing des requetes comme Neo esquive les balles)
- Pourquoi c'est fun : Le load balancer est le "One" qui distribue le travail aux copies de Smith

## Scenarios d'Echec (5 mutants concrets)
1. Mutant A (Algorithm) : Round-robin ne cycle pas correctement → toujours meme backend
2. Mutant B (Health) : Pas de timeout sur health check → thread bloquee indefiniment
3. Mutant C (Safety) : Backend unhealthy pas retire du pool → erreurs cascadees
4. Mutant D (Concurrency) : Race condition sur active_connections → least-conn incorrect
5. Mutant E (Headers) : X-Forwarded-For pas propage → backend ne voit pas IP client

## Verdict
VALIDE - Exercice avance couvrant les fondamentaux du load balancing production
</thinking>

# Exercice 5.1.7-a : load_balancer

**Module :**
5.1.7 — Load Balancing & Reverse Proxy

**Concept :**
a — HTTP Layer 7 Load Balancer (distribution algorithms, health checks, failover)

**Difficulte :**
★★★★★★★★☆☆ (8/10)

**Type :**
code

**Tiers :**
2 — Integration multi-concepts

**Langage :**
Rust Edition 2024

**Prerequis :**
- 2.5 — Concurrence avancee (Arc, RwLock, atomics)
- 5.1.2 — TCP Fundamentals (serveur multi-client)
- 5.1.3 — HTTP Protocol (hyper, requetes/reponses)
- 3.3 — Async/Await (tokio runtime)

**Domaines :**
Net, Algo, Process

**Duree estimee :**
180 min

**XP Base :**
250

**Complexite :**
T2 O(n) x S2 O(n) ou n = nombre de backends

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichiers a rendre :**
```
src/lib.rs
src/main.rs
config.toml
```

**Dependances autorisees :**
- `tokio = { version = "1", features = ["full"] }`
- `hyper = { version = "1", features = ["full"] }`
- `hyper-util = "0.1"`
- `http-body-util = "0.1"`
- `serde = { version = "1", features = ["derive"] }`
- `toml = "0.8"`
- `tracing = "0.1"`
- `std::sync::{Arc, atomic::{AtomicUsize, AtomicU64, Ordering}}`
- `tokio::sync::RwLock`

**Fonctions/methodes interdites :**
- `unsafe` blocks
- `std::thread` (utiliser tokio uniquement)
- Crates de load balancing externes (tower-balance, etc.)

### 1.2 Consigne

**CONTEXTE : "The Matrix Reloaded"**

*"Tu vois ces Agents Smith ? Ils sont tous identiques, tous capables de traiter la meme requete. Ton role, c'est d'etre l'Oracle : tu decides quel Smith recoit quelle requete. Et quand un Smith tombe, tu le sais avant tout le monde."* — Morpheus, architecte systeme

Dans les architectures modernes, un load balancer est le point d'entree critique. Il distribue le trafic entre plusieurs serveurs backends, surveille leur sante, et garantit la haute disponibilite. Un backend qui tombe ne doit jamais recevoir de trafic.

**Ta mission :**

Implementer un load balancer HTTP Layer 7 qui :
1. Ecoute sur un port configurable et route vers N backends
2. Supporte 3 algorithmes : Round-Robin, Least-Connections, IP-Hash
3. Execute des health checks HTTP periodiques sur chaque backend
4. Retire automatiquement les backends en echec du pool
5. Reintegre les backends qui redeviennent sains
6. Propage les headers X-Forwarded-For et X-Real-IP
7. Collecte des metriques par backend (requetes, latence, erreurs)
8. Expose un endpoint `/lb/health` pour son propre statut

**Configuration (config.toml) :**
```toml
[loadbalancer]
listen_address = "0.0.0.0"
listen_port = 8080
algorithm = "round_robin"  # round_robin | least_connections | ip_hash

[health_check]
interval_secs = 5
timeout_secs = 2
unhealthy_threshold = 3    # echecs consecutifs avant retrait
healthy_threshold = 2      # succes consecutifs avant reintegration

[[backends]]
address = "127.0.0.1"
port = 3001
weight = 1
health_check_path = "/health"

[[backends]]
address = "127.0.0.1"
port = 3002
weight = 2
health_check_path = "/health"

[[backends]]
address = "127.0.0.1"
port = 3003
weight = 1
health_check_path = "/health"
```

**Entree :**
- Fichier de configuration TOML
- Requetes HTTP entrantes

**Sortie :**
- Reponses HTTP proxifiees depuis les backends
- Logs de health checks et changements d'etat
- Endpoint metriques `/lb/metrics` (JSON)

**Contraintes :**
- Le load balancer doit supporter au moins 1000 requetes/seconde
- Latence ajoutee par le proxy < 5ms en moyenne
- Un backend marque unhealthy ne doit JAMAIS recevoir de requete
- Les metriques doivent etre thread-safe et lock-free quand possible
- Le health checker tourne en background sans bloquer le routing

**Exemples :**

| Scenario | Requete | Backend choisi | Raison |
|----------|---------|----------------|--------|
| Round-Robin initial | GET /api | Backend 1 | Index 0 |
| Round-Robin suivant | GET /api | Backend 2 | Index 1 |
| Backend 2 unhealthy | GET /api | Backend 3 | Skip unhealthy |
| Least-Connections | GET /api | Backend avec min conn | Selection dynamique |
| IP-Hash client A | GET /api | Backend 2 | hash(IP) % 3 = 1 |
| IP-Hash client A (bis) | GET /api | Backend 2 | Meme hash = meme backend |

### 1.2.2 Consigne Academique

Implementer un reverse proxy HTTP Layer 7 utilisant le pattern d'architecture distribuee "load balancing". Le systeme doit gerer un pool de backends avec detection de pannes active (health checks) et selection dynamique selon differents algorithmes (round-robin, least-connections, consistent hashing). L'implementation doit etre entierement asynchrone et thread-safe.

### 1.3 Prototype

```rust
use std::collections::HashMap;
use std::net::{IpAddr, SocketAddr};
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, AtomicU64, AtomicBool, Ordering};
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};

/// Algorithme de load balancing
#[derive(Debug, Clone, Copy, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "snake_case")]
pub enum Algorithm {
    RoundRobin,
    LeastConnections,
    IpHash,
}

/// Configuration d'un backend
#[derive(Debug, Clone, Deserialize)]
pub struct BackendConfig {
    pub address: String,
    pub port: u16,
    #[serde(default = "default_weight")]
    pub weight: u32,
    #[serde(default = "default_health_path")]
    pub health_check_path: String,
}

fn default_weight() -> u32 { 1 }
fn default_health_path() -> String { "/health".to_string() }

/// Configuration du health check
#[derive(Debug, Clone, Deserialize)]
pub struct HealthCheckConfig {
    pub interval_secs: u64,
    pub timeout_secs: u64,
    #[serde(default = "default_unhealthy_threshold")]
    pub unhealthy_threshold: u32,
    #[serde(default = "default_healthy_threshold")]
    pub healthy_threshold: u32,
}

fn default_unhealthy_threshold() -> u32 { 3 }
fn default_healthy_threshold() -> u32 { 2 }

/// Configuration du load balancer
#[derive(Debug, Clone, Deserialize)]
pub struct LoadBalancerConfig {
    pub listen_address: String,
    pub listen_port: u16,
    pub algorithm: Algorithm,
}

/// Configuration globale
#[derive(Debug, Clone, Deserialize)]
pub struct Config {
    pub loadbalancer: LoadBalancerConfig,
    pub health_check: HealthCheckConfig,
    pub backends: Vec<BackendConfig>,
}

/// Etat d'un backend
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum BackendState {
    Healthy,
    Unhealthy,
    Draining,
}

/// Metriques atomiques d'un backend
#[derive(Debug, Default)]
pub struct BackendMetrics {
    pub total_requests: AtomicU64,
    pub active_connections: AtomicUsize,
    pub total_errors: AtomicU64,
    pub total_latency_us: AtomicU64,
    pub consecutive_failures: AtomicUsize,
    pub consecutive_successes: AtomicUsize,
}

/// Backend avec son etat et metriques
pub struct Backend {
    pub config: BackendConfig,
    pub state: RwLock<BackendState>,
    pub metrics: BackendMetrics,
    pub socket_addr: SocketAddr,
}

impl Backend {
    pub fn new(config: BackendConfig) -> std::io::Result<Self>;
    pub async fn is_healthy(&self) -> bool;
    pub fn address(&self) -> String;
}

/// Pool de backends
pub struct BackendPool {
    backends: Vec<Arc<Backend>>,
    round_robin_index: AtomicUsize,
}

/// Resultat d'une selection de backend
pub struct BackendSelection {
    pub backend: Arc<Backend>,
    pub index: usize,
}

impl BackendPool {
    /// Cree un nouveau pool a partir des configurations
    pub fn new(configs: Vec<BackendConfig>) -> std::io::Result<Self>;

    /// Selectionne un backend selon l'algorithme
    pub async fn select(
        &self,
        algorithm: Algorithm,
        client_ip: Option<IpAddr>,
    ) -> Option<BackendSelection>;

    /// Selection Round-Robin (saute les unhealthy)
    async fn select_round_robin(&self) -> Option<BackendSelection>;

    /// Selection Least-Connections (parmi les healthy)
    async fn select_least_connections(&self) -> Option<BackendSelection>;

    /// Selection IP-Hash (consistent hashing)
    async fn select_ip_hash(&self, client_ip: IpAddr) -> Option<BackendSelection>;

    /// Retourne tous les backends sains
    pub async fn healthy_backends(&self) -> Vec<Arc<Backend>>;

    /// Retourne le nombre total de backends
    pub fn len(&self) -> usize;

    /// Retourne un backend par index
    pub fn get(&self, index: usize) -> Option<Arc<Backend>>;
}

/// Health checker avec detection de pannes
pub struct HealthChecker {
    pool: Arc<BackendPool>,
    config: HealthCheckConfig,
}

impl HealthChecker {
    pub fn new(pool: Arc<BackendPool>, config: HealthCheckConfig) -> Self;

    /// Demarre les health checks en background
    pub fn start(self) -> tokio::task::JoinHandle<()>;

    /// Effectue un health check sur un backend specifique
    async fn check_backend(&self, backend: &Backend) -> bool;

    /// Met a jour l'etat du backend selon le resultat
    async fn update_backend_state(&self, backend: &Backend, success: bool);
}

/// Metriques globales du load balancer
#[derive(Debug, Default, Serialize)]
pub struct LoadBalancerMetrics {
    pub total_requests: u64,
    pub total_errors: u64,
    pub requests_per_second: f64,
    pub avg_latency_us: f64,
    pub healthy_backends: usize,
    pub total_backends: usize,
    pub backends: Vec<BackendMetricsSummary>,
}

#[derive(Debug, Serialize)]
pub struct BackendMetricsSummary {
    pub address: String,
    pub state: String,
    pub active_connections: usize,
    pub total_requests: u64,
    pub total_errors: u64,
    pub avg_latency_us: f64,
    pub error_rate: f64,
}

/// Le load balancer principal
pub struct LoadBalancer {
    config: Config,
    pool: Arc<BackendPool>,
    start_time: Instant,
}

impl LoadBalancer {
    /// Charge la configuration et cree le load balancer
    pub fn from_config(config: Config) -> std::io::Result<Self>;

    /// Charge depuis un fichier TOML
    pub fn from_file(path: &str) -> Result<Self, Box<dyn std::error::Error>>;

    /// Demarre le load balancer (bloquant)
    pub async fn run(&self) -> std::io::Result<()>;

    /// Retourne les metriques courantes
    pub async fn get_metrics(&self) -> LoadBalancerMetrics;

    /// Gere une requete entrante
    async fn handle_request(
        &self,
        req: hyper::Request<hyper::body::Incoming>,
        client_addr: SocketAddr,
    ) -> Result<hyper::Response<http_body_util::Full<bytes::Bytes>>, hyper::Error>;

    /// Forwarde une requete vers un backend
    async fn forward_request(
        &self,
        backend: Arc<Backend>,
        mut req: hyper::Request<hyper::body::Incoming>,
        client_ip: IpAddr,
    ) -> Result<hyper::Response<http_body_util::Full<bytes::Bytes>>, Box<dyn std::error::Error>>;
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 Les Trois Mousquetaires du Load Balancing

Il existe trois grands niveaux de load balancing :

```
┌─────────────────────────────────────────────────────────────────┐
│                    COUCHES DE LOAD BALANCING                    │
├─────────────┬───────────────────┬───────────────────────────────┤
│   Layer 4   │      Layer 7      │          DNS-based            │
│   (TCP/UDP) │      (HTTP)       │         (GeoDNS)              │
├─────────────┼───────────────────┼───────────────────────────────┤
│ • Tres rapide│ • Inspection HTTP │ • Distribution globale        │
│ • Pas d'inspection│ • Routing par URL│ • Latence variable        │
│ • NAT/DSR   │ • Headers custom  │ • TTL-dependent               │
│ • HAProxy L4│ • NGINX, HAProxy  │ • Route 53, Cloudflare       │
└─────────────┴───────────────────┴───────────────────────────────┘
```

### 2.2 Pourquoi Health Checks Actifs ?

Les health checks **passifs** (observer les erreurs) reagissent trop tard. Les health checks **actifs** (GET /health periodiques) detectent les problemes AVANT qu'un vrai utilisateur ne soit impacte.

```
TIMELINE COMPARAISON
───────────────────────────────────────────────────────────────►
     │                    │                     │
     │  Backend crash     │  Premier user       │  Detection passive
     │        ▼           │  impacte ▼          │        ▼
     ├────────────────────┼─────────────────────┼──────────────────
     │                    │                     │
     │  Backend crash     │  Health check       │  Retrait auto
     │        ▼           │  detecte ▼          │        ▼
     ├────────────────────┼─────────────────────┼──────────────────
                          │◄── Active gagne ──►│
                              (5-10 sec)
```

### 2.3 Consistent Hashing (IP-Hash)

L'IP-Hash assure qu'un meme client arrive toujours sur le meme backend (utile pour les sessions locales). Le "consistent" hashing minimise la redistribution quand un backend tombe :

```
AVANT (3 backends)           APRES (backend 2 down)
     ┌───┐                        ┌───┐
 IP1 │ 1 │                    IP1 │ 1 │  (inchange)
     ├───┤                        ├───┤
 IP2 │ 2 │  ─────────────►    IP2 │ 3 │  (remappe)
     ├───┤                        ├───┤
 IP3 │ 3 │                    IP3 │ 3 │  (inchange)
     └───┘                        └───┘
```

---

## SECTION 2.5 : DANS LA VRAIE VIE

### Metiers concernes

| Metier | Utilisation |
|--------|-------------|
| **SRE/DevOps** | Configuration HAProxy, NGINX, AWS ALB/NLB |
| **Cloud Architect** | Design haute disponibilite, multi-AZ |
| **Backend Developer** | Endpoints /health, graceful shutdown |
| **Performance Engineer** | Tuning algorithmes, metriques latence |

### Cas d'usage concrets

1. **NGINX Plus** : Load balancer commercial avec health checks actifs, ~$2500/an
2. **HAProxy** : Standard open-source, utilisee par GitHub, Reddit, Stack Overflow
3. **AWS ALB/NLB** : Managed load balancing, health checks integres
4. **Kubernetes Ingress** : Load balancing natif pour pods/services
5. **Cloudflare** : Load balancing global avec GeoDNS

### Les metriques qui comptent

| Metrique | Seuil alerte | Action |
|----------|--------------|--------|
| Error rate > 1% | Alerte | Investiguer backend |
| P99 latency > 500ms | Warning | Scale ou optimize |
| Healthy backends < 50% | Critical | Incident majeur |
| Connection queue > 1000 | Warning | Scale load balancer |

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ ls
Cargo.toml  config.toml  src/

$ cat config.toml
[loadbalancer]
listen_address = "0.0.0.0"
listen_port = 8080
algorithm = "round_robin"

[health_check]
interval_secs = 5
timeout_secs = 2
unhealthy_threshold = 3
healthy_threshold = 2

[[backends]]
address = "127.0.0.1"
port = 3001
weight = 1
health_check_path = "/health"

[[backends]]
address = "127.0.0.1"
port = 3002
weight = 1
health_check_path = "/health"

$ cargo build --release
   Compiling load_balancer v0.1.0
    Finished release [optimized] target(s)

# Terminal 1: Demarrer deux backends simples
$ python3 -m http.server 3001 &
$ python3 -m http.server 3002 &

# Terminal 2: Demarrer le load balancer
$ cargo run --release
[INFO] Load balancer starting on 0.0.0.0:8080
[INFO] Algorithm: RoundRobin
[INFO] Backend 0: 127.0.0.1:3001 - Healthy
[INFO] Backend 1: 127.0.0.1:3002 - Healthy
[INFO] Health checker started (interval: 5s)

# Terminal 3: Tester le load balancing
$ curl http://localhost:8080/
# Reponse du backend 3001

$ curl http://localhost:8080/
# Reponse du backend 3002 (round-robin)

$ curl http://localhost:8080/lb/health
{"status":"healthy","healthy_backends":2,"total_backends":2}

$ curl http://localhost:8080/lb/metrics
{
  "total_requests": 3,
  "total_errors": 0,
  "requests_per_second": 0.5,
  "avg_latency_us": 1250.0,
  "healthy_backends": 2,
  "total_backends": 2,
  "backends": [
    {
      "address": "127.0.0.1:3001",
      "state": "Healthy",
      "active_connections": 0,
      "total_requests": 2,
      "avg_latency_us": 1100.0,
      "error_rate": 0.0
    },
    {
      "address": "127.0.0.1:3002",
      "state": "Healthy",
      "active_connections": 0,
      "total_requests": 1,
      "avg_latency_us": 1400.0,
      "error_rate": 0.0
    }
  ]
}

# Simuler panne du backend 3002
$ kill %2  # Tuer le serveur sur port 3002

# Apres quelques health checks...
[WARN] Backend 127.0.0.1:3002 health check failed (1/3)
[WARN] Backend 127.0.0.1:3002 health check failed (2/3)
[WARN] Backend 127.0.0.1:3002 health check failed (3/3)
[ERROR] Backend 127.0.0.1:3002 marked UNHEALTHY

$ curl http://localhost:8080/
# Toujours reponse du backend 3001 (seul healthy)

$ curl http://localhost:8080/lb/health
{"status":"degraded","healthy_backends":1,"total_backends":2}

$ cargo test
running 12 tests
test tests::test_config_parsing ... ok
test tests::test_round_robin_selection ... ok
test tests::test_least_connections_selection ... ok
test tests::test_ip_hash_consistency ... ok
test tests::test_health_check_success ... ok
test tests::test_health_check_failure ... ok
test tests::test_unhealthy_backend_skipped ... ok
test tests::test_backend_recovery ... ok
test tests::test_forwarded_headers ... ok
test tests::test_metrics_accuracy ... ok
test tests::test_all_backends_down ... ok
test tests::test_concurrent_requests ... ok

test result: ok. 12 passed; 0 failed
```

### 3.1 BONUS EXPERT (OPTIONNEL)

**Difficulte Bonus :**
★★★★★★★★★★ (10/10)

**Recompense :**
XP x4

**Time Complexity attendue :**
O(1) pour selection, O(n) pour health checks

**Space Complexity attendue :**
O(n*m) ou n = backends, m = sessions

**Domaines Bonus :**
`Algo, Security`

#### 3.1.1 Consigne Bonus

**"The Architect's Design"**

*"Le probleme, c'est le choix. Chaque session doit retrouver son backend. C'est l'equation fondamentale de la persistence."*

**Ta mission bonus :**

Implementer la **Session Persistence** (sticky sessions) et le **Connection Draining** :

1. **Sticky Sessions** avec cookie ou header
   - Cookie `X-LB-Session` contenant le backend ID
   - TTL configurable pour les sessions
   - Fallback sur algorithme si backend down

2. **Connection Draining**
   - Mode "draining" : n'accepte plus de nouvelles connexions
   - Attend la fin des connexions actives (timeout configurable)
   - Retrait propre sans perte de requetes

3. **API de Gestion Hot-Reload**
   - `POST /lb/admin/backends` - Ajouter un backend
   - `DELETE /lb/admin/backends/{id}` - Retirer (avec drain)
   - `PUT /lb/admin/backends/{id}/drain` - Mettre en draining
   - `GET /lb/admin/config` - Config actuelle

**Contraintes bonus :**
```
┌─────────────────────────────────────────────┐
│  session_ttl >= 60s                         │
│  drain_timeout <= 300s                      │
│  Zero dropped requests during drain         │
│  Admin API avec auth token                  │
└─────────────────────────────────────────────┘
```

#### 3.1.2 Prototype Bonus

```rust
/// Configuration de session persistence
#[derive(Debug, Clone, Deserialize)]
pub struct SessionConfig {
    pub enabled: bool,
    pub cookie_name: String,
    pub ttl_secs: u64,
}

/// Session stockee
pub struct Session {
    pub backend_index: usize,
    pub created_at: Instant,
    pub last_seen: Instant,
}

/// Store de sessions (thread-safe)
pub struct SessionStore {
    sessions: RwLock<HashMap<String, Session>>,
    ttl: Duration,
}

impl SessionStore {
    pub fn new(ttl: Duration) -> Self;
    pub async fn get(&self, session_id: &str) -> Option<usize>;
    pub async fn set(&self, session_id: &str, backend_index: usize);
    pub async fn cleanup_expired(&self);
}

/// Backend avec support draining
impl Backend {
    /// Met le backend en mode draining
    pub async fn start_draining(&self);

    /// Verifie si le drain est termine
    pub async fn is_drain_complete(&self) -> bool;

    /// Attend la fin du drain avec timeout
    pub async fn wait_drain(&self, timeout: Duration) -> bool;
}

/// API d'administration
pub struct AdminApi {
    pool: Arc<RwLock<BackendPool>>,
    auth_token: String,
}

impl AdminApi {
    pub fn new(pool: Arc<RwLock<BackendPool>>, token: String) -> Self;

    /// Ajoute un backend a chaud
    pub async fn add_backend(&self, config: BackendConfig) -> Result<usize, String>;

    /// Retire un backend avec drain
    pub async fn remove_backend(&self, index: usize, drain_timeout: Duration) -> Result<(), String>;

    /// Met un backend en draining
    pub async fn drain_backend(&self, index: usize) -> Result<(), String>;
}
```

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette - Tableau des tests

| Test | Description | Points | Categorie |
|------|-------------|--------|-----------|
| `test_config_parsing` | Parse config.toml correctement | 5 | Setup |
| `test_backend_creation` | Backends crees avec bonnes adresses | 5 | Setup |
| `test_round_robin_selection` | RR cycle correctement entre backends | 15 | Algorithm |
| `test_round_robin_skips_unhealthy` | RR saute les backends down | 10 | Algorithm |
| `test_least_connections_selection` | LC choisit le moins charge | 15 | Algorithm |
| `test_ip_hash_consistency` | Meme IP -> meme backend | 10 | Algorithm |
| `test_health_check_success` | Backend sain detecte | 5 | Health |
| `test_health_check_failure` | Backend mort detecte | 5 | Health |
| `test_unhealthy_threshold` | Retrait apres N echecs | 10 | Health |
| `test_healthy_threshold` | Reintegration apres N succes | 10 | Health |
| `test_forwarded_headers` | X-Forwarded-For propage | 5 | Proxy |
| `test_metrics_accuracy` | Metriques correctes | 5 | Observability |

**Score minimum pour validation : 70/100**

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::net::IpAddr;
    use std::str::FromStr;
    use std::time::Duration;
    use tokio::time::sleep;

    fn create_test_backends() -> Vec<BackendConfig> {
        vec![
            BackendConfig {
                address: "127.0.0.1".to_string(),
                port: 3001,
                weight: 1,
                health_check_path: "/health".to_string(),
            },
            BackendConfig {
                address: "127.0.0.1".to_string(),
                port: 3002,
                weight: 1,
                health_check_path: "/health".to_string(),
            },
            BackendConfig {
                address: "127.0.0.1".to_string(),
                port: 3003,
                weight: 1,
                health_check_path: "/health".to_string(),
            },
        ]
    }

    #[tokio::test]
    async fn test_round_robin_selection() {
        let pool = BackendPool::new(create_test_backends()).unwrap();

        // Tous healthy par defaut
        let sel1 = pool.select(Algorithm::RoundRobin, None).await.unwrap();
        let sel2 = pool.select(Algorithm::RoundRobin, None).await.unwrap();
        let sel3 = pool.select(Algorithm::RoundRobin, None).await.unwrap();
        let sel4 = pool.select(Algorithm::RoundRobin, None).await.unwrap();

        // Doit cycler 0 -> 1 -> 2 -> 0
        assert_eq!(sel1.index, 0);
        assert_eq!(sel2.index, 1);
        assert_eq!(sel3.index, 2);
        assert_eq!(sel4.index, 0);
    }

    #[tokio::test]
    async fn test_round_robin_skips_unhealthy() {
        let pool = BackendPool::new(create_test_backends()).unwrap();

        // Marquer backend 1 comme unhealthy
        {
            let mut state = pool.backends[1].state.write().await;
            *state = BackendState::Unhealthy;
        }

        let sel1 = pool.select(Algorithm::RoundRobin, None).await.unwrap();
        let sel2 = pool.select(Algorithm::RoundRobin, None).await.unwrap();
        let sel3 = pool.select(Algorithm::RoundRobin, None).await.unwrap();

        // Ne doit jamais retourner index 1
        assert_ne!(sel1.index, 1);
        assert_ne!(sel2.index, 1);
        assert_ne!(sel3.index, 1);
    }

    #[tokio::test]
    async fn test_least_connections_selection() {
        let pool = BackendPool::new(create_test_backends()).unwrap();

        // Simuler des connexions actives
        pool.backends[0].metrics.active_connections.store(5, Ordering::SeqCst);
        pool.backends[1].metrics.active_connections.store(2, Ordering::SeqCst);
        pool.backends[2].metrics.active_connections.store(10, Ordering::SeqCst);

        let sel = pool.select(Algorithm::LeastConnections, None).await.unwrap();

        // Doit choisir backend 1 (2 connexions)
        assert_eq!(sel.index, 1);
    }

    #[tokio::test]
    async fn test_ip_hash_consistency() {
        let pool = BackendPool::new(create_test_backends()).unwrap();

        let ip = IpAddr::from_str("192.168.1.100").unwrap();

        let sel1 = pool.select(Algorithm::IpHash, Some(ip)).await.unwrap();
        let sel2 = pool.select(Algorithm::IpHash, Some(ip)).await.unwrap();
        let sel3 = pool.select(Algorithm::IpHash, Some(ip)).await.unwrap();

        // Meme IP = meme backend
        assert_eq!(sel1.index, sel2.index);
        assert_eq!(sel2.index, sel3.index);
    }

    #[tokio::test]
    async fn test_ip_hash_different_ips() {
        let pool = BackendPool::new(create_test_backends()).unwrap();

        let ip1 = IpAddr::from_str("192.168.1.1").unwrap();
        let ip2 = IpAddr::from_str("192.168.1.2").unwrap();
        let ip3 = IpAddr::from_str("10.0.0.1").unwrap();

        let sel1 = pool.select(Algorithm::IpHash, Some(ip1)).await;
        let sel2 = pool.select(Algorithm::IpHash, Some(ip2)).await;
        let sel3 = pool.select(Algorithm::IpHash, Some(ip3)).await;

        // Toutes les selections doivent reussir
        assert!(sel1.is_some());
        assert!(sel2.is_some());
        assert!(sel3.is_some());
    }

    #[tokio::test]
    async fn test_all_unhealthy_returns_none() {
        let pool = BackendPool::new(create_test_backends()).unwrap();

        // Marquer tous comme unhealthy
        for backend in &pool.backends {
            let mut state = backend.state.write().await;
            *state = BackendState::Unhealthy;
        }

        let sel = pool.select(Algorithm::RoundRobin, None).await;
        assert!(sel.is_none());
    }

    #[tokio::test]
    async fn test_healthy_backends_count() {
        let pool = BackendPool::new(create_test_backends()).unwrap();

        // Initialement tous healthy
        let healthy = pool.healthy_backends().await;
        assert_eq!(healthy.len(), 3);

        // Marquer un comme unhealthy
        {
            let mut state = pool.backends[0].state.write().await;
            *state = BackendState::Unhealthy;
        }

        let healthy = pool.healthy_backends().await;
        assert_eq!(healthy.len(), 2);
    }

    #[tokio::test]
    async fn test_metrics_increment() {
        let pool = BackendPool::new(create_test_backends()).unwrap();

        let backend = &pool.backends[0];

        // Incrementer les metriques
        backend.metrics.total_requests.fetch_add(1, Ordering::SeqCst);
        backend.metrics.active_connections.fetch_add(1, Ordering::SeqCst);

        assert_eq!(backend.metrics.total_requests.load(Ordering::SeqCst), 1);
        assert_eq!(backend.metrics.active_connections.load(Ordering::SeqCst), 1);

        // Decrementer connexions actives
        backend.metrics.active_connections.fetch_sub(1, Ordering::SeqCst);
        assert_eq!(backend.metrics.active_connections.load(Ordering::SeqCst), 0);
    }

    #[tokio::test]
    async fn test_backend_state_transitions() {
        let configs = create_test_backends();
        let backend = Backend::new(configs[0].clone()).unwrap();

        // Initial: Healthy
        assert!(backend.is_healthy().await);

        // Transition to Unhealthy
        {
            let mut state = backend.state.write().await;
            *state = BackendState::Unhealthy;
        }
        assert!(!backend.is_healthy().await);

        // Transition to Draining
        {
            let mut state = backend.state.write().await;
            *state = BackendState::Draining;
        }
        assert!(!backend.is_healthy().await);

        // Back to Healthy
        {
            let mut state = backend.state.write().await;
            *state = BackendState::Healthy;
        }
        assert!(backend.is_healthy().await);
    }

    #[test]
    fn test_config_parsing() {
        let toml_str = r#"
            [loadbalancer]
            listen_address = "0.0.0.0"
            listen_port = 8080
            algorithm = "round_robin"

            [health_check]
            interval_secs = 5
            timeout_secs = 2
            unhealthy_threshold = 3
            healthy_threshold = 2

            [[backends]]
            address = "127.0.0.1"
            port = 3001
            weight = 1
            health_check_path = "/health"
        "#;

        let config: Config = toml::from_str(toml_str).unwrap();

        assert_eq!(config.loadbalancer.listen_port, 8080);
        assert_eq!(config.loadbalancer.algorithm, Algorithm::RoundRobin);
        assert_eq!(config.backends.len(), 1);
        assert_eq!(config.health_check.unhealthy_threshold, 3);
    }
}
```

### 4.3 Solution de reference

```rust
use std::collections::hash_map::DefaultHasher;
use std::hash::{Hash, Hasher};
use std::net::{IpAddr, SocketAddr};
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, AtomicU64, Ordering};
use std::time::{Duration, Instant};

use bytes::Bytes;
use http_body_util::{BodyExt, Full};
use hyper::body::Incoming;
use hyper::server::conn::http1;
use hyper::service::service_fn;
use hyper::{Request, Response, StatusCode};
use hyper_util::rt::TokioIo;
use serde::{Deserialize, Serialize};
use tokio::net::TcpListener;
use tokio::sync::RwLock;
use tracing::{info, warn, error};

#[derive(Debug, Clone, Copy, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "snake_case")]
pub enum Algorithm {
    RoundRobin,
    LeastConnections,
    IpHash,
}

#[derive(Debug, Clone, Deserialize)]
pub struct BackendConfig {
    pub address: String,
    pub port: u16,
    #[serde(default = "default_weight")]
    pub weight: u32,
    #[serde(default = "default_health_path")]
    pub health_check_path: String,
}

fn default_weight() -> u32 { 1 }
fn default_health_path() -> String { "/health".to_string() }

#[derive(Debug, Clone, Deserialize)]
pub struct HealthCheckConfig {
    pub interval_secs: u64,
    pub timeout_secs: u64,
    #[serde(default = "default_unhealthy_threshold")]
    pub unhealthy_threshold: u32,
    #[serde(default = "default_healthy_threshold")]
    pub healthy_threshold: u32,
}

fn default_unhealthy_threshold() -> u32 { 3 }
fn default_healthy_threshold() -> u32 { 2 }

#[derive(Debug, Clone, Deserialize)]
pub struct LoadBalancerConfig {
    pub listen_address: String,
    pub listen_port: u16,
    pub algorithm: Algorithm,
}

#[derive(Debug, Clone, Deserialize)]
pub struct Config {
    pub loadbalancer: LoadBalancerConfig,
    pub health_check: HealthCheckConfig,
    pub backends: Vec<BackendConfig>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum BackendState {
    Healthy,
    Unhealthy,
    Draining,
}

#[derive(Debug, Default)]
pub struct BackendMetrics {
    pub total_requests: AtomicU64,
    pub active_connections: AtomicUsize,
    pub total_errors: AtomicU64,
    pub total_latency_us: AtomicU64,
    pub consecutive_failures: AtomicUsize,
    pub consecutive_successes: AtomicUsize,
}

pub struct Backend {
    pub config: BackendConfig,
    pub state: RwLock<BackendState>,
    pub metrics: BackendMetrics,
    pub socket_addr: SocketAddr,
}

impl Backend {
    pub fn new(config: BackendConfig) -> std::io::Result<Self> {
        let socket_addr: SocketAddr = format!("{}:{}", config.address, config.port)
            .parse()
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidInput, e))?;

        Ok(Self {
            config,
            state: RwLock::new(BackendState::Healthy),
            metrics: BackendMetrics::default(),
            socket_addr,
        })
    }

    pub async fn is_healthy(&self) -> bool {
        *self.state.read().await == BackendState::Healthy
    }

    pub fn address(&self) -> String {
        format!("{}:{}", self.config.address, self.config.port)
    }
}

pub struct BackendPool {
    pub backends: Vec<Arc<Backend>>,
    round_robin_index: AtomicUsize,
}

pub struct BackendSelection {
    pub backend: Arc<Backend>,
    pub index: usize,
}

impl BackendPool {
    pub fn new(configs: Vec<BackendConfig>) -> std::io::Result<Self> {
        let backends: Result<Vec<_>, _> = configs
            .into_iter()
            .map(|c| Backend::new(c).map(Arc::new))
            .collect();

        Ok(Self {
            backends: backends?,
            round_robin_index: AtomicUsize::new(0),
        })
    }

    pub async fn select(
        &self,
        algorithm: Algorithm,
        client_ip: Option<IpAddr>,
    ) -> Option<BackendSelection> {
        match algorithm {
            Algorithm::RoundRobin => self.select_round_robin().await,
            Algorithm::LeastConnections => self.select_least_connections().await,
            Algorithm::IpHash => {
                if let Some(ip) = client_ip {
                    self.select_ip_hash(ip).await
                } else {
                    self.select_round_robin().await
                }
            }
        }
    }

    async fn select_round_robin(&self) -> Option<BackendSelection> {
        let len = self.backends.len();
        if len == 0 {
            return None;
        }

        // Essayer jusqu'a len fois pour trouver un backend healthy
        for _ in 0..len {
            let index = self.round_robin_index.fetch_add(1, Ordering::SeqCst) % len;
            let backend = &self.backends[index];

            if backend.is_healthy().await {
                return Some(BackendSelection {
                    backend: Arc::clone(backend),
                    index,
                });
            }
        }

        None
    }

    async fn select_least_connections(&self) -> Option<BackendSelection> {
        let mut min_conn = usize::MAX;
        let mut selected: Option<(Arc<Backend>, usize)> = None;

        for (index, backend) in self.backends.iter().enumerate() {
            if !backend.is_healthy().await {
                continue;
            }

            let conn = backend.metrics.active_connections.load(Ordering::SeqCst);
            if conn < min_conn {
                min_conn = conn;
                selected = Some((Arc::clone(backend), index));
            }
        }

        selected.map(|(backend, index)| BackendSelection { backend, index })
    }

    async fn select_ip_hash(&self, client_ip: IpAddr) -> Option<BackendSelection> {
        let healthy: Vec<_> = self.healthy_backends().await;
        if healthy.is_empty() {
            return None;
        }

        let mut hasher = DefaultHasher::new();
        client_ip.hash(&mut hasher);
        let hash = hasher.finish();

        let index = (hash as usize) % healthy.len();
        let backend = Arc::clone(&healthy[index]);

        // Trouver l'index original
        let original_index = self.backends
            .iter()
            .position(|b| Arc::ptr_eq(b, &backend))
            .unwrap_or(0);

        Some(BackendSelection {
            backend,
            index: original_index,
        })
    }

    pub async fn healthy_backends(&self) -> Vec<Arc<Backend>> {
        let mut healthy = Vec::new();
        for backend in &self.backends {
            if backend.is_healthy().await {
                healthy.push(Arc::clone(backend));
            }
        }
        healthy
    }

    pub fn len(&self) -> usize {
        self.backends.len()
    }

    pub fn get(&self, index: usize) -> Option<Arc<Backend>> {
        self.backends.get(index).cloned()
    }
}

pub struct HealthChecker {
    pool: Arc<BackendPool>,
    config: HealthCheckConfig,
}

impl HealthChecker {
    pub fn new(pool: Arc<BackendPool>, config: HealthCheckConfig) -> Self {
        Self { pool, config }
    }

    pub fn start(self) -> tokio::task::JoinHandle<()> {
        tokio::spawn(async move {
            let interval = Duration::from_secs(self.config.interval_secs);

            loop {
                for backend in &self.pool.backends {
                    let success = self.check_backend(backend).await;
                    self.update_backend_state(backend, success).await;
                }

                tokio::time::sleep(interval).await;
            }
        })
    }

    async fn check_backend(&self, backend: &Backend) -> bool {
        let timeout = Duration::from_secs(self.config.timeout_secs);
        let url = format!(
            "http://{}{}",
            backend.address(),
            backend.config.health_check_path
        );

        let client = hyper_util::client::legacy::Client::builder(
            hyper_util::rt::TokioExecutor::new()
        ).build_http();

        let result = tokio::time::timeout(timeout, async {
            let req = Request::builder()
                .method("GET")
                .uri(&url)
                .body(Full::<Bytes>::new(Bytes::new()))
                .unwrap();

            client.request(req).await
        }).await;

        match result {
            Ok(Ok(resp)) => resp.status().is_success(),
            _ => false,
        }
    }

    async fn update_backend_state(&self, backend: &Backend, success: bool) {
        if success {
            backend.metrics.consecutive_failures.store(0, Ordering::SeqCst);
            let successes = backend.metrics.consecutive_successes
                .fetch_add(1, Ordering::SeqCst) + 1;

            let mut state = backend.state.write().await;
            if *state == BackendState::Unhealthy
                && successes >= self.config.healthy_threshold as usize
            {
                *state = BackendState::Healthy;
                info!("Backend {} marked HEALTHY", backend.address());
            }
        } else {
            backend.metrics.consecutive_successes.store(0, Ordering::SeqCst);
            let failures = backend.metrics.consecutive_failures
                .fetch_add(1, Ordering::SeqCst) + 1;

            warn!(
                "Backend {} health check failed ({}/{})",
                backend.address(),
                failures,
                self.config.unhealthy_threshold
            );

            let mut state = backend.state.write().await;
            if *state == BackendState::Healthy
                && failures >= self.config.unhealthy_threshold as usize
            {
                *state = BackendState::Unhealthy;
                error!("Backend {} marked UNHEALTHY", backend.address());
            }
        }
    }
}

#[derive(Debug, Default, Serialize)]
pub struct LoadBalancerMetrics {
    pub total_requests: u64,
    pub total_errors: u64,
    pub requests_per_second: f64,
    pub avg_latency_us: f64,
    pub healthy_backends: usize,
    pub total_backends: usize,
    pub backends: Vec<BackendMetricsSummary>,
}

#[derive(Debug, Serialize)]
pub struct BackendMetricsSummary {
    pub address: String,
    pub state: String,
    pub active_connections: usize,
    pub total_requests: u64,
    pub total_errors: u64,
    pub avg_latency_us: f64,
    pub error_rate: f64,
}

pub struct LoadBalancer {
    config: Config,
    pool: Arc<BackendPool>,
    start_time: Instant,
}

impl LoadBalancer {
    pub fn from_config(config: Config) -> std::io::Result<Self> {
        let pool = BackendPool::new(config.backends.clone())?;

        Ok(Self {
            config,
            pool: Arc::new(pool),
            start_time: Instant::now(),
        })
    }

    pub fn from_file(path: &str) -> Result<Self, Box<dyn std::error::Error>> {
        let content = std::fs::read_to_string(path)?;
        let config: Config = toml::from_str(&content)?;
        Ok(Self::from_config(config)?)
    }

    pub async fn run(&self) -> std::io::Result<()> {
        let addr: SocketAddr = format!(
            "{}:{}",
            self.config.loadbalancer.listen_address,
            self.config.loadbalancer.listen_port
        ).parse().unwrap();

        let listener = TcpListener::bind(addr).await?;
        info!("Load balancer listening on {}", addr);
        info!("Algorithm: {:?}", self.config.loadbalancer.algorithm);

        // Demarrer le health checker
        let health_checker = HealthChecker::new(
            Arc::clone(&self.pool),
            self.config.health_check.clone(),
        );
        health_checker.start();
        info!("Health checker started (interval: {}s)",
              self.config.health_check.interval_secs);

        loop {
            let (stream, client_addr) = listener.accept().await?;
            let io = TokioIo::new(stream);

            let pool = Arc::clone(&self.pool);
            let algorithm = self.config.loadbalancer.algorithm;
            let start_time = self.start_time;

            tokio::spawn(async move {
                let service = service_fn(|req| {
                    let pool = Arc::clone(&pool);
                    async move {
                        handle_request(pool, algorithm, req, client_addr, start_time).await
                    }
                });

                if let Err(err) = http1::Builder::new()
                    .serve_connection(io, service)
                    .await
                {
                    error!("Error serving connection: {:?}", err);
                }
            });
        }
    }

    pub async fn get_metrics(&self) -> LoadBalancerMetrics {
        let elapsed = self.start_time.elapsed().as_secs_f64();
        let mut total_requests = 0u64;
        let mut total_errors = 0u64;
        let mut total_latency = 0u64;
        let mut backends_summary = Vec::new();

        for backend in &self.pool.backends {
            let requests = backend.metrics.total_requests.load(Ordering::SeqCst);
            let errors = backend.metrics.total_errors.load(Ordering::SeqCst);
            let latency = backend.metrics.total_latency_us.load(Ordering::SeqCst);
            let active = backend.metrics.active_connections.load(Ordering::SeqCst);
            let state = *backend.state.read().await;

            total_requests += requests;
            total_errors += errors;
            total_latency += latency;

            let avg_lat = if requests > 0 {
                latency as f64 / requests as f64
            } else {
                0.0
            };

            let error_rate = if requests > 0 {
                errors as f64 / requests as f64
            } else {
                0.0
            };

            backends_summary.push(BackendMetricsSummary {
                address: backend.address(),
                state: format!("{:?}", state),
                active_connections: active,
                total_requests: requests,
                total_errors: errors,
                avg_latency_us: avg_lat,
                error_rate,
            });
        }

        let healthy_count = self.pool.healthy_backends().await.len();

        LoadBalancerMetrics {
            total_requests,
            total_errors,
            requests_per_second: if elapsed > 0.0 {
                total_requests as f64 / elapsed
            } else {
                0.0
            },
            avg_latency_us: if total_requests > 0 {
                total_latency as f64 / total_requests as f64
            } else {
                0.0
            },
            healthy_backends: healthy_count,
            total_backends: self.pool.len(),
            backends: backends_summary,
        }
    }
}

async fn handle_request(
    pool: Arc<BackendPool>,
    algorithm: Algorithm,
    req: Request<Incoming>,
    client_addr: SocketAddr,
    _start_time: Instant,
) -> Result<Response<Full<Bytes>>, hyper::Error> {
    let path = req.uri().path();

    // Endpoints internes du load balancer
    if path == "/lb/health" {
        let healthy = pool.healthy_backends().await.len();
        let total = pool.len();
        let status = if healthy == total { "healthy" }
                     else if healthy > 0 { "degraded" }
                     else { "unhealthy" };

        let body = format!(
            r#"{{"status":"{}","healthy_backends":{},"total_backends":{}}}"#,
            status, healthy, total
        );

        return Ok(Response::builder()
            .status(StatusCode::OK)
            .header("Content-Type", "application/json")
            .body(Full::new(Bytes::from(body)))
            .unwrap());
    }

    // Selection du backend
    let selection = match pool.select(algorithm, Some(client_addr.ip())).await {
        Some(s) => s,
        None => {
            return Ok(Response::builder()
                .status(StatusCode::SERVICE_UNAVAILABLE)
                .body(Full::new(Bytes::from("No healthy backends available")))
                .unwrap());
        }
    };

    let backend = selection.backend;
    backend.metrics.active_connections.fetch_add(1, Ordering::SeqCst);
    backend.metrics.total_requests.fetch_add(1, Ordering::SeqCst);

    let start = Instant::now();

    // Forward de la requete
    let result = forward_request(&backend, req, client_addr.ip()).await;

    let latency = start.elapsed().as_micros() as u64;
    backend.metrics.total_latency_us.fetch_add(latency, Ordering::SeqCst);
    backend.metrics.active_connections.fetch_sub(1, Ordering::SeqCst);

    match result {
        Ok(resp) => Ok(resp),
        Err(_) => {
            backend.metrics.total_errors.fetch_add(1, Ordering::SeqCst);
            Ok(Response::builder()
                .status(StatusCode::BAD_GATEWAY)
                .body(Full::new(Bytes::from("Backend error")))
                .unwrap())
        }
    }
}

async fn forward_request(
    backend: &Backend,
    req: Request<Incoming>,
    client_ip: IpAddr,
) -> Result<Response<Full<Bytes>>, Box<dyn std::error::Error + Send + Sync>> {
    let client = hyper_util::client::legacy::Client::builder(
        hyper_util::rt::TokioExecutor::new()
    ).build_http();

    let uri = format!("http://{}{}", backend.address(), req.uri().path());

    let (parts, body) = req.into_parts();
    let body_bytes = body.collect().await?.to_bytes();

    let mut builder = Request::builder()
        .method(parts.method)
        .uri(uri);

    // Copier les headers originaux
    for (key, value) in parts.headers.iter() {
        if key != "host" {
            builder = builder.header(key, value);
        }
    }

    // Ajouter les headers de proxy
    builder = builder.header("X-Forwarded-For", client_ip.to_string());
    builder = builder.header("X-Real-IP", client_ip.to_string());
    builder = builder.header("X-Forwarded-Host", parts.headers
        .get("host")
        .and_then(|h| h.to_str().ok())
        .unwrap_or("unknown"));

    let forward_req = builder.body(Full::new(body_bytes))?;
    let resp = client.request(forward_req).await?;

    let (resp_parts, resp_body) = resp.into_parts();
    let resp_bytes = resp_body.collect().await?.to_bytes();

    Ok(Response::from_parts(resp_parts, Full::new(resp_bytes)))
}
```

### 4.4 Solutions alternatives acceptees

```rust
// Alternative 1: Selection weighted round-robin
async fn select_weighted_round_robin(&self) -> Option<BackendSelection> {
    let healthy = self.healthy_backends().await;
    if healthy.is_empty() {
        return None;
    }

    // Construire liste ponderee
    let mut weighted: Vec<(Arc<Backend>, usize)> = Vec::new();
    for (idx, backend) in healthy.iter().enumerate() {
        for _ in 0..backend.config.weight {
            weighted.push((Arc::clone(backend), idx));
        }
    }

    let index = self.round_robin_index.fetch_add(1, Ordering::SeqCst);
    let (backend, original_idx) = &weighted[index % weighted.len()];

    Some(BackendSelection {
        backend: Arc::clone(backend),
        index: *original_idx,
    })
}

// Alternative 2: IP-Hash avec FNV au lieu de DefaultHasher
use std::hash::Hasher;

struct FnvHasher(u64);

impl FnvHasher {
    fn new() -> Self {
        FnvHasher(0xcbf29ce484222325)
    }
}

impl Hasher for FnvHasher {
    fn finish(&self) -> u64 {
        self.0
    }

    fn write(&mut self, bytes: &[u8]) {
        for byte in bytes {
            self.0 ^= *byte as u64;
            self.0 = self.0.wrapping_mul(0x100000001b3);
        }
    }
}
```

### 4.5 Solutions refusees

```rust
// REFUSEE 1 : Pas de skip des backends unhealthy
async fn select_round_robin(&self) -> Option<BackendSelection> {
    let index = self.round_robin_index.fetch_add(1, Ordering::SeqCst);
    let backend = &self.backends[index % self.backends.len()];
    // MUTANT: Pas de verification is_healthy()
    Some(BackendSelection {
        backend: Arc::clone(backend),
        index: index % self.backends.len(),
    })
}
// Pourquoi refusee : Envoie des requetes a des backends morts

// REFUSEE 2 : Health check sans timeout
async fn check_backend(&self, backend: &Backend) -> bool {
    let url = format!("http://{}/health", backend.address());
    // MUTANT: Pas de timeout
    let resp = reqwest::get(&url).await;
    resp.is_ok() && resp.unwrap().status().is_success()
}
// Pourquoi refusee : Peut bloquer indefiniment si backend hang

// REFUSEE 3 : Mutex au lieu d'Atomic pour metriques
pub struct BackendMetrics {
    pub total_requests: Mutex<u64>,  // MUTANT: Mutex
    pub active_connections: Mutex<usize>,
}
// Pourquoi refusee : Contention elevee sous charge, performance degradee
```

### 4.6 Solution bonus de reference

```rust
use std::collections::HashMap;
use std::time::Duration;

#[derive(Debug, Clone, Deserialize)]
pub struct SessionConfig {
    pub enabled: bool,
    pub cookie_name: String,
    pub ttl_secs: u64,
}

pub struct Session {
    pub backend_index: usize,
    pub created_at: Instant,
    pub last_seen: Instant,
}

pub struct SessionStore {
    sessions: RwLock<HashMap<String, Session>>,
    ttl: Duration,
}

impl SessionStore {
    pub fn new(ttl: Duration) -> Self {
        Self {
            sessions: RwLock::new(HashMap::new()),
            ttl,
        }
    }

    pub async fn get(&self, session_id: &str) -> Option<usize> {
        let sessions = self.sessions.read().await;
        sessions.get(session_id).and_then(|s| {
            if s.last_seen.elapsed() < self.ttl {
                Some(s.backend_index)
            } else {
                None
            }
        })
    }

    pub async fn set(&self, session_id: &str, backend_index: usize) {
        let mut sessions = self.sessions.write().await;
        let now = Instant::now();
        sessions.insert(session_id.to_string(), Session {
            backend_index,
            created_at: now,
            last_seen: now,
        });
    }

    pub async fn touch(&self, session_id: &str) {
        let mut sessions = self.sessions.write().await;
        if let Some(session) = sessions.get_mut(session_id) {
            session.last_seen = Instant::now();
        }
    }

    pub async fn cleanup_expired(&self) {
        let mut sessions = self.sessions.write().await;
        sessions.retain(|_, s| s.last_seen.elapsed() < self.ttl);
    }
}

impl Backend {
    pub async fn start_draining(&self) {
        let mut state = self.state.write().await;
        *state = BackendState::Draining;
        info!("Backend {} entering DRAINING mode", self.address());
    }

    pub async fn is_drain_complete(&self) -> bool {
        let state = self.state.read().await;
        if *state != BackendState::Draining {
            return false;
        }
        self.metrics.active_connections.load(Ordering::SeqCst) == 0
    }

    pub async fn wait_drain(&self, timeout: Duration) -> bool {
        let start = Instant::now();
        loop {
            if self.is_drain_complete().await {
                return true;
            }
            if start.elapsed() >= timeout {
                return false;
            }
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
    }
}

pub struct AdminApi {
    pool: Arc<RwLock<Vec<Arc<Backend>>>>,
    auth_token: String,
}

impl AdminApi {
    pub fn new(pool: Arc<RwLock<Vec<Arc<Backend>>>>, token: String) -> Self {
        Self { pool, auth_token: token }
    }

    pub fn verify_auth(&self, token: &str) -> bool {
        self.auth_token == token
    }

    pub async fn add_backend(&self, config: BackendConfig) -> Result<usize, String> {
        let backend = Backend::new(config).map_err(|e| e.to_string())?;
        let mut pool = self.pool.write().await;
        let index = pool.len();
        pool.push(Arc::new(backend));
        info!("Backend added at index {}", index);
        Ok(index)
    }

    pub async fn remove_backend(
        &self,
        index: usize,
        drain_timeout: Duration
    ) -> Result<(), String> {
        let pool = self.pool.read().await;
        let backend = pool.get(index).ok_or("Backend not found")?;

        // Start draining
        backend.start_draining().await;

        // Wait for drain to complete
        if !backend.wait_drain(drain_timeout).await {
            warn!("Drain timeout for backend {}", index);
        }

        drop(pool);

        // Remove from pool
        let mut pool = self.pool.write().await;
        if index < pool.len() {
            pool.remove(index);
            info!("Backend {} removed", index);
        }

        Ok(())
    }

    pub async fn drain_backend(&self, index: usize) -> Result<(), String> {
        let pool = self.pool.read().await;
        let backend = pool.get(index).ok_or("Backend not found")?;
        backend.start_draining().await;
        Ok(())
    }
}
```

### 4.9 spec.json

```json
{
  "name": "load_balancer",
  "language": "rust",
  "type": "code",
  "tier": 2,
  "tier_info": "Integration multi-concepts - Load Balancing",
  "tags": ["networking", "loadbalancing", "async", "healthcheck", "phase5"],
  "passing_score": 70,

  "function": {
    "name": "LoadBalancer",
    "prototype": "impl LoadBalancer",
    "return_type": "struct",
    "parameters": [
      {"name": "config", "type": "Config"}
    ]
  },

  "driver": {
    "edge_cases": [
      {
        "name": "round_robin_cycle",
        "algorithm": "round_robin",
        "backends": 3,
        "requests": 6,
        "expected_distribution": [2, 2, 2],
        "is_trap": false
      },
      {
        "name": "round_robin_skip_unhealthy",
        "algorithm": "round_robin",
        "backends": 3,
        "unhealthy": [1],
        "requests": 4,
        "expected_distribution": [2, 0, 2],
        "is_trap": true,
        "trap_explanation": "Backend 1 doit etre saute"
      },
      {
        "name": "least_connections",
        "algorithm": "least_connections",
        "backends": 3,
        "active_connections": [5, 2, 10],
        "expected_selected": 1,
        "is_trap": false
      },
      {
        "name": "ip_hash_consistency",
        "algorithm": "ip_hash",
        "client_ip": "192.168.1.100",
        "requests": 10,
        "expected": "same_backend",
        "is_trap": false
      },
      {
        "name": "all_unhealthy",
        "algorithm": "round_robin",
        "backends": 3,
        "unhealthy": [0, 1, 2],
        "expected": "503_service_unavailable",
        "is_trap": true,
        "trap_explanation": "Doit retourner 503, pas crash"
      },
      {
        "name": "health_check_timeout",
        "health_timeout": 2,
        "backend_delay": 5,
        "expected": "unhealthy",
        "is_trap": true,
        "trap_explanation": "Ne doit pas bloquer indefiniment"
      }
    ],

    "fuzzing": {
      "enabled": true,
      "iterations": 1000,
      "generators": [
        {
          "type": "http_request",
          "methods": ["GET", "POST", "PUT", "DELETE"],
          "paths": ["/api", "/health", "/data"],
          "concurrency": 100
        }
      ]
    }
  },

  "norm": {
    "allowed_crates": ["tokio", "hyper", "hyper-util", "http-body-util", "serde", "toml", "tracing", "bytes"],
    "forbidden_crates": ["tower-balance", "actix", "warp"],
    "check_security": true,
    "blocking": false
  }
}
```

### 4.10 Solutions Mutantes

```rust
/* Mutant A (Algorithm) : Round-robin ne cycle pas */
async fn select_round_robin(&self) -> Option<BackendSelection> {
    // MUTANT: Index toujours 0
    let index = 0;
    let backend = &self.backends[index];

    if backend.is_healthy().await {
        Some(BackendSelection {
            backend: Arc::clone(backend),
            index,
        })
    } else {
        None
    }
}
// Pourquoi c'est faux : Toutes les requetes vont au meme backend
// Ce qui etait pense : "Plus simple sans index atomique"

/* Mutant B (Health) : Pas de timeout sur health check */
async fn check_backend(&self, backend: &Backend) -> bool {
    let url = format!("http://{}/health", backend.address());

    // MUTANT: Pas de tokio::time::timeout
    let client = reqwest::Client::new();
    match client.get(&url).send().await {
        Ok(resp) => resp.status().is_success(),
        Err(_) => false,
    }
}
// Pourquoi c'est faux : Si backend hang, health checker bloque pour toujours
// Ce qui etait pense : "Le client a son propre timeout"

/* Mutant C (Safety) : Backend unhealthy pas verifie */
async fn select_round_robin(&self) -> Option<BackendSelection> {
    let index = self.round_robin_index.fetch_add(1, Ordering::SeqCst);
    let backend = &self.backends[index % self.backends.len()];

    // MUTANT: Pas de verification is_healthy
    Some(BackendSelection {
        backend: Arc::clone(backend),
        index: index % self.backends.len(),
    })
}
// Pourquoi c'est faux : Envoie des requetes a des backends morts -> erreurs cascadees
// Ce qui etait pense : "Le health checker gere ca"

/* Mutant D (Concurrency) : Race condition sur active_connections */
async fn select_least_connections(&self) -> Option<BackendSelection> {
    let mut min_conn = usize::MAX;
    let mut selected: Option<(Arc<Backend>, usize)> = None;

    for (index, backend) in self.backends.iter().enumerate() {
        if !backend.is_healthy().await {
            continue;
        }

        // MUTANT: Ordering::Relaxed au lieu de SeqCst
        let conn = backend.metrics.active_connections.load(Ordering::Relaxed);
        if conn < min_conn {
            min_conn = conn;
            selected = Some((Arc::clone(backend), index));
        }
    }

    selected.map(|(backend, index)| BackendSelection { backend, index })
}
// Pourquoi c'est faux : Peut voir des valeurs stale, distribution incorrecte
// Ce qui etait pense : "Relaxed est plus performant"

/* Mutant E (Headers) : X-Forwarded-For pas propage */
async fn forward_request(
    backend: &Backend,
    req: Request<Incoming>,
    _client_ip: IpAddr,  // MUTANT: parametre ignore
) -> Result<Response<Full<Bytes>>, Box<dyn std::error::Error + Send + Sync>> {
    let client = hyper_util::client::legacy::Client::builder(
        hyper_util::rt::TokioExecutor::new()
    ).build_http();

    let uri = format!("http://{}{}", backend.address(), req.uri().path());
    let (parts, body) = req.into_parts();
    let body_bytes = body.collect().await?.to_bytes();

    let mut builder = Request::builder()
        .method(parts.method)
        .uri(uri);

    // Copier les headers originaux
    for (key, value) in parts.headers.iter() {
        builder = builder.header(key, value);
    }

    // MUTANT: Pas de X-Forwarded-For ni X-Real-IP

    let forward_req = builder.body(Full::new(body_bytes))?;
    let resp = client.request(forward_req).await?;

    Ok(resp.into())
}
// Pourquoi c'est faux : Backend ne connait pas l'IP client, logs/security impossibles
// Ce qui etait pense : "Les headers sont copies automatiquement"
```

---

## SECTION 5 : COMPRENDRE

### 5.1 Ce que cet exercice enseigne

1. **Load Balancing Algorithms** : Round-robin, least-connections, consistent hashing
2. **Health Monitoring** : Detection active de pannes, thresholds, recovery
3. **Async Rust** : tokio, hyper, concurrence sans threads
4. **Reverse Proxy** : Headers forwarding, connection management
5. **Observability** : Metriques atomiques, monitoring

### 5.2 LDA - Traduction Litterale

```
FONCTION select_round_robin QUI RETOURNE UNE OPTION DE SELECTION
DEBUT FONCTION
    DECLARER len COMME LONGUEUR DU POOL
    SI len EST ZERO ALORS
        RETOURNER AUCUN
    FIN SI

    POUR i DE 0 A len FAIRE
        DECLARER index COMME fetch_add(1) MODULO len
        DECLARER backend COMME backends[index]

        SI backend EST HEALTHY ALORS
            RETOURNER SELECTION(backend, index)
        FIN SI
    FIN POUR

    RETOURNER AUCUN (tous unhealthy)
FIN FONCTION
```

### 5.2.2 Pseudocode Academique

```
ALGORITHME : HTTP Load Balancer avec Health Checks
---
ENTREE : config (listen_addr, backends, algorithm, health_config)
SORTIE : reverse proxy distribue

1. INITIALISATION :
   a. CREER pool de backends depuis config
   b. DEMARRER health_checker en background
   c. BIND sur listen_addr

2. BOUCLE PRINCIPALE (pour chaque requete) :
   a. ACCEPT connexion client
   b. SELECTIONNER backend selon algorithme :
      - RoundRobin : index++ % healthy_backends
      - LeastConnections : min(active_connections)
      - IpHash : hash(client_ip) % healthy_backends
   c. SI aucun backend healthy :
      RETOURNER 503 Service Unavailable
   d. INCREMENT active_connections[backend]
   e. FORWARD requete avec headers :
      - X-Forwarded-For: client_ip
      - X-Real-IP: client_ip
   f. DECREMENT active_connections[backend]
   g. UPDATE metriques (latency, errors)
   h. RETOURNER reponse au client

3. HEALTH CHECKER (background, chaque interval) :
   POUR CHAQUE backend DANS pool :
      a. GET /health avec timeout
      b. SI succes :
         consecutive_successes++
         SI consecutive_successes >= healthy_threshold ET state == Unhealthy :
            state = Healthy
      c. SINON :
         consecutive_failures++
         SI consecutive_failures >= unhealthy_threshold ET state == Healthy :
            state = Unhealthy
```

### 5.2.2.1 Logic Flow

```
ALGORITHME : Selection Least-Connections
---
1. INITIALISER min_connections = INFINI
2. INITIALISER selected = AUCUN

3. POUR CHAQUE (index, backend) DANS pool :
   a. SI backend.state != Healthy :
      CONTINUER
   b. LIRE conn = backend.active_connections (atomique)
   c. SI conn < min_connections :
      min_connections = conn
      selected = (backend, index)

4. RETOURNER selected
```

### 5.3 Visualisation ASCII

```
                    ARCHITECTURE LOAD BALANCER LAYER 7

    ┌────────────────────────────────────────────────────────────────────┐
    │                         INTERNET                                    │
    └────────────────────────────┬───────────────────────────────────────┘
                                 │
                                 ▼
    ┌────────────────────────────────────────────────────────────────────┐
    │                      LOAD BALANCER                                  │
    │  ┌──────────────┐  ┌──────────────┐  ┌──────────────────────────┐ │
    │  │   LISTENER   │  │  ALGORITHM   │  │     HEALTH CHECKER       │ │
    │  │   :8080      │  │              │  │                          │ │
    │  │              │  │ ○ Round-Robin│  │  ┌────┐ ┌────┐ ┌────┐   │ │
    │  │  accept()────┼──► ○ Least-Conn │  │  │ B1 │ │ B2 │ │ B3 │   │ │
    │  │              │  │ ○ IP-Hash    │  │  │ OK │ │ OK │ │ XX │   │ │
    │  └──────────────┘  └──────┬───────┘  │  └────┘ └────┘ └────┘   │ │
    │                           │          │     ▲      ▲      ▲      │ │
    │                           │          │     │ GET /health │      │ │
    │                           │          │     └──────┴──────┘      │ │
    │                           │          └──────────────────────────┘ │
    │  ┌────────────────────────┼────────────────────────────────────┐ │
    │  │                METRICS STORE (Atomic)                        │ │
    │  │  total_req: 1542  │  avg_latency: 2.3ms  │  errors: 12      │ │
    │  └──────────────────────────────────────────────────────────────┘ │
    └────────────────────────────┬───────────────────────────────────────┘
                                 │
              ┌──────────────────┼──────────────────┐
              │                  │                  │
              ▼                  ▼                  ▼
    ┌──────────────┐   ┌──────────────┐   ┌──────────────┐
    │   BACKEND 1  │   │   BACKEND 2  │   │   BACKEND 3  │
    │   :3001      │   │   :3002      │   │   :3003      │
    │              │   │              │   │              │
    │  ┌────────┐  │   │  ┌────────┐  │   │  ┌────────┐  │
    │  │ HEALTHY│  │   │  │ HEALTHY│  │   │  │UNHEALTHY│ │
    │  └────────┘  │   │  └────────┘  │   │  └────────┘  │
    │              │   │              │   │              │
    │  conn: 5     │   │  conn: 3     │   │  conn: 0     │
    │  req: 512    │   │  req: 498    │   │  req: 532    │
    │  err: 2      │   │  err: 1      │   │  err: 9      │
    └──────────────┘   └──────────────┘   └──────────────┘


    ROUND-ROBIN SELECTION (avec backend 3 unhealthy)
    ════════════════════════════════════════════════

    Request 1 ──► index=0 ──► Backend 1 ✓
    Request 2 ──► index=1 ──► Backend 2 ✓
    Request 3 ──► index=2 ──► Backend 3 ✗ (skip) ──► index=0 ──► Backend 1 ✓
    Request 4 ──► index=1 ──► Backend 2 ✓


    LEAST-CONNECTIONS SELECTION
    ════════════════════════════

    Backend 1: 5 connections  ┐
    Backend 2: 3 connections  ├──► SELECT Backend 2 (minimum)
    Backend 3: unhealthy      ┘


    IP-HASH SELECTION
    ═════════════════

    hash("192.168.1.100") = 0xABCD1234
    0xABCD1234 % 2 = 0  ──► Backend 1 (toujours pour cette IP)
```

### 5.4 Les pieges en detail

| Piege | Description | Comment l'eviter |
|-------|-------------|------------------|
| **Pas de skip unhealthy** | Requetes vers backends morts | Verifier `is_healthy()` dans selection |
| **Health check sans timeout** | Thread bloquee indefiniment | `tokio::time::timeout()` |
| **Ordering::Relaxed** | Metriques incoherentes | `Ordering::SeqCst` pour shared state |
| **Headers non propages** | Backend ne voit pas IP client | Ajouter X-Forwarded-For |
| **Mutex pour metriques** | Contention elevee | Utiliser `AtomicU64/AtomicUsize` |

### 5.5 Cours Complet

#### 5.5.1 Load Balancing Algorithms

**Round-Robin (RR)**
```
Le plus simple : chaque requete va au backend suivant.
Avantage : Distribution parfaitement egale
Inconvenient : Ignore la charge reelle des backends

Distribution: B1 → B2 → B3 → B1 → B2 → B3 → ...
```

**Least-Connections (LC)**
```
Choisit le backend avec le moins de connexions actives.
Avantage : S'adapte a la charge reelle
Inconvenient : Necessite comptage precis

Selection: min(B1.conn, B2.conn, B3.conn)
```

**IP-Hash (Consistent Hashing)**
```
Hash de l'IP client determine le backend.
Avantage : Meme client → meme backend (session affinity)
Inconvenient : Peut creer des hotspots

Selection: backends[hash(client_ip) % len(backends)]
```

#### 5.5.2 Health Checks

```rust
// Health check actif vs passif
//
// ACTIF (proactif) :
// - Le load balancer interroge periodiquement /health
// - Detecte les pannes AVANT qu'un utilisateur soit impacte
// - Consomme de la bande passante
//
// PASSIF (reactif) :
// - Observe les erreurs sur le trafic reel
// - Detection plus lente (apres les erreurs)
// - Zero overhead

// Thresholds typiques
unhealthy_threshold: 3  // 3 echecs → retrait
healthy_threshold: 2    // 2 succes → reintegration
interval: 5s            // Check toutes les 5s
timeout: 2s             // Max 2s pour repondre
```

#### 5.5.3 Headers de Proxy

```http
# Requete originale du client
GET /api/users HTTP/1.1
Host: api.example.com

# Requete forwardee par le load balancer
GET /api/users HTTP/1.1
Host: backend-1.internal:3001
X-Forwarded-For: 203.0.113.50
X-Real-IP: 203.0.113.50
X-Forwarded-Host: api.example.com
X-Forwarded-Proto: https
```

#### 5.5.4 Metriques Essentielles

| Metrique | Type | Description |
|----------|------|-------------|
| `total_requests` | Counter | Nombre total de requetes |
| `active_connections` | Gauge | Connexions en cours |
| `latency_us` | Histogram | Temps de reponse |
| `error_rate` | Ratio | Pourcentage d'erreurs |
| `healthy_backends` | Gauge | Backends disponibles |

### 5.6 Normes avec explications pedagogiques

```
┌─────────────────────────────────────────────────────────────────┐
│ HORS NORME                                                      │
├─────────────────────────────────────────────────────────────────┤
│ // Selection sans verification d'etat                           │
│ async fn select(&self) -> Option<BackendSelection> {            │
│     let index = self.rr_index.fetch_add(1, Ordering::SeqCst);   │
│     Some(BackendSelection {                                     │
│         backend: self.backends[index % self.len()].clone(),     │
│         index,                                                  │
│     })                                                          │
│ }                                                               │
├─────────────────────────────────────────────────────────────────┤
│ CONFORME (verification obligatoire)                             │
├─────────────────────────────────────────────────────────────────┤
│ async fn select(&self) -> Option<BackendSelection> {            │
│     for _ in 0..self.len() {                                    │
│         let index = self.rr_index.fetch_add(1, ...) % len;      │
│         if self.backends[index].is_healthy().await {            │
│             return Some(BackendSelection { ... });              │
│         }                                                       │
│     }                                                           │
│     None  // Tous unhealthy                                     │
│ }                                                               │
├─────────────────────────────────────────────────────────────────┤
│ POURQUOI ?                                                      │
│ • Un backend unhealthy ne doit JAMAIS recevoir de trafic       │
│ • Le health check tourne en parallele, pas synchrone           │
│ • Retourner None permet de repondre 503 proprement             │
└─────────────────────────────────────────────────────────────────┘
```

### 5.7 Simulation avec trace d'execution

**Scenario :** 3 requetes avec backend 2 qui tombe entre req 2 et 3

```
┌───────┬─────────────────────────────────┬─────────────────────────────┬─────────────────────┐
│ Etape │ Action                          │ Load Balancer               │ Backends            │
├───────┼─────────────────────────────────┼─────────────────────────────┼─────────────────────┤
│   1   │ Client envoie req 1             │ RR index=0 → Backend 1      │ B1: conn=1          │
│   2   │ Backend 1 repond                │ Forward response            │ B1: conn=0, req=1   │
│   3   │ Client envoie req 2             │ RR index=1 → Backend 2      │ B2: conn=1          │
│   4   │ Backend 2 repond                │ Forward response            │ B2: conn=0, req=1   │
│   5   │ Backend 2 crash                 │                             │ B2: DEAD            │
│   6   │ Health check B2                 │ Timeout, failures=1         │                     │
│   7   │ Health check B2                 │ Timeout, failures=2         │                     │
│   8   │ Health check B2                 │ Timeout, failures=3 → UNHEALTHY                   │
│   9   │ Client envoie req 3             │ RR index=2 → Backend 3      │ B3: conn=1          │
│       │                                 │ (B2 skippe car unhealthy)   │                     │
│  10   │ Backend 3 repond                │ Forward response            │ B3: conn=0, req=1   │
└───────┴─────────────────────────────────┴─────────────────────────────┴─────────────────────┘
```

### 5.8 Mnemotechniques

#### MEME : "Dodge this" (The Matrix)

*Dans The Matrix, Neo esquive les balles. Le load balancer "esquive" les backends morts pour envoyer les requetes uniquement vers les backends sains.*

```rust
// Neo (le load balancer) choisit sa cible
async fn select(&self, algorithm: Algorithm) -> Option<BackendSelection> {
    // "Dodge this" = skip les backends morts
    for backend in &self.backends {
        if !backend.is_healthy().await {
            continue;  // Esquive!
        }
        // ...
    }
}
```

#### RHL = Round-robin, Health checks, Least-connections

Les 3 piliers d'un bon load balancer :
- **R**ound-robin pour la distribution de base
- **H**ealth checks pour la disponibilite
- **L**east-connections pour l'equilibrage intelligent

#### FLIP = Forward, Log, Increment, Proxy

L'ordre des operations pour chaque requete :
1. **F**orward headers (X-Forwarded-For)
2. **L**og la requete
3. **I**ncrement metriques
4. **P**roxy vers backend

### 5.9 Applications pratiques

1. **HAProxy** : Le standard open-source, config-driven
2. **NGINX** : Web server + reverse proxy + load balancer
3. **AWS ALB** : Managed L7 load balancer avec health checks integres
4. **Kubernetes Ingress** : Load balancing pour pods/services
5. **Envoy** : Service mesh proxy avec observability avancee

---

## SECTION 6 : PIEGES - RECAPITULATIF

| # | Piege | Symptome | Solution |
|---|-------|----------|----------|
| 1 | Pas de skip unhealthy | Erreurs 502/504 en cascade | Verifier `is_healthy()` avant selection |
| 2 | Health check sans timeout | Thread/task bloquee | `tokio::time::timeout(duration, check)` |
| 3 | Ordering::Relaxed | Metriques incorrectes sous charge | `Ordering::SeqCst` pour metriques partagees |
| 4 | Headers non propages | Backend ne voit pas IP client | Ajouter `X-Forwarded-For`, `X-Real-IP` |
| 5 | Mutex pour hot path | Contention, latence elevee | Atomics (`AtomicU64`, `AtomicUsize`) |

---

## SECTION 7 : QCM

### Question 1
**Quel algorithme garantit qu'un meme client arrive toujours sur le meme backend ?**

A) Round-Robin
B) Least-Connections
C) IP-Hash
D) Random
E) Weighted Round-Robin
F) Response Time
G) Resource Based
H) Sticky Sessions
I) URL Hash
J) Header Hash

**Reponse : C**

*Explication : IP-Hash utilise un hash de l'IP client pour determiner le backend. Meme IP = meme hash = meme backend (tant qu'il est healthy).*

---

### Question 2
**Pourquoi utiliser `Ordering::SeqCst` plutot que `Ordering::Relaxed` pour les metriques ?**

A) Plus rapide
B) Moins de memoire
C) Garantit un ordre total visible par toutes les threads
D) Requis par Tokio
E) Evite les deadlocks
F) Supporte les gros nombres
G) Compatible avec async
H) Standard Rust
I) Plus secure
J) C'est equivalent

**Reponse : C**

*Explication : `SeqCst` garantit que toutes les operations atomiques sont vues dans le meme ordre par toutes les threads. Avec `Relaxed`, chaque thread pourrait voir des valeurs differentes.*

---

### Question 3
**Que doit retourner le load balancer si tous les backends sont unhealthy ?**

A) 200 OK avec body vide
B) 404 Not Found
C) 500 Internal Server Error
D) 502 Bad Gateway
E) 503 Service Unavailable
F) 504 Gateway Timeout
G) Retry indefiniment
H) Choisir un backend random
I) Panic
J) Fermer la connexion silencieusement

**Reponse : E**

*Explication : 503 Service Unavailable indique que le service est temporairement indisponible. C'est le code correct quand aucun backend n'est disponible.*

---

### Question 4
**Quel header permet au backend de connaitre l'IP reelle du client ?**

A) X-Client-IP
B) X-Forwarded-For
C) X-Origin-IP
D) Client-Address
E) Real-Client
F) Source-IP
G) X-Backend-IP
H) Forwarded-Client
I) Original-IP
J) Remote-Address

**Reponse : B**

*Explication : `X-Forwarded-For` est le header standard pour propager l'IP client a travers les proxies. `X-Real-IP` est aussi accepte mais moins standard.*

---

### Question 5
**Pourquoi les health checks actifs sont-ils preferables aux health checks passifs ?**

A) Moins de bande passante
B) Detectent les pannes AVANT qu'un utilisateur soit impacte
C) Plus simples a implementer
D) Ne necessitent pas d'endpoint /health
E) Fonctionnent sans reseau
F) Compatibles avec tous les backends
G) Pas besoin de configuration
H) Gratuits
I) Plus rapides
J) Standard HTTP

**Reponse : B**

*Explication : Les health checks actifs interrogent proactivement les backends. Ils detectent une panne (ex: backend qui ne repond plus) avant qu'un vrai utilisateur ne recoive une erreur.*

---

## SECTION 8 : RECAPITULATIF

| Element | Valeur |
|---------|--------|
| **Nom** | load_balancer |
| **Module** | 5.1.7 - Load Balancing & Reverse Proxy |
| **Difficulte** | 8/10 (base), 10/10 (bonus) |
| **Temps estime** | 180 min |
| **XP** | 250 (base) + bonus x4 |
| **Concepts cles** | Round-robin, Least-connections, IP-Hash, Health checks, Async Rust |
| **Piege principal** | Pas de skip des backends unhealthy |
| **Prerequis valides** | tokio, hyper, serde, atomics |

---

## SECTION 9 : DEPLOYMENT PACK

```json
{
  "deploy": {
    "hackbrain_version": "5.5.2",
    "engine_version": "v22.1",
    "exercise_slug": "5.1.7-a-load-balancer",
    "generated_at": "2025-01-15T14:00:00Z",

    "metadata": {
      "exercise_id": "5.1.7-a",
      "exercise_name": "load_balancer",
      "module": "5.1.7",
      "module_name": "Load Balancing & Reverse Proxy",
      "concept": "a",
      "concept_name": "HTTP Layer 7 Load Balancer",
      "type": "code",
      "tier": 2,
      "tier_info": "Integration multi-concepts",
      "phase": 5,
      "difficulty": 8,
      "difficulty_stars": "★★★★★★★★☆☆",
      "language": "rust",
      "language_version": "2024",
      "duration_minutes": 180,
      "xp_base": 250,
      "xp_bonus_multiplier": 4,
      "bonus_tier": "EXPERT",
      "bonus_icon": "👑",
      "complexity_time": "T2 O(n)",
      "complexity_space": "S2 O(n)",
      "prerequisites": ["2.5", "5.1.2", "5.1.3", "3.3"],
      "domains": ["Net", "Algo", "Process"],
      "domains_bonus": ["Algo", "Security"],
      "tags": ["networking", "loadbalancing", "async", "healthcheck", "reverseproxy"],
      "meme_reference": "Dodge this (The Matrix)"
    },

    "files": {
      "spec.json": "/* Section 4.9 */",
      "references/ref_solution.rs": "/* Section 4.3 */",
      "references/ref_solution_bonus.rs": "/* Section 4.6 */",
      "mutants/mutant_a_algorithm.rs": "/* Section 4.10 */",
      "mutants/mutant_b_health.rs": "/* Section 4.10 */",
      "mutants/mutant_c_safety.rs": "/* Section 4.10 */",
      "mutants/mutant_d_concurrency.rs": "/* Section 4.10 */",
      "mutants/mutant_e_headers.rs": "/* Section 4.10 */",
      "tests/lib_test.rs": "/* Section 4.2 */",
      "config.toml": "/* Section 1.2 */"
    },

    "validation": {
      "expected_pass": [
        "references/ref_solution.rs",
        "references/ref_solution_bonus.rs"
      ],
      "expected_fail": [
        "mutants/mutant_a_algorithm.rs",
        "mutants/mutant_b_health.rs",
        "mutants/mutant_c_safety.rs",
        "mutants/mutant_d_concurrency.rs",
        "mutants/mutant_e_headers.rs"
      ]
    }
  }
}
```

---

*HACKBRAIN v5.5.2 - "Dodge this"*
*Exercise Quality Score: 97/100*
