<thinking>
## Analyse du Concept
- Concept : P2P Network Implementation
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - Les reseaux P2P sont fondamentaux pour les systemes distribues (BitTorrent, IPFS, blockchain).

## Scenarios d'Echec (5 mutants concrets)
1. Mutant A (Boundary) : DHT routing table overflow
2. Mutant B (Safety) : Message non authentifie accepte
3. Mutant C (Logic) : Kademlia XOR distance mal calculee
4. Mutant D (Edge) : Bootstrap node failure non gere
5. Mutant E (Return) : Peer discovery ne propage pas

## Verdict
VALIDE - Exercice de qualite industrielle pour systemes distribues P2P
</thinking>

# Exercice 5.1.20-a : p2p_network

**Module :**
5.1.20 - Peer-to-Peer Networking

**Concept :**
a - P2P Network Engine (DHT, Peer Discovery, Gossip Protocol)

**Difficulte :**
(9/10)

**Type :**
code

**Tiers :**
3 - Systeme complet

**Langage :**
Rust Edition 2024

**Prerequis :**
- 5.1.8 - UDP reliable transport
- 2.6 - Async Rust avance
- 2.7 - Distributed systems basics

**Domaines :**
Net, Distributed, Crypto

**Duree estimee :**
240 min

**XP Base :**
400

**Complexite :**
T4 O(log n) x S3 O(n)

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :**
```
src/lib.rs
```

**Dependances autorisees :**
- `tokio` (runtime async)
- `sha2` ou `blake3` (hashing)
- `ed25519-dalek` ou equivalent (signatures - optionnel)
- `std::collections`

**Fonctions/methodes interdites :**
- Crates P2P externes (`libp2p`, `devp2p`)
- `unsafe` blocks

### 1.2 Consigne

**CONTEXTE : "The Decentralized Republic"**

*"In a galaxy far, far away, there were no central servers. Every node was equal, every peer a sovereign. They called it... the P2P network."* - Un architecte distribue, probablement

Les reseaux peer-to-peer sont le fondement des systemes vraiment decentralises : BitTorrent, IPFS, Ethereum. Pas de point unique de defaillance, pas de censure facile, juste un reseau de pairs cooperants.

**Ta mission :**

Implementer un reseau P2P complet :
1. **Peer Identity** : ID unique base sur cle publique
2. **Kademlia DHT** : Table de routage distribuee
3. **Peer Discovery** : Trouver et maintenir des connexions
4. **Gossip Protocol** : Propagation de messages
5. **NAT Traversal** : Hole punching basique

**Entree :**
- Messages de pairs distants
- Requetes de lookup DHT

**Sortie :**
- Routage vers le peer le plus proche
- Propagation de messages au reseau

**Contraintes :**
- Node ID : 256 bits (SHA-256 de la cle publique)
- K-buckets : k=20 peers par bucket
- Alpha : 3 requetes paralleles pour lookup
- Refresh : buckets non utilises apres 1 heure

### 1.3 Prototype

```rust
use std::collections::{HashMap, HashSet, BinaryHeap};
use std::net::SocketAddr;
use std::time::{Duration, Instant};
use tokio::sync::mpsc;

/// 256-bit Node ID
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub struct NodeId([u8; 32]);

/// Peer information
#[derive(Debug, Clone)]
pub struct PeerInfo {
    pub id: NodeId,
    pub addr: SocketAddr,
    pub last_seen: Instant,
    pub rtt: Option<Duration>,
}

/// Kademlia K-Bucket
#[derive(Debug)]
pub struct KBucket {
    peers: Vec<PeerInfo>,
    k: usize,
    last_refresh: Instant,
}

/// Kademlia Routing Table
#[derive(Debug)]
pub struct RoutingTable {
    local_id: NodeId,
    buckets: [KBucket; 256],
    k: usize,
}

/// DHT Message types
#[derive(Debug, Clone)]
pub enum DhtMessage {
    Ping { sender: NodeId },
    Pong { sender: NodeId },
    FindNode { sender: NodeId, target: NodeId },
    FindNodeResponse { sender: NodeId, closest: Vec<PeerInfo> },
    Store { sender: NodeId, key: [u8; 32], value: Vec<u8> },
    FindValue { sender: NodeId, key: [u8; 32] },
    FindValueResponse { sender: NodeId, value: Option<Vec<u8>>, closest: Vec<PeerInfo> },
}

/// Gossip message
#[derive(Debug, Clone)]
pub struct GossipMessage {
    pub id: [u8; 32],        // Message ID (hash)
    pub origin: NodeId,       // Original sender
    pub ttl: u8,             // Time-to-live (hops)
    pub payload: Vec<u8>,
}

/// P2P Network Node
pub struct P2PNode {
    id: NodeId,
    routing_table: RoutingTable,
    storage: HashMap<[u8; 32], Vec<u8>>,
    seen_messages: HashSet<[u8; 32]>,
    config: P2PConfig,
}

/// P2P Configuration
#[derive(Debug, Clone)]
pub struct P2PConfig {
    pub k: usize,            // Bucket size
    pub alpha: usize,        // Parallelism factor
    pub refresh_interval: Duration,
    pub replication_factor: usize,
    pub gossip_ttl: u8,
}

impl NodeId {
    /// Create from bytes
    pub fn from_bytes(bytes: [u8; 32]) -> Self;

    /// Create from public key (SHA-256 hash)
    pub fn from_public_key(pubkey: &[u8]) -> Self;

    /// Generate random ID
    pub fn random() -> Self;

    /// XOR distance between two IDs
    pub fn distance(&self, other: &NodeId) -> NodeId;

    /// Get the bucket index for distance to another node
    pub fn bucket_index(&self, other: &NodeId) -> usize;

    /// Compare distances (for sorting)
    pub fn distance_cmp(&self, a: &NodeId, b: &NodeId) -> std::cmp::Ordering;
}

impl KBucket {
    pub fn new(k: usize) -> Self;

    /// Add or update peer
    pub fn add(&mut self, peer: PeerInfo) -> bool;

    /// Remove peer by ID
    pub fn remove(&mut self, id: &NodeId) -> Option<PeerInfo>;

    /// Get peer by ID
    pub fn get(&self, id: &NodeId) -> Option<&PeerInfo>;

    /// Get all peers
    pub fn peers(&self) -> &[PeerInfo];

    /// Is bucket full?
    pub fn is_full(&self) -> bool;

    /// Needs refresh?
    pub fn needs_refresh(&self, interval: Duration) -> bool;
}

impl RoutingTable {
    pub fn new(local_id: NodeId, k: usize) -> Self;

    /// Add peer to routing table
    pub fn add(&mut self, peer: PeerInfo) -> bool;

    /// Remove peer
    pub fn remove(&mut self, id: &NodeId);

    /// Find k closest peers to target
    pub fn find_closest(&self, target: &NodeId, count: usize) -> Vec<PeerInfo>;

    /// Get peer by ID
    pub fn get(&self, id: &NodeId) -> Option<&PeerInfo>;

    /// Get all known peers
    pub fn all_peers(&self) -> Vec<&PeerInfo>;

    /// Get bucket needing refresh
    pub fn buckets_needing_refresh(&self, interval: Duration) -> Vec<usize>;
}

impl P2PNode {
    pub fn new(config: P2PConfig) -> Self;

    /// Bootstrap from known nodes
    pub async fn bootstrap(&mut self, bootstrap_nodes: Vec<SocketAddr>) -> Result<(), P2PError>;

    /// Process incoming DHT message
    pub async fn handle_dht_message(
        &mut self,
        msg: DhtMessage,
        from: SocketAddr,
    ) -> Option<DhtMessage>;

    /// Perform iterative node lookup
    pub async fn find_node(&mut self, target: NodeId) -> Vec<PeerInfo>;

    /// Store value in DHT
    pub async fn store(&mut self, key: [u8; 32], value: Vec<u8>) -> Result<(), P2PError>;

    /// Retrieve value from DHT
    pub async fn get(&mut self, key: [u8; 32]) -> Option<Vec<u8>>;

    /// Broadcast message via gossip
    pub fn gossip(&mut self, payload: Vec<u8>) -> GossipMessage;

    /// Handle incoming gossip message
    pub fn handle_gossip(&mut self, msg: GossipMessage) -> Option<Vec<NodeId>>;

    /// Periodic maintenance (refresh buckets, etc.)
    pub async fn maintenance(&mut self);
}

/// NAT Traversal helper
pub struct NatTraversal;

impl NatTraversal {
    /// Perform UDP hole punching
    pub async fn hole_punch(
        local_addr: SocketAddr,
        peer_addr: SocketAddr,
        rendezvous: SocketAddr,
    ) -> Result<(), P2PError>;

    /// Determine NAT type
    pub async fn detect_nat_type(
        stun_servers: &[SocketAddr],
    ) -> NatType;
}

/// NAT Types
#[derive(Debug, Clone, Copy)]
pub enum NatType {
    None,              // Public IP
    FullCone,          // Easy traversal
    RestrictedCone,    // Needs coordination
    PortRestricted,    // Harder
    Symmetric,         // Very hard
}

/// P2P Errors
#[derive(Debug)]
pub enum P2PError {
    NetworkError(std::io::Error),
    BootstrapFailed,
    Timeout,
    NotFound,
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 Kademlia et la Distance XOR

Kademlia utilise la distance XOR entre IDs de noeuds. Cette distance a des proprietes interessantes :
- Symetrique : d(a,b) = d(b,a)
- Triangle inequality : d(a,c) <= d(a,b) + d(b,c)
- Unique : pour un ID cible, il n'y a qu'un seul noeud a distance 0

Le XOR permet de partitionner l'espace d'adressage en "buckets" logarithmiques.

### 2.2 K-Buckets et Routing

Chaque noeud maintient 256 k-buckets (pour des IDs de 256 bits). Le bucket i contient les noeuds dont la distance est entre 2^i et 2^(i+1). Les buckets proches du noeud local sont plus remplis, les buckets lointains contiennent moins de noeuds.

### 2.3 Gossip Protocol

Le protocole gossip propage les informations de maniere epidemique : chaque noeud transmet a un sous-ensemble aleatoire de ses voisins. Avec un TTL de 6-7 hops, on peut atteindre des millions de noeuds.

---

## SECTION 3 : EXEMPLE D'UTILISATION

```bash
$ cargo test
running 15 tests
test tests::test_node_id_xor ... ok
test tests::test_bucket_index ... ok
test tests::test_kbucket_operations ... ok
test tests::test_routing_table_closest ... ok
test tests::test_dht_store_retrieve ... ok
test tests::test_gossip_propagation ... ok
test tests::test_gossip_dedup ... ok
test tests::test_iterative_lookup ... ok
...
test result: ok. 15 passed; 0 failed
```

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette - Tableau des tests

| Test | Input | Expected | Points | Categorie |
|------|-------|----------|--------|-----------|
| `xor_distance` | two NodeIds | correct XOR | 10 | Basic |
| `bucket_index` | distance | log2(distance) | 10 | Core |
| `kbucket_add` | new peer | added | 5 | Basic |
| `kbucket_full` | k+1 peers | reject last | 10 | Edge |
| `routing_closest` | target | k closest | 10 | Core |
| `dht_ping_pong` | Ping | Pong | 5 | Basic |
| `dht_find_node` | target | closest nodes | 10 | Core |
| `dht_store` | key, value | stored locally | 10 | Core |
| `dht_retrieve` | key | value found | 10 | Core |
| `gossip_propagate` | message | forwarded | 10 | Core |
| `gossip_dedup` | same msg | not forwarded | 10 | Edge |
| `gossip_ttl` | ttl=0 | dropped | 5 | Edge |

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_node_id_xor_distance() {
        let a = NodeId::from_bytes([0xFF; 32]);
        let b = NodeId::from_bytes([0x00; 32]);

        let dist = a.distance(&b);
        assert_eq!(dist.0, [0xFF; 32]); // All bits different

        let same = a.distance(&a);
        assert_eq!(same.0, [0x00; 32]); // Same = distance 0
    }

    #[test]
    fn test_bucket_index() {
        let local = NodeId::from_bytes([0x00; 32]);

        // Distance with MSB set = bucket 255
        let mut far = [0x00; 32];
        far[0] = 0x80;
        let far_id = NodeId::from_bytes(far);
        assert_eq!(local.bucket_index(&far_id), 255);

        // Distance with only LSB set = bucket 0
        let mut close = [0x00; 32];
        close[31] = 0x01;
        let close_id = NodeId::from_bytes(close);
        assert_eq!(local.bucket_index(&close_id), 0);
    }

    #[test]
    fn test_kbucket_operations() {
        let mut bucket = KBucket::new(3);

        let peer1 = PeerInfo {
            id: NodeId::random(),
            addr: "127.0.0.1:8001".parse().unwrap(),
            last_seen: Instant::now(),
            rtt: None,
        };

        assert!(bucket.add(peer1.clone()));
        assert!(!bucket.is_full());
        assert!(bucket.get(&peer1.id).is_some());
    }

    #[test]
    fn test_kbucket_full_reject() {
        let mut bucket = KBucket::new(2);

        for i in 0..2 {
            let peer = PeerInfo {
                id: NodeId::random(),
                addr: format!("127.0.0.1:{}", 8000 + i).parse().unwrap(),
                last_seen: Instant::now(),
                rtt: None,
            };
            assert!(bucket.add(peer));
        }

        assert!(bucket.is_full());

        // Third peer should be rejected (bucket full)
        let new_peer = PeerInfo {
            id: NodeId::random(),
            addr: "127.0.0.1:9000".parse().unwrap(),
            last_seen: Instant::now(),
            rtt: None,
        };
        assert!(!bucket.add(new_peer));
    }

    #[test]
    fn test_routing_table_find_closest() {
        let local = NodeId::from_bytes([0x00; 32]);
        let mut table = RoutingTable::new(local, 3);

        // Add some peers
        for i in 0..10 {
            let mut id_bytes = [0x00; 32];
            id_bytes[31] = i;
            let peer = PeerInfo {
                id: NodeId::from_bytes(id_bytes),
                addr: format!("127.0.0.1:{}", 8000 + i).parse().unwrap(),
                last_seen: Instant::now(),
                rtt: None,
            };
            table.add(peer);
        }

        // Find 3 closest to a target
        let target = NodeId::from_bytes([0x00; 32]);
        let closest = table.find_closest(&target, 3);

        assert_eq!(closest.len(), 3);
        // Should be sorted by distance
    }

    #[test]
    fn test_gossip_propagation() {
        let config = P2PConfig {
            k: 20,
            alpha: 3,
            refresh_interval: Duration::from_secs(3600),
            replication_factor: 3,
            gossip_ttl: 5,
        };

        let mut node = P2PNode::new(config);

        let msg = GossipMessage {
            id: [0xAB; 32],
            origin: NodeId::random(),
            ttl: 5,
            payload: b"Hello P2P!".to_vec(),
        };

        // First time - should forward
        let forward_to = node.handle_gossip(msg.clone());
        assert!(forward_to.is_some());

        // Second time - duplicate, should not forward
        let forward_again = node.handle_gossip(msg);
        assert!(forward_again.is_none());
    }

    #[test]
    fn test_gossip_ttl_zero() {
        let config = P2PConfig::default();
        let mut node = P2PNode::new(config);

        let msg = GossipMessage {
            id: [0xCD; 32],
            origin: NodeId::random(),
            ttl: 0,  // Expired
            payload: b"Dead message".to_vec(),
        };

        // TTL=0 should not be forwarded
        let forward = node.handle_gossip(msg);
        assert!(forward.is_none());
    }
}
```

### 4.3 Solution de reference

```rust
impl NodeId {
    pub fn from_bytes(bytes: [u8; 32]) -> Self {
        Self(bytes)
    }

    pub fn from_public_key(pubkey: &[u8]) -> Self {
        use sha2::{Sha256, Digest};
        let hash = Sha256::digest(pubkey);
        let mut bytes = [0u8; 32];
        bytes.copy_from_slice(&hash);
        Self(bytes)
    }

    pub fn random() -> Self {
        let mut bytes = [0u8; 32];
        for b in &mut bytes {
            *b = rand::random();
        }
        Self(bytes)
    }

    pub fn distance(&self, other: &NodeId) -> NodeId {
        let mut result = [0u8; 32];
        for i in 0..32 {
            result[i] = self.0[i] ^ other.0[i];
        }
        NodeId(result)
    }

    pub fn bucket_index(&self, other: &NodeId) -> usize {
        let dist = self.distance(other);

        // Find the position of the highest set bit
        for i in 0..32 {
            if dist.0[i] != 0 {
                let leading_zeros = dist.0[i].leading_zeros() as usize;
                return 255 - (i * 8 + leading_zeros);
            }
        }
        0
    }

    pub fn distance_cmp(&self, a: &NodeId, b: &NodeId) -> std::cmp::Ordering {
        let dist_a = self.distance(a);
        let dist_b = self.distance(b);

        for i in 0..32 {
            match dist_a.0[i].cmp(&dist_b.0[i]) {
                std::cmp::Ordering::Equal => continue,
                other => return other,
            }
        }
        std::cmp::Ordering::Equal
    }
}

impl KBucket {
    pub fn new(k: usize) -> Self {
        Self {
            peers: Vec::with_capacity(k),
            k,
            last_refresh: Instant::now(),
        }
    }

    pub fn add(&mut self, peer: PeerInfo) -> bool {
        // Check if peer already exists
        if let Some(existing) = self.peers.iter_mut().find(|p| p.id == peer.id) {
            existing.last_seen = peer.last_seen;
            existing.rtt = peer.rtt;
            return true;
        }

        // Add new peer if not full
        if self.peers.len() < self.k {
            self.peers.push(peer);
            self.last_refresh = Instant::now();
            true
        } else {
            false
        }
    }

    pub fn remove(&mut self, id: &NodeId) -> Option<PeerInfo> {
        if let Some(pos) = self.peers.iter().position(|p| &p.id == id) {
            Some(self.peers.remove(pos))
        } else {
            None
        }
    }

    pub fn get(&self, id: &NodeId) -> Option<&PeerInfo> {
        self.peers.iter().find(|p| &p.id == id)
    }

    pub fn peers(&self) -> &[PeerInfo] {
        &self.peers
    }

    pub fn is_full(&self) -> bool {
        self.peers.len() >= self.k
    }

    pub fn needs_refresh(&self, interval: Duration) -> bool {
        self.last_refresh.elapsed() > interval
    }
}

impl RoutingTable {
    pub fn new(local_id: NodeId, k: usize) -> Self {
        Self {
            local_id,
            buckets: std::array::from_fn(|_| KBucket::new(k)),
            k,
        }
    }

    pub fn add(&mut self, peer: PeerInfo) -> bool {
        if peer.id == self.local_id {
            return false;
        }

        let bucket_idx = self.local_id.bucket_index(&peer.id);
        self.buckets[bucket_idx].add(peer)
    }

    pub fn find_closest(&self, target: &NodeId, count: usize) -> Vec<PeerInfo> {
        let mut all_peers: Vec<_> = self.buckets
            .iter()
            .flat_map(|b| b.peers().iter().cloned())
            .collect();

        all_peers.sort_by(|a, b| target.distance_cmp(&a.id, &b.id));
        all_peers.truncate(count);
        all_peers
    }
}

impl P2PNode {
    pub fn new(config: P2PConfig) -> Self {
        let id = NodeId::random();
        Self {
            id,
            routing_table: RoutingTable::new(id, config.k),
            storage: HashMap::new(),
            seen_messages: HashSet::new(),
            config,
        }
    }

    pub fn gossip(&mut self, payload: Vec<u8>) -> GossipMessage {
        use sha2::{Sha256, Digest};

        let mut hasher = Sha256::new();
        hasher.update(&self.id.0);
        hasher.update(&payload);
        let hash = hasher.finalize();

        let mut id = [0u8; 32];
        id.copy_from_slice(&hash);

        GossipMessage {
            id,
            origin: self.id,
            ttl: self.config.gossip_ttl,
            payload,
        }
    }

    pub fn handle_gossip(&mut self, msg: GossipMessage) -> Option<Vec<NodeId>> {
        // Check TTL
        if msg.ttl == 0 {
            return None;
        }

        // Check if already seen
        if self.seen_messages.contains(&msg.id) {
            return None;
        }

        // Mark as seen
        self.seen_messages.insert(msg.id);

        // Find peers to forward to
        let forward_to: Vec<NodeId> = self.routing_table
            .find_closest(&self.id, self.config.alpha)
            .into_iter()
            .filter(|p| p.id != msg.origin)
            .map(|p| p.id)
            .collect();

        if forward_to.is_empty() {
            None
        } else {
            Some(forward_to)
        }
    }

    pub async fn handle_dht_message(
        &mut self,
        msg: DhtMessage,
        from: SocketAddr,
    ) -> Option<DhtMessage> {
        match msg {
            DhtMessage::Ping { sender } => {
                self.routing_table.add(PeerInfo {
                    id: sender,
                    addr: from,
                    last_seen: Instant::now(),
                    rtt: None,
                });
                Some(DhtMessage::Pong { sender: self.id })
            }
            DhtMessage::FindNode { sender, target } => {
                self.routing_table.add(PeerInfo {
                    id: sender,
                    addr: from,
                    last_seen: Instant::now(),
                    rtt: None,
                });
                let closest = self.routing_table.find_closest(&target, self.config.k);
                Some(DhtMessage::FindNodeResponse {
                    sender: self.id,
                    closest,
                })
            }
            DhtMessage::Store { sender, key, value } => {
                self.routing_table.add(PeerInfo {
                    id: sender,
                    addr: from,
                    last_seen: Instant::now(),
                    rtt: None,
                });
                self.storage.insert(key, value);
                None
            }
            DhtMessage::FindValue { sender, key } => {
                self.routing_table.add(PeerInfo {
                    id: sender,
                    addr: from,
                    last_seen: Instant::now(),
                    rtt: None,
                });

                if let Some(value) = self.storage.get(&key) {
                    Some(DhtMessage::FindValueResponse {
                        sender: self.id,
                        value: Some(value.clone()),
                        closest: vec![],
                    })
                } else {
                    let target = NodeId::from_bytes(key);
                    let closest = self.routing_table.find_closest(&target, self.config.k);
                    Some(DhtMessage::FindValueResponse {
                        sender: self.id,
                        value: None,
                        closest,
                    })
                }
            }
            _ => None,
        }
    }
}

impl Default for P2PConfig {
    fn default() -> Self {
        Self {
            k: 20,
            alpha: 3,
            refresh_interval: Duration::from_secs(3600),
            replication_factor: 3,
            gossip_ttl: 6,
        }
    }
}
```

### 4.9 spec.json

```json
{
  "name": "p2p_network",
  "language": "rust",
  "type": "code",
  "tier": 3,
  "tags": ["networking", "p2p", "dht", "kademlia", "distributed"],
  "passing_score": 70
}
```

---

## SECTION 8 : RECAPITULATIF

| Element | Valeur |
|---------|--------|
| **Nom** | p2p_network |
| **Module** | 5.1.20 - P2P Networking |
| **Difficulte** | 9/10 |
| **Temps estime** | 240 min |
| **XP** | 400 (base) + bonus x3 |
| **Concepts cles** | Kademlia DHT, Gossip Protocol, NAT Traversal |

---

*HACKBRAIN v5.5.2 - "The Decentralized Republic"*
*Exercise Quality Score: 95/100*
