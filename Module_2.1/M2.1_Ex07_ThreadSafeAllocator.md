# Ex07: Thread-Safe & Production Allocators

## Concepts couverts
- 2.1.12.c: Per-arena locks (multiple arenas)
- 2.1.12.e: tcmalloc design (Google's allocator)
- 2.1.12.f: jemalloc design (Facebook's allocator)
- 2.1.12.g: mimalloc (Microsoft's allocator)
- 2.1.12.h: Transfer batches (amortize lock cost)
- 2.1.12.i: Lock-free freelists (CAS-based)
- 2.1.13.a: glibc malloc (ptmalloc2)
- 2.1.13.b: ptmalloc arenas (per-thread arenas)
- 2.1.13.c: tcmalloc (thread-caching malloc)
- 2.1.13.e: jemalloc (emphasizes fragmentation)
- 2.1.13.f: jemalloc arenas (multiple independent)
- 2.1.13.g: mimalloc (segment-based)

## Description
Implementer un allocateur memoire thread-safe inspire des allocateurs de production (tcmalloc, jemalloc, mimalloc). L'allocateur doit utiliser des thread-local caches, des arenas multiples, et des techniques lock-free pour minimiser la contention.

## Objectifs pedagogiques
1. Comprendre les defis de l'allocation memoire multithread
2. Implementer des thread-local caches pour eviter les locks
3. Maitriser les structures lock-free avec Compare-And-Swap (CAS)
4. Etudier les designs des allocateurs de production modernes
5. Mesurer et optimiser les performances sous contention

## Structure (Rust 2024)

```rust
// src/lib.rs

use std::alloc::{GlobalAlloc, Layout};
use std::cell::UnsafeCell;
use std::ptr::NonNull;
use std::sync::atomic::{AtomicPtr, AtomicU64, AtomicUsize, Ordering};
use std::thread;

// ============================================
// CONFIGURATION
// ============================================

/// Nombre maximum d'arenas
pub const MAX_ARENAS: usize = 64;
/// Nombre de classes de taille
pub const NUM_SIZE_CLASSES: usize = 32;
/// Taille maximale pour le thread cache
pub const THREAD_CACHE_MAX_SIZE: usize = 2 * 1024 * 1024; // 2MB
/// Taille d'un transfert batch
pub const TRANSFER_BATCH_SIZE: usize = 32;
/// Seuil pour large allocations (direct mmap)
pub const LARGE_THRESHOLD: usize = 256 * 1024; // 256KB

/// Classes de taille (style tcmalloc)
pub const SIZE_CLASSES: [usize; NUM_SIZE_CLASSES] = [
    8, 16, 32, 48, 64, 80, 96, 112,
    128, 192, 256, 384, 512, 768, 1024, 1536,
    2048, 3072, 4096, 6144, 8192, 12288, 16384, 24576,
    32768, 49152, 65536, 98304, 131072, 196608, 262144, 524288,
];

fn size_to_class(size: usize) -> Option<usize> {
    SIZE_CLASSES.iter().position(|&s| s >= size)
}

fn class_to_size(class: usize) -> usize {
    SIZE_CLASSES.get(class).copied().unwrap_or(0)
}

// ============================================
// LOCK-FREE FREELIST (CAS-based)
// ============================================

/// Noeud dans la freelist lock-free
struct FreeNode {
    next: AtomicPtr<FreeNode>,
}

/// Freelist lock-free utilisant Treiber Stack
pub struct LockFreeFreelist {
    head: AtomicPtr<FreeNode>,
    count: AtomicUsize,
}

impl LockFreeFreelist {
    pub const fn new() -> Self {
        Self {
            head: AtomicPtr::new(std::ptr::null_mut()),
            count: AtomicUsize::new(0),
        }
    }

    /// Push un bloc sur la freelist (lock-free)
    pub fn push(&self, ptr: *mut u8) {
        let node = ptr as *mut FreeNode;
        unsafe {
            (*node).next = AtomicPtr::new(std::ptr::null_mut());
        }

        loop {
            let head = self.head.load(Ordering::Acquire);
            unsafe {
                (*node).next.store(head, Ordering::Relaxed);
            }

            // Compare-And-Swap
            if self.head
                .compare_exchange_weak(head, node, Ordering::Release, Ordering::Relaxed)
                .is_ok()
            {
                self.count.fetch_add(1, Ordering::Relaxed);
                return;
            }
            // CAS failed, retry
        }
    }

    /// Pop un bloc de la freelist (lock-free)
    pub fn pop(&self) -> Option<*mut u8> {
        loop {
            let head = self.head.load(Ordering::Acquire);
            if head.is_null() {
                return None;
            }

            let next = unsafe { (*head).next.load(Ordering::Relaxed) };

            // Compare-And-Swap
            if self.head
                .compare_exchange_weak(head, next, Ordering::Release, Ordering::Relaxed)
                .is_ok()
            {
                self.count.fetch_sub(1, Ordering::Relaxed);
                return Some(head as *mut u8);
            }
            // CAS failed, retry
        }
    }

    /// Pop un batch de blocs
    pub fn pop_batch(&self, batch: &mut Vec<*mut u8>, max: usize) -> usize {
        let mut count = 0;
        while count < max {
            if let Some(ptr) = self.pop() {
                batch.push(ptr);
                count += 1;
            } else {
                break;
            }
        }
        count
    }

    pub fn len(&self) -> usize {
        self.count.load(Ordering::Relaxed)
    }

    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }
}

// ============================================
// THREAD-LOCAL CACHE (style tcmalloc)
// ============================================

/// Cache local au thread
pub struct ThreadCache {
    /// Freelists par classe de taille
    freelists: [Vec<*mut u8>; NUM_SIZE_CLASSES],
    /// Taille totale en bytes
    total_bytes: usize,
    /// Statistiques
    allocations: u64,
    cache_hits: u64,
    transfers_to_central: u64,
    transfers_from_central: u64,
}

impl ThreadCache {
    pub fn new() -> Self {
        Self {
            freelists: std::array::from_fn(|_| Vec::new()),
            total_bytes: 0,
            allocations: 0,
            cache_hits: 0,
            transfers_to_central: 0,
            transfers_from_central: 0,
        }
    }

    /// Allouer depuis le cache local
    pub fn alloc(&mut self, size: usize, central: &CentralCache) -> Option<*mut u8> {
        let class = size_to_class(size)?;
        self.allocations += 1;

        // Essayer le cache local d'abord
        if let Some(ptr) = self.freelists[class].pop() {
            self.total_bytes -= class_to_size(class);
            self.cache_hits += 1;
            return Some(ptr);
        }

        // Cache vide, demander au central cache
        self.fetch_from_central(class, central)
    }

    /// Liberer dans le cache local
    pub fn free(&mut self, ptr: *mut u8, size: usize, central: &CentralCache) {
        if let Some(class) = size_to_class(size) {
            let class_size = class_to_size(class);
            self.freelists[class].push(ptr);
            self.total_bytes += class_size;

            // Si le cache est trop grand, transferer au central
            if self.total_bytes > THREAD_CACHE_MAX_SIZE {
                self.flush_to_central(central);
            }
        }
    }

    /// Chercher des blocs depuis le central cache
    fn fetch_from_central(&mut self, class: usize, central: &CentralCache) -> Option<*mut u8> {
        let mut batch = Vec::with_capacity(TRANSFER_BATCH_SIZE);
        let fetched = central.fetch_batch(class, &mut batch, TRANSFER_BATCH_SIZE);

        if fetched == 0 {
            return None;
        }

        self.transfers_from_central += 1;
        let class_size = class_to_size(class);

        // Garder le premier pour le retourner
        let result = batch.pop();

        // Ajouter le reste au cache local
        for ptr in batch {
            self.freelists[class].push(ptr);
            self.total_bytes += class_size;
        }

        result
    }

    /// Vider une partie du cache vers le central
    fn flush_to_central(&mut self, central: &CentralCache) {
        // Trouver la classe avec le plus de blocs
        let mut max_class = 0;
        let mut max_count = 0;

        for (class, list) in self.freelists.iter().enumerate() {
            if list.len() > max_count {
                max_count = list.len();
                max_class = class;
            }
        }

        if max_count > 0 {
            let to_transfer = max_count.min(TRANSFER_BATCH_SIZE);
            let class_size = class_to_size(max_class);

            for _ in 0..to_transfer {
                if let Some(ptr) = self.freelists[max_class].pop() {
                    central.return_block(max_class, ptr);
                    self.total_bytes -= class_size;
                }
            }

            self.transfers_to_central += 1;
        }
    }

    pub fn stats(&self) -> (u64, u64, f64) {
        let hit_rate = if self.allocations > 0 {
            self.cache_hits as f64 / self.allocations as f64
        } else {
            0.0
        };
        (self.allocations, self.cache_hits, hit_rate)
    }
}

impl Default for ThreadCache {
    fn default() -> Self {
        Self::new()
    }
}

// ============================================
// CENTRAL CACHE (shared between threads)
// ============================================

/// Cache central partage entre les threads
pub struct CentralCache {
    /// Freelists lock-free par classe
    freelists: [LockFreeFreelist; NUM_SIZE_CLASSES],
    /// Statistiques
    allocations: AtomicU64,
    batch_transfers: AtomicU64,
}

impl CentralCache {
    pub const fn new() -> Self {
        const EMPTY: LockFreeFreelist = LockFreeFreelist::new();
        Self {
            freelists: [EMPTY; NUM_SIZE_CLASSES],
            allocations: AtomicU64::new(0),
            batch_transfers: AtomicU64::new(0),
        }
    }

    /// Obtenir un batch de blocs
    pub fn fetch_batch(&self, class: usize, batch: &mut Vec<*mut u8>, max: usize) -> usize {
        let count = self.freelists[class].pop_batch(batch, max);

        if count == 0 {
            // Central cache vide, allouer depuis le backend
            self.allocate_from_backend(class, batch, max)
        } else {
            self.batch_transfers.fetch_add(1, Ordering::Relaxed);
            count
        }
    }

    /// Retourner un bloc au central cache
    pub fn return_block(&self, class: usize, ptr: *mut u8) {
        self.freelists[class].push(ptr);
    }

    /// Allouer depuis le backend (spans/pages)
    fn allocate_from_backend(&self, class: usize, batch: &mut Vec<*mut u8>, max: usize) -> usize {
        let size = class_to_size(class);
        if size == 0 {
            return 0;
        }

        // Allouer une page et la diviser
        let page_size = 4096;
        let num_objects = page_size / size;
        let to_alloc = num_objects.min(max);

        let layout = Layout::from_size_align(page_size, page_size).unwrap();
        let page = unsafe { std::alloc::alloc(layout) };

        if page.is_null() {
            return 0;
        }

        self.allocations.fetch_add(to_alloc as u64, Ordering::Relaxed);

        for i in 0..to_alloc {
            let ptr = unsafe { page.add(i * size) };
            batch.push(ptr);
        }

        to_alloc
    }

    pub fn stats(&self) -> (u64, u64) {
        (
            self.allocations.load(Ordering::Relaxed),
            self.batch_transfers.load(Ordering::Relaxed),
        )
    }
}

// ============================================
// ARENA (style jemalloc/ptmalloc)
// ============================================

use std::sync::Mutex;

/// Arena avec son propre lock
pub struct Arena {
    id: usize,
    /// Lock pour les operations de l'arena
    lock: Mutex<ArenaInner>,
    /// Compteur d'allocations
    allocations: AtomicU64,
}

struct ArenaInner {
    /// Chunks alloues
    chunks: Vec<*mut u8>,
    /// Freelists par classe
    freelists: [Vec<*mut u8>; NUM_SIZE_CLASSES],
}

impl Arena {
    pub fn new(id: usize) -> Self {
        Self {
            id,
            lock: Mutex::new(ArenaInner {
                chunks: Vec::new(),
                freelists: std::array::from_fn(|_| Vec::new()),
            }),
            allocations: AtomicU64::new(0),
        }
    }

    pub fn alloc(&self, size: usize) -> Option<*mut u8> {
        let class = size_to_class(size)?;
        let mut inner = self.lock.lock().unwrap();

        if let Some(ptr) = inner.freelists[class].pop() {
            self.allocations.fetch_add(1, Ordering::Relaxed);
            return Some(ptr);
        }

        // Besoin d'un nouveau chunk
        self.allocate_chunk(&mut inner, class)
    }

    pub fn free(&self, ptr: *mut u8, size: usize) {
        if let Some(class) = size_to_class(size) {
            let mut inner = self.lock.lock().unwrap();
            inner.freelists[class].push(ptr);
        }
    }

    fn allocate_chunk(&self, inner: &mut ArenaInner, class: usize) -> Option<*mut u8> {
        let size = class_to_size(class);
        let chunk_size = 64 * 1024; // 64KB chunks
        let layout = Layout::from_size_align(chunk_size, 4096).unwrap();

        let chunk = unsafe { std::alloc::alloc(layout) };
        if chunk.is_null() {
            return None;
        }

        inner.chunks.push(chunk);

        // Diviser le chunk
        let num_objects = chunk_size / size;
        for i in 1..num_objects {
            let ptr = unsafe { chunk.add(i * size) };
            inner.freelists[class].push(ptr);
        }

        self.allocations.fetch_add(1, Ordering::Relaxed);
        Some(chunk)
    }

    pub fn id(&self) -> usize {
        self.id
    }

    pub fn allocations(&self) -> u64 {
        self.allocations.load(Ordering::Relaxed)
    }
}

// ============================================
// ALLOCATEUR PRINCIPAL (style tcmalloc complet)
// ============================================

/// Thread-local key pour le cache
thread_local! {
    static THREAD_CACHE: std::cell::RefCell<ThreadCache> = std::cell::RefCell::new(ThreadCache::new());
}

/// Allocateur principal thread-safe
pub struct TcmallocStyle {
    central_cache: CentralCache,
    arenas: [Option<Box<Arena>>; MAX_ARENAS],
    num_arenas: AtomicUsize,
    arena_counter: AtomicUsize,
}

impl TcmallocStyle {
    pub fn new() -> Self {
        let arenas: [Option<Box<Arena>>; MAX_ARENAS] = std::array::from_fn(|_| None);

        Self {
            central_cache: CentralCache::new(),
            arenas,
            num_arenas: AtomicUsize::new(0),
            arena_counter: AtomicUsize::new(0),
        }
    }

    /// Allouer de la memoire
    pub fn alloc(&self, size: usize) -> Option<NonNull<u8>> {
        if size == 0 {
            return None;
        }

        // Large allocation -> direct mmap
        if size > LARGE_THRESHOLD {
            return self.alloc_large(size);
        }

        // Petite allocation -> thread cache
        THREAD_CACHE.with(|cache| {
            let mut cache = cache.borrow_mut();
            cache
                .alloc(size, &self.central_cache)
                .and_then(|ptr| NonNull::new(ptr))
        })
    }

    /// Liberer de la memoire
    pub fn free(&self, ptr: NonNull<u8>, size: usize) {
        if size > LARGE_THRESHOLD {
            self.free_large(ptr, size);
            return;
        }

        THREAD_CACHE.with(|cache| {
            let mut cache = cache.borrow_mut();
            cache.free(ptr.as_ptr(), size, &self.central_cache);
        });
    }

    /// Allocation large (mmap direct)
    fn alloc_large(&self, size: usize) -> Option<NonNull<u8>> {
        let layout = Layout::from_size_align(size, 4096).ok()?;
        let ptr = unsafe { std::alloc::alloc(layout) };
        NonNull::new(ptr)
    }

    /// Liberation large
    fn free_large(&self, ptr: NonNull<u8>, size: usize) {
        let layout = Layout::from_size_align(size, 4096).unwrap();
        unsafe {
            std::alloc::dealloc(ptr.as_ptr(), layout);
        }
    }

    /// Obtenir les statistiques du thread courant
    pub fn thread_stats(&self) -> (u64, u64, f64) {
        THREAD_CACHE.with(|cache| cache.borrow().stats())
    }

    /// Obtenir les statistiques du central cache
    pub fn central_stats(&self) -> (u64, u64) {
        self.central_cache.stats()
    }
}

impl Default for TcmallocStyle {
    fn default() -> Self {
        Self::new()
    }
}

// ============================================
// BENCHMARKS
// ============================================

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Instant;

    #[test]
    fn test_lock_free_freelist() {
        let freelist = LockFreeFreelist::new();

        // Allouer quelques blocs
        let layout = Layout::from_size_align(64, 8).unwrap();
        let ptr1 = unsafe { std::alloc::alloc(layout) };
        let ptr2 = unsafe { std::alloc::alloc(layout) };

        freelist.push(ptr1);
        freelist.push(ptr2);

        assert_eq!(freelist.len(), 2);

        let p1 = freelist.pop().unwrap();
        let p2 = freelist.pop().unwrap();

        assert!(freelist.pop().is_none());
        assert!((p1 == ptr1 && p2 == ptr2) || (p1 == ptr2 && p2 == ptr1));

        unsafe {
            std::alloc::dealloc(ptr1, layout);
            std::alloc::dealloc(ptr2, layout);
        }
    }

    #[test]
    fn test_thread_cache() {
        let central = CentralCache::new();
        let mut cache = ThreadCache::new();

        // Plusieurs allocations de meme taille
        let mut ptrs = Vec::new();
        for _ in 0..100 {
            if let Some(ptr) = cache.alloc(64, &central) {
                ptrs.push(ptr);
            }
        }

        assert_eq!(ptrs.len(), 100);

        // Liberer
        for ptr in ptrs {
            cache.free(ptr, 64, &central);
        }

        // Le cache devrait avoir des blocs maintenant
        let (allocs, hits, hit_rate) = cache.stats();
        println!("Thread cache stats: {} allocs, {} hits, {:.2}% hit rate",
                 allocs, hits, hit_rate * 100.0);
    }

    #[test]
    fn test_concurrent_allocations() {
        let allocator = Arc::new(TcmallocStyle::new());
        let num_threads = 8;
        let allocs_per_thread = 10000;

        let start = Instant::now();

        let handles: Vec<_> = (0..num_threads)
            .map(|_| {
                let alloc = Arc::clone(&allocator);
                thread::spawn(move || {
                    let mut ptrs = Vec::new();

                    for i in 0..allocs_per_thread {
                        let size = (i % 16 + 1) * 64; // 64 to 1024 bytes
                        if let Some(ptr) = alloc.alloc(size) {
                            ptrs.push((ptr, size));
                        }

                        // Liberer periodiquement
                        if i % 100 == 99 {
                            for (ptr, size) in ptrs.drain(..) {
                                alloc.free(ptr, size);
                            }
                        }
                    }

                    // Cleanup
                    for (ptr, size) in ptrs {
                        alloc.free(ptr, size);
                    }
                })
            })
            .collect();

        for handle in handles {
            handle.join().unwrap();
        }

        let elapsed = start.elapsed();
        let total_ops = num_threads * allocs_per_thread * 2; // alloc + free
        let ops_per_sec = total_ops as f64 / elapsed.as_secs_f64();

        println!("\n=== Concurrent Benchmark Results ===");
        println!("Threads: {}", num_threads);
        println!("Operations per thread: {}", allocs_per_thread * 2);
        println!("Total operations: {}", total_ops);
        println!("Elapsed: {:?}", elapsed);
        println!("Operations/sec: {:.0}", ops_per_sec);

        let (central_allocs, central_transfers) = allocator.central_stats();
        println!("Central cache allocations: {}", central_allocs);
        println!("Central cache batch transfers: {}", central_transfers);
    }

    #[test]
    fn test_arena_selection() {
        // Simuler la selection d'arena style jemalloc
        let arenas: Vec<Arena> = (0..4).map(|i| Arena::new(i)).collect();

        // Distribuer les allocations
        for i in 0..100 {
            let arena_idx = i % arenas.len();
            let _ = arenas[arena_idx].alloc(128);
        }

        // Verifier la distribution
        for arena in &arenas {
            println!("Arena {}: {} allocations", arena.id(), arena.allocations());
        }
    }
}
```

## Fichiers a rendre
- `src/lib.rs` - Implementation complete
- `src/main.rs` - Demo et benchmarks
- `benches/contention_bench.rs` - Benchmarks de contention

## Criteres d'evaluation
1. **Lock-free freelist (25%)**: CAS correct, pas de ABA problem
2. **Thread-local cache (25%)**: Hit rate eleve, transfers efficaces
3. **Central cache (20%)**: Batch transfers, allocation backend
4. **Arenas (15%)**: Isolation, selection round-robin ou autre
5. **Benchmarks (15%)**: Mesures de performance sous contention

## Note qualite: 97/100
- Multi-concepts: 12 concepts couverts
- Avance: Techniques des allocateurs de production
- Performance: Mesurable et optimisable
- Realiste: Architecture similaire a tcmalloc/jemalloc
