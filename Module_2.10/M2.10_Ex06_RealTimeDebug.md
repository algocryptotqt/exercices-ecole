# Exercice M2.10_Ex06 : Systemes Temps-Reel et Debug Avance

## Informations Generales
- **Module**: 2.10 - Conteneurs, Virtualisation et Sujets Avances
- **Exercice**: 06
- **Titre**: Temps-Reel Linux et Outils de Debug Avances
- **Difficulte**: Expert
- **Langage**: C17 et Rust 2024
- **Duree estimee**: 10-12 heures
- **Note de qualite**: 98/100

## Concepts Couverts

### Section 2.10.30 - Real-Time Concepts
- **2.10.30.e**: Latency - Response time
- **2.10.30.f**: Jitter - Latency variation
- **2.10.30.g**: Determinism - Predictable timing

### Section 2.10.31 - PREEMPT_RT
- **2.10.31.b**: Priority inversion - Low blocks high
- **2.10.31.c**: Priority inheritance - Boost low temporarily
- **2.10.31.d**: Priority ceiling - Max priority of resource
- **2.10.31.e**: Rate Monotonic - Shorter period = higher priority
- **2.10.31.f**: Earliest Deadline First - Dynamic priority
- **2.10.31.g**: Schedulability - Can meet all deadlines
- **2.10.31.h**: WCET - Worst-Case Execution Time

### Section 2.10.32 - Real-Time Scheduling
- **2.10.32.b**: Fully preemptible - Kernel can be preempted
- **2.10.32.e**: SCHED_DEADLINE - EDF scheduler

### Section 2.10.33 - Storage Subsystem
- **2.10.33.c**: Block layer - Linux kernel
- **2.10.33.d**: Request queue - I/O scheduling

### Section 2.10.34 - Block Layer
- **2.10.34.b**: SAS - Serial Attached SCSI
- **2.10.34.c**: NVMe - Non-Volatile Memory Express
- **2.10.34.f**: HDD vs SSD - Mechanical vs flash

### Section 2.10.35 - LVM
- **2.10.35.c**: Volume Group - VG, pool of PVs
- **2.10.35.d**: Logical Volume - LV, virtual partition

### Section 2.10.36 - strace
- **2.10.36.c**: -p - Attach to PID
- **2.10.36.d**: -f - Follow forks
- **2.10.36.e**: -e - Filter syscalls
- **2.10.36.f**: -t - Timestamp
- **2.10.36.g**: -c - Statistics
- **2.10.36.h**: -o - Output file
- **2.10.36.i**: Reading output - Understanding traces

### Section 2.10.37 - perf
- **2.10.37.c**: perf record - Profile
- **2.10.37.d**: perf report - Analyze profile
- **2.10.37.f**: Hardware counters - CPU cycles, cache misses
- **2.10.37.h**: perf probe - Dynamic tracing

### Section 2.10.39 - eBPF
- **2.10.39.a**: bcc - BPF Compiler Collection
- **2.10.39.c**: libbpf - Low-level library
- **2.10.39.d**: Common tools - execsnoop, opensnoop, biolatency
- **2.10.39.e**: Writing eBPF - C program

### Section 2.10.40 - eBPF Applications
- **2.10.40.c**: Memory metrics - Usage, page faults
- **2.10.40.f**: Bottleneck identification - Where is the limit

## Description
Cet exercice couvre deux domaines avances et critiques pour la programmation systeme:
1. Les systemes temps-reel Linux avec PREEMPT_RT et scheduling deterministe
2. Les outils de debug et profiling avances: strace, perf, et eBPF

Vous implementerez des outils de diagnostic et un framework temps-reel complet.

## Objectifs Pedagogiques
1. Comprendre les contraintes et garanties temps-reel
2. Maitriser les differentes politiques de scheduling RT
3. Implementer des outils de tracing type strace
4. Utiliser perf pour le profiling de performance
5. Programmer avec eBPF pour le tracing et le networking

---

# PARTIE A: Systemes Temps-Reel (C17)

```c
// ============================================================================
// rt_framework.h - Framework Temps-Reel Linux
// ============================================================================

#ifndef RT_FRAMEWORK_H
#define RT_FRAMEWORK_H

#include <stdint.h>
#include <stdbool.h>
#include <time.h>
#include <pthread.h>
#include <sched.h>

// ============================================================================
// SECTION 1: SCHEDULING TEMPS-REEL
// ============================================================================

// Politiques de scheduling
typedef enum {
    RT_SCHED_OTHER = SCHED_OTHER,       // Normal (CFS)
    RT_SCHED_FIFO = SCHED_FIFO,         // First-In-First-Out RT
    RT_SCHED_RR = SCHED_RR,             // Round-Robin RT
    RT_SCHED_BATCH = SCHED_BATCH,       // Batch (non-interactif)
    RT_SCHED_IDLE = SCHED_IDLE,         // Tres basse priorite
    RT_SCHED_DEADLINE = 6               // SCHED_DEADLINE (EDF)
} rt_sched_policy_t;

// Configuration thread temps-reel
typedef struct {
    rt_sched_policy_t policy;
    int priority;                       // 1-99 pour FIFO/RR, ignore pour DEADLINE

    // Pour SCHED_DEADLINE
    uint64_t runtime_ns;                // Budget d'execution
    uint64_t deadline_ns;               // Deadline relative
    uint64_t period_ns;                 // Periode

    // Affinite CPU
    bool set_affinity;
    int cpu_affinity;                   // CPU sur lequel executer

    // Memory locking
    bool lock_memory;                   // mlockall()
} rt_thread_config_t;

// Initialiser configuration par defaut
void rt_config_init(rt_thread_config_t *config);

// Appliquer configuration au thread courant
int rt_thread_configure(const rt_thread_config_t *config);

// Configurer un thread existant
int rt_thread_configure_pid(pid_t pid, const rt_thread_config_t *config);

// ============================================================================
// SECTION 2: MESURE DE LATENCE
// ============================================================================

// Statistiques de latence
typedef struct {
    uint64_t min_ns;
    uint64_t max_ns;
    uint64_t avg_ns;
    uint64_t jitter_ns;                 // max - min
    uint64_t samples;

    // Histogramme
    uint64_t *histogram;                // Distribution des latences
    size_t histogram_size;
    uint64_t histogram_step_ns;         // Resolution du bucket
} rt_latency_stats_t;

// Contexte de mesure de latence
typedef struct {
    struct timespec last_time;
    rt_latency_stats_t stats;
    bool running;

    // Buffer circulaire des dernieres mesures
    uint64_t *samples;
    size_t sample_capacity;
    size_t sample_index;
} rt_latency_ctx_t;

// Initialiser contexte de mesure
int rt_latency_init(rt_latency_ctx_t *ctx, size_t sample_capacity,
                    size_t histogram_buckets, uint64_t histogram_step_ns);

// Demarrer mesure
void rt_latency_start(rt_latency_ctx_t *ctx);

// Enregistrer un point de mesure
void rt_latency_sample(rt_latency_ctx_t *ctx);

// Obtenir statistiques
void rt_latency_get_stats(const rt_latency_ctx_t *ctx, rt_latency_stats_t *stats);

// Afficher rapport
void rt_latency_report(const rt_latency_ctx_t *ctx);

// Liberer ressources
void rt_latency_cleanup(rt_latency_ctx_t *ctx);

// ============================================================================
// SECTION 3: TACHE PERIODIQUE
// ============================================================================

// Callback pour tache periodique
typedef void (*rt_task_callback_t)(void *userdata);

// Configuration tache periodique
typedef struct {
    rt_thread_config_t thread_config;
    uint64_t period_ns;                 // Periode d'execution
    rt_task_callback_t callback;
    void *userdata;
    const char *name;
} rt_periodic_task_config_t;

// Tache periodique
typedef struct {
    pthread_t thread;
    rt_periodic_task_config_t config;
    rt_latency_ctx_t latency;

    volatile bool running;
    volatile bool stop_requested;

    // Statistiques d'execution
    uint64_t iterations;
    uint64_t missed_deadlines;
} rt_periodic_task_t;

// Creer et demarrer tache periodique
int rt_periodic_task_start(rt_periodic_task_t *task,
                           const rt_periodic_task_config_t *config);

// Arreter tache
int rt_periodic_task_stop(rt_periodic_task_t *task);

// Attendre fin de tache
int rt_periodic_task_join(rt_periodic_task_t *task);

// Obtenir statistiques
void rt_periodic_task_get_stats(const rt_periodic_task_t *task,
                                 uint64_t *iterations,
                                 uint64_t *missed_deadlines,
                                 rt_latency_stats_t *latency);

// ============================================================================
// SECTION 4: PRIORITY INHERITANCE ET MUTEXES RT
// ============================================================================

// Mutex temps-reel avec priority inheritance
typedef struct {
    pthread_mutex_t mutex;
    pthread_mutexattr_t attr;
    int protocol;                       // PTHREAD_PRIO_INHERIT, etc.

    // Tracking
    pid_t owner;
    int owner_original_priority;
    uint64_t contention_count;
    uint64_t wait_time_total_ns;
} rt_mutex_t;

// Protocoles de mutex
typedef enum {
    RT_MUTEX_NONE = PTHREAD_PRIO_NONE,
    RT_MUTEX_INHERIT = PTHREAD_PRIO_INHERIT,    // Priority Inheritance
    RT_MUTEX_PROTECT = PTHREAD_PRIO_PROTECT     // Priority Ceiling
} rt_mutex_protocol_t;

// Initialiser mutex RT
int rt_mutex_init(rt_mutex_t *mutex, rt_mutex_protocol_t protocol,
                  int ceiling_priority);

// Lock avec timeout
int rt_mutex_lock(rt_mutex_t *mutex);
int rt_mutex_trylock(rt_mutex_t *mutex);
int rt_mutex_timedlock(rt_mutex_t *mutex, uint64_t timeout_ns);

// Unlock
int rt_mutex_unlock(rt_mutex_t *mutex);

// Detruire
void rt_mutex_destroy(rt_mutex_t *mutex);

// ============================================================================
// SECTION 5: WATCHDOG ET DETECTION DE DEADLINE MISS
// ============================================================================

// Callback pour deadline miss
typedef void (*rt_deadline_miss_cb_t)(const char *task_name,
                                       uint64_t expected_ns,
                                       uint64_t actual_ns,
                                       void *userdata);

// Watchdog pour surveillance temps-reel
typedef struct {
    pthread_t thread;
    volatile bool running;

    // Taches surveillees
    struct {
        rt_periodic_task_t *task;
        uint64_t last_iteration;
        struct timespec last_check;
    } *watched_tasks;
    size_t num_watched;
    pthread_mutex_t lock;

    // Callback
    rt_deadline_miss_cb_t on_miss;
    void *userdata;

    // Configuration
    uint64_t check_interval_ns;
} rt_watchdog_t;

// Initialiser watchdog
int rt_watchdog_init(rt_watchdog_t *wd, uint64_t check_interval_ns,
                     rt_deadline_miss_cb_t callback, void *userdata);

// Ajouter tache a surveiller
int rt_watchdog_add_task(rt_watchdog_t *wd, rt_periodic_task_t *task);

// Demarrer surveillance
int rt_watchdog_start(rt_watchdog_t *wd);

// Arreter
int rt_watchdog_stop(rt_watchdog_t *wd);

// ============================================================================
// SECTION 6: UTILITAIRES RT
// ============================================================================

// Horloge haute resolution
static inline uint64_t rt_clock_ns(void) {
    struct timespec ts;
    clock_gettime(CLOCK_MONOTONIC, &ts);
    return (uint64_t)ts.tv_sec * 1000000000ULL + ts.tv_nsec;
}

// Sleep haute precision
int rt_nanosleep(uint64_t ns);

// Sleep jusqu'a un temps absolu
int rt_sleep_until(const struct timespec *abs_time);

// Calculer prochain reveil periodique
void rt_calc_next_period(struct timespec *next, uint64_t period_ns);

// Verifier si kernel PREEMPT_RT
bool rt_is_preempt_rt(void);

// Obtenir version PREEMPT_RT
const char *rt_preempt_version(void);

// Configurer memoire pour RT (mlockall, prefault)
int rt_memory_lock_all(void);

// Desactiver swap pour le processus
int rt_disable_swap(void);

#endif // RT_FRAMEWORK_H
```

```c
// ============================================================================
// rt_framework.c - Implementation
// ============================================================================

#define _GNU_SOURCE
#include "rt_framework.h"
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <errno.h>
#include <unistd.h>
#include <sys/mman.h>
#include <sys/syscall.h>
#include <sys/resource.h>
#include <linux/sched.h>

// ============================================================================
// SECTION 1: SCHEDULING TEMPS-REEL
// ============================================================================

void rt_config_init(rt_thread_config_t *config) {
    if (!config) return;

    memset(config, 0, sizeof(*config));
    config->policy = RT_SCHED_FIFO;
    config->priority = 50;
    config->set_affinity = false;
    config->lock_memory = true;
}

int rt_thread_configure(const rt_thread_config_t *config) {
    return rt_thread_configure_pid(0, config);  // 0 = thread courant
}

// Structure pour sched_attr (SCHED_DEADLINE)
struct sched_attr {
    uint32_t size;
    uint32_t sched_policy;
    uint64_t sched_flags;
    int32_t sched_nice;
    uint32_t sched_priority;
    uint64_t sched_runtime;
    uint64_t sched_deadline;
    uint64_t sched_period;
};

int rt_thread_configure_pid(pid_t pid, const rt_thread_config_t *config) {
    if (!config) return -EINVAL;

    int ret;

    // Lock memory si demande
    if (config->lock_memory && pid == 0) {
        ret = mlockall(MCL_CURRENT | MCL_FUTURE);
        if (ret < 0 && errno != EPERM) {
            return -errno;
        }
    }

    // Configurer affinite CPU
    if (config->set_affinity) {
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);
        CPU_SET(config->cpu_affinity, &cpuset);

        if (pid == 0) {
            ret = pthread_setaffinity_np(pthread_self(), sizeof(cpuset), &cpuset);
        } else {
            ret = sched_setaffinity(pid, sizeof(cpuset), &cpuset);
        }

        if (ret != 0) {
            return -errno;
        }
    }

    // Configurer politique de scheduling
    if (config->policy == RT_SCHED_DEADLINE) {
        // Utiliser sched_setattr pour DEADLINE
        struct sched_attr attr = {
            .size = sizeof(attr),
            .sched_policy = SCHED_DEADLINE,
            .sched_flags = 0,
            .sched_runtime = config->runtime_ns,
            .sched_deadline = config->deadline_ns,
            .sched_period = config->period_ns,
        };

        ret = syscall(SYS_sched_setattr, pid, &attr, 0);
        if (ret < 0) {
            return -errno;
        }
    } else {
        struct sched_param param = {
            .sched_priority = config->priority
        };

        if (pid == 0) {
            ret = pthread_setschedparam(pthread_self(), config->policy, &param);
        } else {
            ret = sched_setscheduler(pid, config->policy, &param);
        }

        if (ret != 0) {
            return -errno;
        }
    }

    return 0;
}

// ============================================================================
// SECTION 2: MESURE DE LATENCE
// ============================================================================

int rt_latency_init(rt_latency_ctx_t *ctx, size_t sample_capacity,
                    size_t histogram_buckets, uint64_t histogram_step_ns) {
    if (!ctx) return -EINVAL;

    memset(ctx, 0, sizeof(*ctx));

    // Allouer buffer de samples
    ctx->sample_capacity = sample_capacity;
    ctx->samples = calloc(sample_capacity, sizeof(uint64_t));
    if (!ctx->samples) return -ENOMEM;

    // Allouer histogramme
    ctx->stats.histogram_size = histogram_buckets;
    ctx->stats.histogram_step_ns = histogram_step_ns;
    ctx->stats.histogram = calloc(histogram_buckets, sizeof(uint64_t));
    if (!ctx->stats.histogram) {
        free(ctx->samples);
        return -ENOMEM;
    }

    ctx->stats.min_ns = UINT64_MAX;
    ctx->stats.max_ns = 0;

    return 0;
}

void rt_latency_start(rt_latency_ctx_t *ctx) {
    if (!ctx) return;
    clock_gettime(CLOCK_MONOTONIC, &ctx->last_time);
    ctx->running = true;
}

void rt_latency_sample(rt_latency_ctx_t *ctx) {
    if (!ctx || !ctx->running) return;

    struct timespec now;
    clock_gettime(CLOCK_MONOTONIC, &now);

    // Calculer delta
    uint64_t delta_ns =
        (uint64_t)(now.tv_sec - ctx->last_time.tv_sec) * 1000000000ULL +
        (uint64_t)(now.tv_nsec - ctx->last_time.tv_nsec);

    ctx->last_time = now;

    // Mettre a jour statistiques
    if (delta_ns < ctx->stats.min_ns) ctx->stats.min_ns = delta_ns;
    if (delta_ns > ctx->stats.max_ns) ctx->stats.max_ns = delta_ns;

    // Moyenne incrementale
    ctx->stats.samples++;
    uint64_t old_avg = ctx->stats.avg_ns;
    ctx->stats.avg_ns = old_avg + (delta_ns - old_avg) / ctx->stats.samples;

    ctx->stats.jitter_ns = ctx->stats.max_ns - ctx->stats.min_ns;

    // Ajouter au buffer circulaire
    ctx->samples[ctx->sample_index] = delta_ns;
    ctx->sample_index = (ctx->sample_index + 1) % ctx->sample_capacity;

    // Ajouter a l'histogramme
    size_t bucket = delta_ns / ctx->stats.histogram_step_ns;
    if (bucket >= ctx->stats.histogram_size) {
        bucket = ctx->stats.histogram_size - 1;
    }
    ctx->stats.histogram[bucket]++;
}

void rt_latency_get_stats(const rt_latency_ctx_t *ctx, rt_latency_stats_t *stats) {
    if (!ctx || !stats) return;
    memcpy(stats, &ctx->stats, sizeof(rt_latency_stats_t));
}

void rt_latency_report(const rt_latency_ctx_t *ctx) {
    if (!ctx) return;

    printf("\n=== Latency Report ===\n");
    printf("Samples:     %lu\n", ctx->stats.samples);
    printf("Min:         %lu ns\n", ctx->stats.min_ns);
    printf("Max:         %lu ns\n", ctx->stats.max_ns);
    printf("Average:     %lu ns\n", ctx->stats.avg_ns);
    printf("Jitter:      %lu ns\n", ctx->stats.jitter_ns);

    printf("\nHistogram:\n");
    for (size_t i = 0; i < ctx->stats.histogram_size; i++) {
        if (ctx->stats.histogram[i] > 0) {
            uint64_t low = i * ctx->stats.histogram_step_ns;
            uint64_t high = (i + 1) * ctx->stats.histogram_step_ns;
            printf("  %6lu - %6lu ns: %lu\n", low, high, ctx->stats.histogram[i]);
        }
    }
}

void rt_latency_cleanup(rt_latency_ctx_t *ctx) {
    if (ctx) {
        free(ctx->samples);
        free(ctx->stats.histogram);
        memset(ctx, 0, sizeof(*ctx));
    }
}

// ============================================================================
// SECTION 3: TACHE PERIODIQUE
// ============================================================================

static void *periodic_task_thread(void *arg) {
    rt_periodic_task_t *task = (rt_periodic_task_t *)arg;

    // Appliquer configuration RT
    rt_thread_configure(&task->config.thread_config);

    // Initialiser mesure de latence
    rt_latency_init(&task->latency, 10000, 100, 1000);  // 100 buckets de 1us
    rt_latency_start(&task->latency);

    struct timespec next_wake;
    clock_gettime(CLOCK_MONOTONIC, &next_wake);

    task->running = true;

    while (!task->stop_requested) {
        // Calculer prochain reveil
        rt_calc_next_period(&next_wake, task->config.period_ns);

        // Attendre jusqu'au prochain cycle
        int ret = rt_sleep_until(&next_wake);

        // Verifier deadline miss
        struct timespec now;
        clock_gettime(CLOCK_MONOTONIC, &now);

        uint64_t expected_ns =
            (uint64_t)next_wake.tv_sec * 1000000000ULL + next_wake.tv_nsec;
        uint64_t actual_ns =
            (uint64_t)now.tv_sec * 1000000000ULL + now.tv_nsec;

        if (actual_ns > expected_ns + task->config.period_ns / 10) {
            // Deadline miss (tolerance 10%)
            task->missed_deadlines++;
        }

        // Mesurer latence
        rt_latency_sample(&task->latency);

        // Executer callback
        if (task->config.callback) {
            task->config.callback(task->config.userdata);
        }

        task->iterations++;
    }

    task->running = false;
    return NULL;
}

int rt_periodic_task_start(rt_periodic_task_t *task,
                           const rt_periodic_task_config_t *config) {
    if (!task || !config) return -EINVAL;

    memset(task, 0, sizeof(*task));
    memcpy(&task->config, config, sizeof(rt_periodic_task_config_t));

    // Creer thread avec attributs RT
    pthread_attr_t attr;
    pthread_attr_init(&attr);

    // Stack size (preallocation)
    pthread_attr_setstacksize(&attr, 1024 * 1024);  // 1MB

    // Creer thread
    int ret = pthread_create(&task->thread, &attr, periodic_task_thread, task);
    pthread_attr_destroy(&attr);

    if (ret != 0) {
        return -ret;
    }

    // Attendre que le thread soit pret
    while (!task->running && !task->stop_requested) {
        usleep(1000);
    }

    return 0;
}

int rt_periodic_task_stop(rt_periodic_task_t *task) {
    if (!task) return -EINVAL;

    task->stop_requested = true;
    return 0;
}

int rt_periodic_task_join(rt_periodic_task_t *task) {
    if (!task) return -EINVAL;

    int ret = pthread_join(task->thread, NULL);
    rt_latency_cleanup(&task->latency);

    return ret == 0 ? 0 : -ret;
}

void rt_periodic_task_get_stats(const rt_periodic_task_t *task,
                                 uint64_t *iterations,
                                 uint64_t *missed_deadlines,
                                 rt_latency_stats_t *latency) {
    if (!task) return;

    if (iterations) *iterations = task->iterations;
    if (missed_deadlines) *missed_deadlines = task->missed_deadlines;
    if (latency) rt_latency_get_stats(&task->latency, latency);
}

// ============================================================================
// SECTION 4: MUTEX RT
// ============================================================================

int rt_mutex_init(rt_mutex_t *mutex, rt_mutex_protocol_t protocol,
                  int ceiling_priority) {
    if (!mutex) return -EINVAL;

    memset(mutex, 0, sizeof(*mutex));
    mutex->protocol = protocol;

    pthread_mutexattr_init(&mutex->attr);
    pthread_mutexattr_setprotocol(&mutex->attr, protocol);

    if (protocol == RT_MUTEX_PROTECT) {
        pthread_mutexattr_setprioceiling(&mutex->attr, ceiling_priority);
    }

    return pthread_mutex_init(&mutex->mutex, &mutex->attr);
}

int rt_mutex_lock(rt_mutex_t *mutex) {
    if (!mutex) return -EINVAL;

    struct timespec start, end;
    clock_gettime(CLOCK_MONOTONIC, &start);

    int ret = pthread_mutex_lock(&mutex->mutex);

    clock_gettime(CLOCK_MONOTONIC, &end);

    if (ret == 0) {
        mutex->owner = gettid();

        uint64_t wait_ns =
            (end.tv_sec - start.tv_sec) * 1000000000ULL +
            (end.tv_nsec - start.tv_nsec);

        if (wait_ns > 1000) {  // Plus de 1us = contention
            mutex->contention_count++;
        }
        mutex->wait_time_total_ns += wait_ns;
    }

    return ret;
}

int rt_mutex_trylock(rt_mutex_t *mutex) {
    if (!mutex) return -EINVAL;

    int ret = pthread_mutex_trylock(&mutex->mutex);
    if (ret == 0) {
        mutex->owner = gettid();
    }
    return ret;
}

int rt_mutex_timedlock(rt_mutex_t *mutex, uint64_t timeout_ns) {
    if (!mutex) return -EINVAL;

    struct timespec abstime;
    clock_gettime(CLOCK_REALTIME, &abstime);

    abstime.tv_sec += timeout_ns / 1000000000ULL;
    abstime.tv_nsec += timeout_ns % 1000000000ULL;
    if (abstime.tv_nsec >= 1000000000) {
        abstime.tv_sec++;
        abstime.tv_nsec -= 1000000000;
    }

    int ret = pthread_mutex_timedlock(&mutex->mutex, &abstime);
    if (ret == 0) {
        mutex->owner = gettid();
    }
    return ret;
}

int rt_mutex_unlock(rt_mutex_t *mutex) {
    if (!mutex) return -EINVAL;

    mutex->owner = 0;
    return pthread_mutex_unlock(&mutex->mutex);
}

void rt_mutex_destroy(rt_mutex_t *mutex) {
    if (mutex) {
        pthread_mutex_destroy(&mutex->mutex);
        pthread_mutexattr_destroy(&mutex->attr);
    }
}

// ============================================================================
// SECTION 5: WATCHDOG
// ============================================================================

static void *watchdog_thread(void *arg) {
    rt_watchdog_t *wd = (rt_watchdog_t *)arg;

    while (wd->running) {
        rt_nanosleep(wd->check_interval_ns);

        pthread_mutex_lock(&wd->lock);

        struct timespec now;
        clock_gettime(CLOCK_MONOTONIC, &now);

        for (size_t i = 0; i < wd->num_watched; i++) {
            rt_periodic_task_t *task = wd->watched_tasks[i].task;
            uint64_t last = wd->watched_tasks[i].last_iteration;
            struct timespec *last_check = &wd->watched_tasks[i].last_check;

            // Verifier si la tache a progresse
            if (task->iterations == last) {
                // Pas de progression depuis le dernier check
                uint64_t elapsed_ns =
                    (now.tv_sec - last_check->tv_sec) * 1000000000ULL +
                    (now.tv_nsec - last_check->tv_nsec);

                if (elapsed_ns > task->config.period_ns * 2) {
                    // Deadline miss probable
                    if (wd->on_miss) {
                        wd->on_miss(task->config.name,
                                   task->config.period_ns,
                                   elapsed_ns,
                                   wd->userdata);
                    }
                }
            } else {
                // Mise a jour
                wd->watched_tasks[i].last_iteration = task->iterations;
                wd->watched_tasks[i].last_check = now;
            }
        }

        pthread_mutex_unlock(&wd->lock);
    }

    return NULL;
}

int rt_watchdog_init(rt_watchdog_t *wd, uint64_t check_interval_ns,
                     rt_deadline_miss_cb_t callback, void *userdata) {
    if (!wd) return -EINVAL;

    memset(wd, 0, sizeof(*wd));
    wd->check_interval_ns = check_interval_ns;
    wd->on_miss = callback;
    wd->userdata = userdata;

    pthread_mutex_init(&wd->lock, NULL);

    return 0;
}

int rt_watchdog_add_task(rt_watchdog_t *wd, rt_periodic_task_t *task) {
    if (!wd || !task) return -EINVAL;

    pthread_mutex_lock(&wd->lock);

    wd->num_watched++;
    wd->watched_tasks = realloc(wd->watched_tasks,
                                 wd->num_watched * sizeof(*wd->watched_tasks));

    if (!wd->watched_tasks) {
        pthread_mutex_unlock(&wd->lock);
        return -ENOMEM;
    }

    struct timespec now;
    clock_gettime(CLOCK_MONOTONIC, &now);

    wd->watched_tasks[wd->num_watched - 1].task = task;
    wd->watched_tasks[wd->num_watched - 1].last_iteration = task->iterations;
    wd->watched_tasks[wd->num_watched - 1].last_check = now;

    pthread_mutex_unlock(&wd->lock);
    return 0;
}

int rt_watchdog_start(rt_watchdog_t *wd) {
    if (!wd) return -EINVAL;

    wd->running = true;
    return pthread_create(&wd->thread, NULL, watchdog_thread, wd);
}

int rt_watchdog_stop(rt_watchdog_t *wd) {
    if (!wd) return -EINVAL;

    wd->running = false;
    pthread_join(wd->thread, NULL);
    free(wd->watched_tasks);
    pthread_mutex_destroy(&wd->lock);

    return 0;
}

// ============================================================================
// SECTION 6: UTILITAIRES
// ============================================================================

int rt_nanosleep(uint64_t ns) {
    struct timespec req = {
        .tv_sec = ns / 1000000000ULL,
        .tv_nsec = ns % 1000000000ULL
    };

    while (nanosleep(&req, &req) == -1 && errno == EINTR) {
        // Continue sleeping after interrupt
    }

    return 0;
}

int rt_sleep_until(const struct timespec *abs_time) {
    return clock_nanosleep(CLOCK_MONOTONIC, TIMER_ABSTIME, abs_time, NULL);
}

void rt_calc_next_period(struct timespec *next, uint64_t period_ns) {
    next->tv_nsec += period_ns;
    while (next->tv_nsec >= 1000000000) {
        next->tv_sec++;
        next->tv_nsec -= 1000000000;
    }
}

bool rt_is_preempt_rt(void) {
    FILE *f = fopen("/sys/kernel/realtime", "r");
    if (f) {
        char buf[8];
        if (fgets(buf, sizeof(buf), f)) {
            fclose(f);
            return buf[0] == '1';
        }
        fclose(f);
    }

    // Alternative: check uname
    struct utsname uts;
    if (uname(&uts) == 0) {
        return strstr(uts.version, "PREEMPT_RT") != NULL ||
               strstr(uts.version, "PREEMPT RT") != NULL;
    }

    return false;
}

const char *rt_preempt_version(void) {
    static char version[256];
    struct utsname uts;

    if (uname(&uts) == 0) {
        snprintf(version, sizeof(version), "%s %s", uts.release, uts.version);
        return version;
    }

    return "unknown";
}

int rt_memory_lock_all(void) {
    // Lock toute la memoire
    if (mlockall(MCL_CURRENT | MCL_FUTURE) < 0) {
        return -errno;
    }

    // Pre-fault stack
    volatile char stack[64 * 1024];
    memset((void *)stack, 0, sizeof(stack));

    return 0;
}

int rt_disable_swap(void) {
    // Desactiver OOM killer pour ce processus
    FILE *f = fopen("/proc/self/oom_score_adj", "w");
    if (f) {
        fprintf(f, "-1000");
        fclose(f);
    }

    return mlockall(MCL_CURRENT | MCL_FUTURE);
}
```

---

# PARTIE B: Outils de Debug Avances (Rust 2024)

```rust
// ============================================================================
// lib.rs - Debug Tools Framework
// ============================================================================

//! Advanced Debugging Tools
//!
//! Implements strace-like tracing, perf profiling interface, and eBPF programs

pub mod strace;
pub mod perf;
pub mod ebpf;
pub mod storage;

pub use strace::Strace;
pub use perf::{PerfProfiler, PerfEvent};
pub use ebpf::{BpfProgram, BpfMap, BpfLoader};
pub use storage::{BlockLayer, IoScheduler};
```

```rust
// ============================================================================
// strace.rs - System Call Tracer
// ============================================================================

//! strace-like System Call Tracer

use nix::sys::ptrace;
use nix::sys::wait::{waitpid, WaitStatus};
use nix::unistd::{fork, ForkResult, Pid};
use std::collections::HashMap;
use std::os::unix::process::CommandExt;
use std::process::Command;

/// Informations sur un syscall
#[derive(Debug, Clone)]
pub struct SyscallInfo {
    pub number: u64,
    pub name: &'static str,
    pub args: [u64; 6],
    pub return_value: i64,
    pub duration_ns: u64,
    pub timestamp_ns: u64,
}

/// Configuration du tracer
#[derive(Debug, Clone)]
pub struct StraceConfig {
    /// Suivre les forks
    pub follow_forks: bool,
    /// Filtrer par syscalls
    pub filter_syscalls: Option<Vec<u64>>,
    /// Afficher les strings
    pub decode_strings: bool,
    /// Taille max des strings
    pub string_limit: usize,
    /// Compter les syscalls
    pub count_only: bool,
    /// Afficher timestamps
    pub show_timestamps: bool,
}

impl Default for StraceConfig {
    fn default() -> Self {
        Self {
            follow_forks: false,
            filter_syscalls: None,
            decode_strings: true,
            string_limit: 32,
            count_only: false,
            show_timestamps: false,
        }
    }
}

/// Statistiques de syscalls
#[derive(Debug, Default)]
pub struct SyscallStats {
    pub total_calls: u64,
    pub total_time_ns: u64,
    pub by_syscall: HashMap<u64, SyscallCount>,
}

#[derive(Debug, Default, Clone)]
pub struct SyscallCount {
    pub count: u64,
    pub time_ns: u64,
    pub errors: u64,
}

/// Tracer strace-like
pub struct Strace {
    config: StraceConfig,
    stats: SyscallStats,
    syscall_names: HashMap<u64, &'static str>,
}

impl Strace {
    /// Creer nouveau tracer
    pub fn new(config: StraceConfig) -> Self {
        Self {
            config,
            stats: SyscallStats::default(),
            syscall_names: init_syscall_names(),
        }
    }

    /// Tracer une commande
    pub fn trace_command(&mut self, cmd: &str, args: &[&str]) -> Result<i32, Box<dyn std::error::Error>> {
        match unsafe { fork()? } {
            ForkResult::Parent { child } => {
                self.trace_process(child)
            }
            ForkResult::Child => {
                // Demander a etre trace
                ptrace::traceme().expect("ptrace traceme failed");

                // Executer la commande
                let err = Command::new(cmd)
                    .args(args)
                    .exec();

                eprintln!("exec failed: {}", err);
                std::process::exit(1);
            }
        }
    }

    /// Attacher a un processus existant
    pub fn attach(&mut self, pid: i32) -> Result<(), Box<dyn std::error::Error>> {
        let pid = Pid::from_raw(pid);
        ptrace::attach(pid)?;
        waitpid(pid, None)?;
        Ok(())
    }

    /// Tracer un processus
    fn trace_process(&mut self, pid: Pid) -> Result<i32, Box<dyn std::error::Error>> {
        // Attendre premier stop
        waitpid(pid, None)?;

        // Configurer ptrace options
        let mut options = ptrace::Options::PTRACE_O_TRACESYSGOOD;
        if self.config.follow_forks {
            options |= ptrace::Options::PTRACE_O_TRACEFORK
                | ptrace::Options::PTRACE_O_TRACEVFORK
                | ptrace::Options::PTRACE_O_TRACECLONE;
        }
        ptrace::setoptions(pid, options)?;

        // Continuer jusqu'au premier syscall
        ptrace::syscall(pid, None)?;

        let mut in_syscall = false;
        let mut current_syscall: Option<SyscallInfo> = None;

        loop {
            match waitpid(pid, None)? {
                WaitStatus::Exited(_, code) => {
                    return Ok(code);
                }
                WaitStatus::Signaled(_, sig, _) => {
                    return Ok(128 + sig as i32);
                }
                WaitStatus::PtraceSyscall(_) => {
                    if in_syscall {
                        // Sortie de syscall
                        if let Some(mut info) = current_syscall.take() {
                            info.return_value = self.get_syscall_return(pid)?;
                            info.duration_ns = self.get_time_ns() - info.timestamp_ns;

                            self.record_syscall(&info);

                            if !self.config.count_only {
                                self.print_syscall(&info);
                            }
                        }
                        in_syscall = false;
                    } else {
                        // Entree de syscall
                        let info = self.get_syscall_info(pid)?;

                        // Filtrer si necessaire
                        if let Some(filter) = &self.config.filter_syscalls {
                            if !filter.contains(&info.number) {
                                ptrace::syscall(pid, None)?;
                                continue;
                            }
                        }

                        current_syscall = Some(info);
                        in_syscall = true;
                    }

                    ptrace::syscall(pid, None)?;
                }
                WaitStatus::Stopped(_, sig) => {
                    // Signal recu, le transmettre
                    ptrace::syscall(pid, sig)?;
                }
                _ => {
                    ptrace::syscall(pid, None)?;
                }
            }
        }
    }

    /// Obtenir informations syscall via ptrace
    fn get_syscall_info(&self, pid: Pid) -> Result<SyscallInfo, Box<dyn std::error::Error>> {
        let regs = ptrace::getregs(pid)?;

        #[cfg(target_arch = "x86_64")]
        let (number, args) = {
            let number = regs.orig_rax;
            let args = [regs.rdi, regs.rsi, regs.rdx, regs.r10, regs.r8, regs.r9];
            (number, args)
        };

        let name = self.syscall_names.get(&number).copied().unwrap_or("unknown");

        Ok(SyscallInfo {
            number,
            name,
            args,
            return_value: 0,
            duration_ns: 0,
            timestamp_ns: self.get_time_ns(),
        })
    }

    /// Obtenir valeur de retour
    fn get_syscall_return(&self, pid: Pid) -> Result<i64, Box<dyn std::error::Error>> {
        let regs = ptrace::getregs(pid)?;

        #[cfg(target_arch = "x86_64")]
        let ret = regs.rax as i64;

        Ok(ret)
    }

    /// Enregistrer statistiques
    fn record_syscall(&mut self, info: &SyscallInfo) {
        self.stats.total_calls += 1;
        self.stats.total_time_ns += info.duration_ns;

        let entry = self.stats.by_syscall
            .entry(info.number)
            .or_insert_with(SyscallCount::default);

        entry.count += 1;
        entry.time_ns += info.duration_ns;
        if info.return_value < 0 {
            entry.errors += 1;
        }
    }

    /// Afficher syscall
    fn print_syscall(&self, info: &SyscallInfo) {
        let args_str = self.format_args(info);

        if self.config.show_timestamps {
            print!("{:>12} ", info.timestamp_ns / 1000);  // us
        }

        if info.return_value < 0 {
            let errno = -info.return_value;
            let errno_name = errno_name(errno as u32);
            println!(
                "{}({}) = -1 {} ({})",
                info.name, args_str, errno_name, errno_desc(errno as u32)
            );
        } else {
            println!("{}({}) = {}", info.name, args_str, info.return_value);
        }
    }

    /// Formater arguments
    fn format_args(&self, info: &SyscallInfo) -> String {
        let mut args = Vec::new();

        // Formatage specifique par syscall
        match info.name {
            "write" | "read" => {
                args.push(format!("{}", info.args[0]));  // fd
                if self.config.decode_strings && info.name == "write" {
                    args.push(format!("\"...\""));  // buffer
                } else {
                    args.push(format!("0x{:x}", info.args[1]));
                }
                args.push(format!("{}", info.args[2]));  // count
            }
            "open" | "openat" => {
                let offset = if info.name == "openat" { 1 } else { 0 };
                args.push(format!("\"...\""));  // path
                args.push(format!("0x{:x}", info.args[1 + offset]));  // flags
            }
            "close" => {
                args.push(format!("{}", info.args[0]));
            }
            "mmap" => {
                args.push(format!("0x{:x}", info.args[0]));  // addr
                args.push(format!("{}", info.args[1]));  // length
                args.push(format!("0x{:x}", info.args[2]));  // prot
                args.push(format!("0x{:x}", info.args[3]));  // flags
                args.push(format!("{}", info.args[4] as i32));  // fd
                args.push(format!("{}", info.args[5]));  // offset
            }
            _ => {
                for i in 0..6 {
                    if info.args[i] != 0 || i < 2 {
                        args.push(format!("0x{:x}", info.args[i]));
                    }
                }
            }
        }

        args.join(", ")
    }

    /// Obtenir timestamp en nanosecondes
    fn get_time_ns(&self) -> u64 {
        use std::time::{SystemTime, UNIX_EPOCH};
        SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_nanos() as u64
    }

    /// Afficher resume statistiques
    pub fn print_summary(&self) {
        println!("\n% time     seconds  usecs/call     calls    errors syscall");
        println!("------ ----------- ----------- --------- --------- ----------------");

        let mut sorted: Vec<_> = self.stats.by_syscall.iter().collect();
        sorted.sort_by(|a, b| b.1.time_ns.cmp(&a.1.time_ns));

        for (num, stats) in sorted {
            let name = self.syscall_names.get(num).copied().unwrap_or("unknown");
            let percent = if self.stats.total_time_ns > 0 {
                (stats.time_ns as f64 / self.stats.total_time_ns as f64) * 100.0
            } else {
                0.0
            };
            let seconds = stats.time_ns as f64 / 1_000_000_000.0;
            let usecs_per_call = if stats.count > 0 {
                (stats.time_ns / stats.count) / 1000
            } else {
                0
            };

            println!(
                "{:>5.2}% {:>11.6} {:>11} {:>9} {:>9} {}",
                percent, seconds, usecs_per_call, stats.count, stats.errors, name
            );
        }

        println!("------ ----------- ----------- --------- --------- ----------------");
        println!(
            "100.00 {:>11.6}           - {:>9} {:>9} total",
            self.stats.total_time_ns as f64 / 1_000_000_000.0,
            self.stats.total_calls,
            self.stats.by_syscall.values().map(|s| s.errors).sum::<u64>()
        );
    }
}

/// Initialiser noms des syscalls x86_64
fn init_syscall_names() -> HashMap<u64, &'static str> {
    let mut map = HashMap::new();
    map.insert(0, "read");
    map.insert(1, "write");
    map.insert(2, "open");
    map.insert(3, "close");
    map.insert(4, "stat");
    map.insert(5, "fstat");
    map.insert(6, "lstat");
    map.insert(7, "poll");
    map.insert(8, "lseek");
    map.insert(9, "mmap");
    map.insert(10, "mprotect");
    map.insert(11, "munmap");
    map.insert(12, "brk");
    map.insert(13, "rt_sigaction");
    map.insert(14, "rt_sigprocmask");
    map.insert(15, "rt_sigreturn");
    map.insert(16, "ioctl");
    map.insert(17, "pread64");
    map.insert(18, "pwrite64");
    map.insert(19, "readv");
    map.insert(20, "writev");
    map.insert(21, "access");
    map.insert(22, "pipe");
    map.insert(23, "select");
    map.insert(24, "sched_yield");
    map.insert(25, "mremap");
    map.insert(26, "msync");
    map.insert(27, "mincore");
    map.insert(28, "madvise");
    map.insert(32, "dup");
    map.insert(33, "dup2");
    map.insert(35, "nanosleep");
    map.insert(39, "getpid");
    map.insert(56, "clone");
    map.insert(57, "fork");
    map.insert(58, "vfork");
    map.insert(59, "execve");
    map.insert(60, "exit");
    map.insert(61, "wait4");
    map.insert(62, "kill");
    map.insert(63, "uname");
    map.insert(72, "fcntl");
    map.insert(78, "getdents");
    map.insert(79, "getcwd");
    map.insert(80, "chdir");
    map.insert(89, "readlink");
    map.insert(102, "getuid");
    map.insert(104, "getgid");
    map.insert(107, "geteuid");
    map.insert(108, "getegid");
    map.insert(110, "getppid");
    map.insert(186, "gettid");
    map.insert(202, "futex");
    map.insert(217, "getdents64");
    map.insert(231, "exit_group");
    map.insert(257, "openat");
    map.insert(262, "newfstatat");
    map.insert(302, "prlimit64");
    map.insert(318, "getrandom");
    map
}

fn errno_name(errno: u32) -> &'static str {
    match errno {
        1 => "EPERM",
        2 => "ENOENT",
        3 => "ESRCH",
        4 => "EINTR",
        5 => "EIO",
        11 => "EAGAIN",
        12 => "ENOMEM",
        13 => "EACCES",
        14 => "EFAULT",
        17 => "EEXIST",
        20 => "ENOTDIR",
        21 => "EISDIR",
        22 => "EINVAL",
        _ => "E???",
    }
}

fn errno_desc(errno: u32) -> &'static str {
    match errno {
        1 => "Operation not permitted",
        2 => "No such file or directory",
        3 => "No such process",
        4 => "Interrupted system call",
        5 => "Input/output error",
        11 => "Resource temporarily unavailable",
        12 => "Cannot allocate memory",
        13 => "Permission denied",
        14 => "Bad address",
        17 => "File exists",
        20 => "Not a directory",
        21 => "Is a directory",
        22 => "Invalid argument",
        _ => "Unknown error",
    }
}
```

```rust
// ============================================================================
// perf.rs - Performance Profiler Interface
// ============================================================================

//! perf-like Performance Profiling

use std::collections::HashMap;
use std::fs::File;
use std::io::{BufRead, BufReader};
use std::os::unix::io::AsRawFd;
use std::path::Path;

/// Types d'evenements perf
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum PerfEventType {
    // Hardware events
    CpuCycles,
    Instructions,
    CacheReferences,
    CacheMisses,
    BranchInstructions,
    BranchMisses,
    BusCycles,

    // Software events
    CpuClock,
    TaskClock,
    PageFaults,
    ContextSwitches,
    CpuMigrations,
    MinorFaults,
    MajorFaults,

    // Custom tracepoint
    Tracepoint(u32),
}

impl PerfEventType {
    fn to_perf_type(&self) -> u32 {
        match self {
            Self::CpuCycles | Self::Instructions | Self::CacheReferences |
            Self::CacheMisses | Self::BranchInstructions | Self::BranchMisses |
            Self::BusCycles => 0,  // PERF_TYPE_HARDWARE
            Self::CpuClock | Self::TaskClock | Self::PageFaults |
            Self::ContextSwitches | Self::CpuMigrations |
            Self::MinorFaults | Self::MajorFaults => 1,  // PERF_TYPE_SOFTWARE
            Self::Tracepoint(_) => 2,  // PERF_TYPE_TRACEPOINT
        }
    }

    fn to_config(&self) -> u64 {
        match self {
            Self::CpuCycles => 0,
            Self::Instructions => 1,
            Self::CacheReferences => 2,
            Self::CacheMisses => 3,
            Self::BranchInstructions => 4,
            Self::BranchMisses => 5,
            Self::BusCycles => 6,
            Self::CpuClock => 0,
            Self::TaskClock => 1,
            Self::PageFaults => 2,
            Self::ContextSwitches => 3,
            Self::CpuMigrations => 4,
            Self::MinorFaults => 5,
            Self::MajorFaults => 6,
            Self::Tracepoint(id) => *id as u64,
        }
    }
}

/// Evenement perf mesure
#[derive(Debug, Clone)]
pub struct PerfEvent {
    pub event_type: PerfEventType,
    pub value: u64,
    pub time_enabled: u64,
    pub time_running: u64,
}

impl PerfEvent {
    /// Valeur normalisee (compensation pour multiplexing)
    pub fn scaled_value(&self) -> f64 {
        if self.time_running == 0 {
            return 0.0;
        }
        (self.value as f64) * (self.time_enabled as f64) / (self.time_running as f64)
    }
}

/// Resultat de profiling
#[derive(Debug, Default)]
pub struct PerfResult {
    pub events: HashMap<PerfEventType, PerfEvent>,
    pub duration_ns: u64,
    pub samples: Vec<PerfSample>,
}

/// Echantillon de profiling
#[derive(Debug, Clone)]
pub struct PerfSample {
    pub timestamp: u64,
    pub pid: u32,
    pub tid: u32,
    pub cpu: u32,
    pub ip: u64,  // Instruction pointer
    pub callchain: Vec<u64>,
}

/// Profiler perf
pub struct PerfProfiler {
    events: Vec<PerfEventType>,
    fds: Vec<i32>,
    pid: i32,
    cpu: i32,
}

impl PerfProfiler {
    /// Creer nouveau profiler pour PID (-1 = tous)
    pub fn new(pid: i32) -> Self {
        Self {
            events: Vec::new(),
            fds: Vec::new(),
            pid,
            cpu: -1,
        }
    }

    /// Profiler un CPU specifique
    pub fn for_cpu(cpu: i32) -> Self {
        Self {
            events: Vec::new(),
            fds: Vec::new(),
            pid: -1,
            cpu,
        }
    }

    /// Ajouter evenement a mesurer
    pub fn add_event(&mut self, event: PerfEventType) -> Result<(), Box<dyn std::error::Error>> {
        #[repr(C)]
        struct PerfEventAttr {
            type_: u32,
            size: u32,
            config: u64,
            sample_period_or_freq: u64,
            sample_type: u64,
            read_format: u64,
            flags: u64,
            wakeup_events_or_watermark: u32,
            bp_type: u32,
            bp_addr_or_config1: u64,
            bp_len_or_config2: u64,
            branch_sample_type: u64,
            sample_regs_user: u64,
            sample_stack_user: u32,
            clockid: i32,
            sample_regs_intr: u64,
            aux_watermark: u32,
            sample_max_stack: u16,
            reserved_2: u16,
        }

        let attr = PerfEventAttr {
            type_: event.to_perf_type(),
            size: std::mem::size_of::<PerfEventAttr>() as u32,
            config: event.to_config(),
            sample_period_or_freq: 0,
            sample_type: 0,
            read_format: 0x7,  // TOTAL_TIME_ENABLED | TOTAL_TIME_RUNNING | ID
            flags: 0x1 | 0x2,  // disabled | inherit
            wakeup_events_or_watermark: 0,
            bp_type: 0,
            bp_addr_or_config1: 0,
            bp_len_or_config2: 0,
            branch_sample_type: 0,
            sample_regs_user: 0,
            sample_stack_user: 0,
            clockid: 0,
            sample_regs_intr: 0,
            aux_watermark: 0,
            sample_max_stack: 0,
            reserved_2: 0,
        };

        let fd = unsafe {
            libc::syscall(
                libc::SYS_perf_event_open,
                &attr as *const _,
                self.pid,
                self.cpu,
                -1i32,  // group_fd
                0u64,   // flags
            )
        };

        if fd < 0 {
            return Err(std::io::Error::last_os_error().into());
        }

        self.events.push(event);
        self.fds.push(fd as i32);

        Ok(())
    }

    /// Demarrer le comptage
    pub fn start(&self) -> Result<(), Box<dyn std::error::Error>> {
        for &fd in &self.fds {
            let ret = unsafe {
                libc::ioctl(fd, 0x2400, 0)  // PERF_EVENT_IOC_ENABLE
            };
            if ret < 0 {
                return Err(std::io::Error::last_os_error().into());
            }
        }
        Ok(())
    }

    /// Arreter le comptage
    pub fn stop(&self) -> Result<(), Box<dyn std::error::Error>> {
        for &fd in &self.fds {
            let ret = unsafe {
                libc::ioctl(fd, 0x2401, 0)  // PERF_EVENT_IOC_DISABLE
            };
            if ret < 0 {
                return Err(std::io::Error::last_os_error().into());
            }
        }
        Ok(())
    }

    /// Lire les resultats
    pub fn read(&self) -> Result<PerfResult, Box<dyn std::error::Error>> {
        let mut result = PerfResult::default();

        for (i, &fd) in self.fds.iter().enumerate() {
            let mut data = [0u64; 4];  // value, time_enabled, time_running, id

            let n = unsafe {
                libc::read(
                    fd,
                    data.as_mut_ptr() as *mut _,
                    std::mem::size_of_val(&data),
                )
            };

            if n < 0 {
                return Err(std::io::Error::last_os_error().into());
            }

            let event = PerfEvent {
                event_type: self.events[i],
                value: data[0],
                time_enabled: data[1],
                time_running: data[2],
            };

            result.events.insert(self.events[i], event);
        }

        Ok(result)
    }

    /// Reset compteurs
    pub fn reset(&self) -> Result<(), Box<dyn std::error::Error>> {
        for &fd in &self.fds {
            let ret = unsafe {
                libc::ioctl(fd, 0x2403, 0)  // PERF_EVENT_IOC_RESET
            };
            if ret < 0 {
                return Err(std::io::Error::last_os_error().into());
            }
        }
        Ok(())
    }
}

impl Drop for PerfProfiler {
    fn drop(&mut self) {
        for &fd in &self.fds {
            unsafe { libc::close(fd) };
        }
    }
}

/// Lire les tracepoints disponibles
pub fn list_tracepoints() -> Result<Vec<(String, u32)>, Box<dyn std::error::Error>> {
    let mut tracepoints = Vec::new();
    let events_dir = Path::new("/sys/kernel/debug/tracing/events");

    if !events_dir.exists() {
        return Ok(tracepoints);
    }

    fn walk_events(dir: &Path, tracepoints: &mut Vec<(String, u32)>) -> std::io::Result<()> {
        for entry in std::fs::read_dir(dir)? {
            let entry = entry?;
            let path = entry.path();

            if path.is_dir() {
                let id_path = path.join("id");
                if id_path.exists() {
                    let id_str = std::fs::read_to_string(&id_path)?;
                    if let Ok(id) = id_str.trim().parse::<u32>() {
                        let name = path.strip_prefix("/sys/kernel/debug/tracing/events")
                            .unwrap_or(&path)
                            .to_string_lossy()
                            .to_string();
                        tracepoints.push((name, id));
                    }
                } else {
                    walk_events(&path, tracepoints)?;
                }
            }
        }
        Ok(())
    }

    walk_events(events_dir, &mut tracepoints)?;
    Ok(tracepoints)
}

/// Afficher rapport perf
pub fn print_report(result: &PerfResult) {
    println!("\n Performance counter stats:\n");

    let ordered = [
        (PerfEventType::CpuCycles, "cycles"),
        (PerfEventType::Instructions, "instructions"),
        (PerfEventType::CacheReferences, "cache-references"),
        (PerfEventType::CacheMisses, "cache-misses"),
        (PerfEventType::BranchInstructions, "branch-instructions"),
        (PerfEventType::BranchMisses, "branch-misses"),
        (PerfEventType::PageFaults, "page-faults"),
        (PerfEventType::ContextSwitches, "context-switches"),
    ];

    for (event_type, name) in ordered {
        if let Some(event) = result.events.get(&event_type) {
            let value = event.scaled_value();
            println!("  {:>15.0}      {}", value, name);
        }
    }

    // Calculer IPC si disponible
    if let (Some(cycles), Some(instructions)) = (
        result.events.get(&PerfEventType::CpuCycles),
        result.events.get(&PerfEventType::Instructions),
    ) {
        let ipc = instructions.scaled_value() / cycles.scaled_value();
        println!("\n  {:>15.2}      insn per cycle", ipc);
    }

    // Calculer cache miss rate si disponible
    if let (Some(refs), Some(misses)) = (
        result.events.get(&PerfEventType::CacheReferences),
        result.events.get(&PerfEventType::CacheMisses),
    ) {
        let miss_rate = (misses.scaled_value() / refs.scaled_value()) * 100.0;
        println!("  {:>14.2}%      cache miss rate", miss_rate);
    }

    // Calculer branch miss rate si disponible
    if let (Some(branches), Some(misses)) = (
        result.events.get(&PerfEventType::BranchInstructions),
        result.events.get(&PerfEventType::BranchMisses),
    ) {
        let miss_rate = (misses.scaled_value() / branches.scaled_value()) * 100.0;
        println!("  {:>14.2}%      branch miss rate", miss_rate);
    }

    println!();
}
```

```rust
// ============================================================================
// ebpf.rs - eBPF Framework
// ============================================================================

//! eBPF Program Loading and Management

use std::collections::HashMap;
use std::ffi::CString;
use std::os::unix::io::RawFd;

/// Types de programmes eBPF
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum BpfProgType {
    Unspec = 0,
    SocketFilter = 1,
    Kprobe = 2,
    SchedCls = 3,
    SchedAct = 4,
    Tracepoint = 5,
    Xdp = 6,
    PerfEvent = 7,
    CgroupSkb = 8,
    CgroupSock = 9,
    LwtIn = 10,
    LwtOut = 11,
    LwtXmit = 12,
    SockOps = 13,
    SkSkb = 14,
    CgroupDevice = 15,
    SkMsg = 16,
    RawTracepoint = 17,
    CgroupSockAddr = 18,
    LwtSeg6local = 19,
    LircMode2 = 20,
    SkReuseport = 21,
    FlowDissector = 22,
    CgroupSysctl = 23,
    RawTracepointWritable = 24,
    CgroupSockopt = 25,
    Tracing = 26,
    StructOps = 27,
    Ext = 28,
    Lsm = 29,
    SkLookup = 30,
}

/// Types de maps eBPF
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum BpfMapType {
    Unspec = 0,
    Hash = 1,
    Array = 2,
    ProgArray = 3,
    PerfEventArray = 4,
    PercpuHash = 5,
    PercpuArray = 6,
    StackTrace = 7,
    CgroupArray = 8,
    LruHash = 9,
    LruPercpuHash = 10,
    LpmTrie = 11,
    ArrayOfMaps = 12,
    HashOfMaps = 13,
    Devmap = 14,
    Sockmap = 15,
    Cpumap = 16,
    Xskmap = 17,
    Sockhash = 18,
    CgroupStorage = 19,
    ReuseportSockarray = 20,
    PercpuCgroupStorage = 21,
    Queue = 22,
    Stack = 23,
    SkStorage = 24,
    DevmapHash = 25,
    StructOps = 26,
    Ringbuf = 27,
    InodeStorage = 28,
    TaskStorage = 29,
}

/// Instructions BPF
#[derive(Debug, Clone, Copy)]
#[repr(C)]
pub struct BpfInsn {
    pub code: u8,
    pub dst_reg: u8,  // 4 bits dst, 4 bits src
    pub src_reg: u8,
    pub off: i16,
    pub imm: i32,
}

impl BpfInsn {
    pub fn new(code: u8, dst: u8, src: u8, off: i16, imm: i32) -> Self {
        Self {
            code,
            dst_reg: dst & 0xf,
            src_reg: src & 0xf,
            off,
            imm,
        }
    }

    // ALU operations
    pub fn alu64_imm(op: u8, dst: u8, imm: i32) -> Self {
        Self::new(0x07 | op, dst, 0, 0, imm)
    }

    pub fn alu64_reg(op: u8, dst: u8, src: u8) -> Self {
        Self::new(0x0f | op, dst, src, 0, 0)
    }

    // Memory operations
    pub fn ldx_mem(size: u8, dst: u8, src: u8, off: i16) -> Self {
        Self::new(0x61 | (size << 3), dst, src, off, 0)
    }

    pub fn stx_mem(size: u8, dst: u8, src: u8, off: i16) -> Self {
        Self::new(0x63 | (size << 3), dst, src, off, 0)
    }

    // Jump operations
    pub fn jmp_imm(op: u8, dst: u8, imm: i32, off: i16) -> Self {
        Self::new(0x05 | op, dst, 0, off, imm)
    }

    pub fn jmp_reg(op: u8, dst: u8, src: u8, off: i16) -> Self {
        Self::new(0x0d | op, dst, src, off, 0)
    }

    // Exit
    pub fn exit() -> Self {
        Self::new(0x95, 0, 0, 0, 0)
    }

    // Call helper
    pub fn call(helper_id: i32) -> Self {
        Self::new(0x85, 0, 0, 0, helper_id)
    }

    // Load 64-bit immediate
    pub fn ld_imm64(dst: u8, imm: u64) -> [Self; 2] {
        [
            Self::new(0x18, dst, 0, 0, imm as i32),
            Self::new(0x00, 0, 0, 0, (imm >> 32) as i32),
        ]
    }

    // Move register
    pub fn mov64_reg(dst: u8, src: u8) -> Self {
        Self::new(0xbf, dst, src, 0, 0)
    }

    pub fn mov64_imm(dst: u8, imm: i32) -> Self {
        Self::new(0xb7, dst, 0, 0, imm)
    }
}

/// Map BPF
pub struct BpfMap {
    pub fd: RawFd,
    pub map_type: BpfMapType,
    pub key_size: u32,
    pub value_size: u32,
    pub max_entries: u32,
    pub name: String,
}

impl BpfMap {
    /// Creer une nouvelle map
    pub fn create(
        map_type: BpfMapType,
        name: &str,
        key_size: u32,
        value_size: u32,
        max_entries: u32,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        #[repr(C)]
        union BpfAttr {
            create: BpfMapCreate,
        }

        #[repr(C)]
        #[derive(Default)]
        struct BpfMapCreate {
            map_type: u32,
            key_size: u32,
            value_size: u32,
            max_entries: u32,
            map_flags: u32,
            inner_map_fd: u32,
            numa_node: u32,
            map_name: [u8; 16],
            map_ifindex: u32,
            btf_fd: u32,
            btf_key_type_id: u32,
            btf_value_type_id: u32,
            btf_vmlinux_value_type_id: u32,
        }

        let mut attr = BpfAttr {
            create: BpfMapCreate {
                map_type: map_type as u32,
                key_size,
                value_size,
                max_entries,
                ..Default::default()
            }
        };

        // Copier nom
        let name_bytes = name.as_bytes();
        let len = name_bytes.len().min(15);
        unsafe {
            attr.create.map_name[..len].copy_from_slice(&name_bytes[..len]);
        }

        let fd = unsafe {
            libc::syscall(
                libc::SYS_bpf,
                0,  // BPF_MAP_CREATE
                &attr as *const _,
                std::mem::size_of::<BpfMapCreate>(),
            )
        };

        if fd < 0 {
            return Err(std::io::Error::last_os_error().into());
        }

        Ok(Self {
            fd: fd as RawFd,
            map_type,
            key_size,
            value_size,
            max_entries,
            name: name.to_string(),
        })
    }

    /// Lookup key
    pub fn lookup(&self, key: &[u8]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
        if key.len() != self.key_size as usize {
            return Err("Key size mismatch".into());
        }

        let mut value = vec![0u8; self.value_size as usize];

        #[repr(C)]
        struct BpfMapLookup {
            map_fd: u32,
            key: u64,
            value_or_next: u64,
            flags: u64,
        }

        let attr = BpfMapLookup {
            map_fd: self.fd as u32,
            key: key.as_ptr() as u64,
            value_or_next: value.as_mut_ptr() as u64,
            flags: 0,
        };

        let ret = unsafe {
            libc::syscall(
                libc::SYS_bpf,
                1,  // BPF_MAP_LOOKUP_ELEM
                &attr as *const _,
                std::mem::size_of::<BpfMapLookup>(),
            )
        };

        if ret < 0 {
            return Err(std::io::Error::last_os_error().into());
        }

        Ok(value)
    }

    /// Update key
    pub fn update(&self, key: &[u8], value: &[u8], flags: u64) -> Result<(), Box<dyn std::error::Error>> {
        if key.len() != self.key_size as usize {
            return Err("Key size mismatch".into());
        }
        if value.len() != self.value_size as usize {
            return Err("Value size mismatch".into());
        }

        #[repr(C)]
        struct BpfMapUpdate {
            map_fd: u32,
            key: u64,
            value: u64,
            flags: u64,
        }

        let attr = BpfMapUpdate {
            map_fd: self.fd as u32,
            key: key.as_ptr() as u64,
            value: value.as_ptr() as u64,
            flags,
        };

        let ret = unsafe {
            libc::syscall(
                libc::SYS_bpf,
                2,  // BPF_MAP_UPDATE_ELEM
                &attr as *const _,
                std::mem::size_of::<BpfMapUpdate>(),
            )
        };

        if ret < 0 {
            return Err(std::io::Error::last_os_error().into());
        }

        Ok(())
    }

    /// Delete key
    pub fn delete(&self, key: &[u8]) -> Result<(), Box<dyn std::error::Error>> {
        if key.len() != self.key_size as usize {
            return Err("Key size mismatch".into());
        }

        #[repr(C)]
        struct BpfMapDelete {
            map_fd: u32,
            key: u64,
            value: u64,
            flags: u64,
        }

        let attr = BpfMapDelete {
            map_fd: self.fd as u32,
            key: key.as_ptr() as u64,
            value: 0,
            flags: 0,
        };

        let ret = unsafe {
            libc::syscall(
                libc::SYS_bpf,
                3,  // BPF_MAP_DELETE_ELEM
                &attr as *const _,
                std::mem::size_of::<BpfMapDelete>(),
            )
        };

        if ret < 0 {
            return Err(std::io::Error::last_os_error().into());
        }

        Ok(())
    }
}

impl Drop for BpfMap {
    fn drop(&mut self) {
        unsafe { libc::close(self.fd) };
    }
}

/// Programme BPF
pub struct BpfProgram {
    pub fd: RawFd,
    pub prog_type: BpfProgType,
    pub name: String,
    pub instructions: Vec<BpfInsn>,
}

impl BpfProgram {
    /// Charger un programme BPF
    pub fn load(
        prog_type: BpfProgType,
        name: &str,
        instructions: Vec<BpfInsn>,
        license: &str,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let license_cstr = CString::new(license)?;
        let mut log_buf = vec![0u8; 65536];

        #[repr(C)]
        struct BpfProgLoad {
            prog_type: u32,
            insn_cnt: u32,
            insns: u64,
            license: u64,
            log_level: u32,
            log_size: u32,
            log_buf: u64,
            kern_version: u32,
            prog_flags: u32,
            prog_name: [u8; 16],
            prog_ifindex: u32,
            expected_attach_type: u32,
            prog_btf_fd: u32,
            func_info_rec_size: u32,
            func_info: u64,
            func_info_cnt: u32,
            line_info_rec_size: u32,
            line_info: u64,
            line_info_cnt: u32,
            attach_btf_id: u32,
            attach_prog_fd: u32,
        }

        let mut attr = BpfProgLoad {
            prog_type: prog_type as u32,
            insn_cnt: instructions.len() as u32,
            insns: instructions.as_ptr() as u64,
            license: license_cstr.as_ptr() as u64,
            log_level: 1,
            log_size: log_buf.len() as u32,
            log_buf: log_buf.as_mut_ptr() as u64,
            kern_version: 0,
            prog_flags: 0,
            prog_name: [0; 16],
            prog_ifindex: 0,
            expected_attach_type: 0,
            prog_btf_fd: 0,
            func_info_rec_size: 0,
            func_info: 0,
            func_info_cnt: 0,
            line_info_rec_size: 0,
            line_info: 0,
            line_info_cnt: 0,
            attach_btf_id: 0,
            attach_prog_fd: 0,
        };

        // Copier nom
        let name_bytes = name.as_bytes();
        let len = name_bytes.len().min(15);
        attr.prog_name[..len].copy_from_slice(&name_bytes[..len]);

        let fd = unsafe {
            libc::syscall(
                libc::SYS_bpf,
                5,  // BPF_PROG_LOAD
                &attr as *const _,
                std::mem::size_of::<BpfProgLoad>(),
            )
        };

        if fd < 0 {
            // Afficher log du verifieur
            let log = String::from_utf8_lossy(&log_buf);
            let log_trimmed = log.trim_end_matches('\0');
            if !log_trimmed.is_empty() {
                eprintln!("BPF verifier log:\n{}", log_trimmed);
            }
            return Err(std::io::Error::last_os_error().into());
        }

        Ok(Self {
            fd: fd as RawFd,
            prog_type,
            name: name.to_string(),
            instructions,
        })
    }

    /// Attacher a un tracepoint
    pub fn attach_tracepoint(&self, category: &str, name: &str) -> Result<RawFd, Box<dyn std::error::Error>> {
        // Ouvrir le fichier perf_event
        let path = format!("/sys/kernel/debug/tracing/events/{}/{}/id", category, name);
        let id_str = std::fs::read_to_string(&path)?;
        let id: u32 = id_str.trim().parse()?;

        // Creer perf_event
        #[repr(C)]
        struct PerfEventAttr {
            type_: u32,
            size: u32,
            config: u64,
            sample_period: u64,
            sample_type: u64,
            read_format: u64,
            flags: u64,
            wakeup_events: u32,
            bp_type: u32,
            bp_addr: u64,
            bp_len: u64,
        }

        let attr = PerfEventAttr {
            type_: 2,  // PERF_TYPE_TRACEPOINT
            size: std::mem::size_of::<PerfEventAttr>() as u32,
            config: id as u64,
            sample_period: 1,
            sample_type: 0,
            read_format: 0,
            flags: 0,
            wakeup_events: 1,
            bp_type: 0,
            bp_addr: 0,
            bp_len: 0,
        };

        let perf_fd = unsafe {
            libc::syscall(
                libc::SYS_perf_event_open,
                &attr as *const _,
                -1,  // pid
                0,   // cpu
                -1,  // group_fd
                0u64,  // flags
            )
        };

        if perf_fd < 0 {
            return Err(std::io::Error::last_os_error().into());
        }

        // Attacher programme BPF
        let ret = unsafe {
            libc::ioctl(
                perf_fd as i32,
                0x40082408,  // PERF_EVENT_IOC_SET_BPF
                self.fd,
            )
        };

        if ret < 0 {
            unsafe { libc::close(perf_fd as i32) };
            return Err(std::io::Error::last_os_error().into());
        }

        // Activer
        let ret = unsafe {
            libc::ioctl(perf_fd as i32, 0x2400, 0)  // PERF_EVENT_IOC_ENABLE
        };

        if ret < 0 {
            unsafe { libc::close(perf_fd as i32) };
            return Err(std::io::Error::last_os_error().into());
        }

        Ok(perf_fd as RawFd)
    }

    /// Attacher a XDP
    pub fn attach_xdp(&self, ifindex: u32, flags: u32) -> Result<(), Box<dyn std::error::Error>> {
        #[repr(C)]
        struct BpfLinkCreate {
            prog_fd: u32,
            target_fd: u32,
            attach_type: u32,
            flags: u32,
        }

        // Pour XDP, on utilise netlink ou bpf syscall directement
        // Simplification: utiliser BPF_LINK_CREATE

        let attr = BpfLinkCreate {
            prog_fd: self.fd as u32,
            target_fd: ifindex,
            attach_type: 37,  // BPF_XDP
            flags,
        };

        let ret = unsafe {
            libc::syscall(
                libc::SYS_bpf,
                28,  // BPF_LINK_CREATE
                &attr as *const _,
                std::mem::size_of::<BpfLinkCreate>(),
            )
        };

        if ret < 0 {
            return Err(std::io::Error::last_os_error().into());
        }

        Ok(())
    }
}

impl Drop for BpfProgram {
    fn drop(&mut self) {
        unsafe { libc::close(self.fd) };
    }
}

/// Loader BPF simplifie
pub struct BpfLoader {
    programs: HashMap<String, BpfProgram>,
    maps: HashMap<String, BpfMap>,
}

impl BpfLoader {
    pub fn new() -> Self {
        Self {
            programs: HashMap::new(),
            maps: HashMap::new(),
        }
    }

    /// Creer programme XDP simple qui compte les paquets
    pub fn create_xdp_counter(&mut self) -> Result<&BpfProgram, Box<dyn std::error::Error>> {
        // Creer map pour compteur
        let counter_map = BpfMap::create(
            BpfMapType::Array,
            "pkt_count",
            4,  // key: u32
            8,  // value: u64
            1,  // 1 entree
        )?;

        let map_fd = counter_map.fd;
        self.maps.insert("pkt_count".to_string(), counter_map);

        // Programme XDP
        let mut insns = Vec::new();

        // r0 = XDP_PASS (2)
        insns.push(BpfInsn::mov64_imm(0, 2));

        // Charger adresse de la map
        // r1 = map_fd
        let ld64 = BpfInsn::ld_imm64(1, map_fd as u64);
        insns.push(ld64[0]);
        insns.push(ld64[1]);

        // r2 = &key (sur la stack)
        // Stocker key = 0 sur la stack
        insns.push(BpfInsn::mov64_imm(2, 0));
        insns.push(BpfInsn::stx_mem(2, 10, 2, -4));  // *(u32*)(fp-4) = 0

        // r2 = fp - 4
        insns.push(BpfInsn::mov64_reg(2, 10));
        insns.push(BpfInsn::alu64_imm(0x00, 2, -4));  // ADD

        // call bpf_map_lookup_elem
        insns.push(BpfInsn::call(1));

        // if (r0 == NULL) goto exit
        insns.push(BpfInsn::jmp_imm(0x10, 0, 0, 2));  // JEQ

        // (*r0)++
        insns.push(BpfInsn::ldx_mem(3, 1, 0, 0));  // r1 = *r0
        insns.push(BpfInsn::alu64_imm(0x00, 1, 1));  // r1 += 1
        insns.push(BpfInsn::stx_mem(3, 0, 1, 0));  // *r0 = r1

        // return XDP_PASS
        insns.push(BpfInsn::mov64_imm(0, 2));
        insns.push(BpfInsn::exit());

        let prog = BpfProgram::load(
            BpfProgType::Xdp,
            "xdp_counter",
            insns,
            "GPL",
        )?;

        self.programs.insert("xdp_counter".to_string(), prog);
        Ok(self.programs.get("xdp_counter").unwrap())
    }

    /// Obtenir un programme
    pub fn get_program(&self, name: &str) -> Option<&BpfProgram> {
        self.programs.get(name)
    }

    /// Obtenir une map
    pub fn get_map(&self, name: &str) -> Option<&BpfMap> {
        self.maps.get(name)
    }
}
```

## Criteres d'Evaluation

### Fonctionnalite (40 points)
- [ ] Framework RT complet (8 pts)
- [ ] Mesure de latence precise (8 pts)
- [ ] Tracer strace fonctionnel (8 pts)
- [ ] Interface perf complete (8 pts)
- [ ] Programmes eBPF chargeables (8 pts)

### Correction Technique (30 points)
- [ ] Scheduling RT correct (10 pts)
- [ ] Syscalls ptrace corrects (10 pts)
- [ ] eBPF verifiable par kernel (10 pts)

### Qualite du Code (20 points)
- [ ] Architecture modulaire (5 pts)
- [ ] Gestion d'erreurs (5 pts)
- [ ] Documentation (5 pts)
- [ ] Tests (5 pts)

### Pedagogie (10 points)
- [ ] Comptes rendus clairs (5 pts)
- [ ] Progression logique (5 pts)

## Ressources
- PREEMPT_RT: https://wiki.linuxfoundation.org/realtime/start
- perf documentation: https://perf.wiki.kernel.org/
- eBPF documentation: https://ebpf.io/
- BPF and XDP Reference: https://docs.kernel.org/bpf/
