<thinking>
## Analyse du Concept
- Concept : Database Sharding Router
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - Le sharding est essentiel pour la scalabilite horizontale. Implementation complete avec routing, rebalancing, et cross-shard queries.
</thinking>

# Exercice 5.2.12-a : sharding_router

**Module :**
5.2.12 -- Database Sharding & Scalability

**Concept :**
a -- Sharding Router (hash/range sharding, routing, resharding)

**Difficulte :**
8/10

**Type :**
code

**Tiers :**
3 -- Systeme complet

**Langage :**
Rust Edition 2024

**Prerequis :**
- 4.3 -- Hash functions
- 5.2.11 -- Replication concepts
- 4.6 -- Distributed systems

**Domaines :**
DB, Distributed, Scalability

**Duree estimee :**
180 min

**XP Base :**
300

**Complexite :**
T2 O(1) routing x S2 O(n) shard map

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :**
```
src/lib.rs
```

### 1.2 Consigne

**CONTEXTE : "The Great Library of Alexandria"**

*"Une seule bibliotheque ne peut pas contenir toutes les connaissances du monde. Nous devons diviser les livres entre plusieurs batiments, chacun specialise, mais tous accessibles depuis un index central."* -- Ptolemee, probablement

Le sharding divise les donnees entre plusieurs serveurs pour depasser les limites d'un seul noeud. Le routeur doit diriger chaque requete vers le bon shard de maniere transparente.

**Ta mission :**

Implementer un routeur de sharding qui :
1. Route les requetes vers le bon shard (hash ou range)
2. Gere la topologie des shards (ajout/suppression)
3. Execute les requetes cross-shard
4. Orchestre le resharding (rebalancing)
5. Detecte les shards indisponibles

### 1.3 Prototype

```rust
use std::collections::{HashMap, BTreeMap};
use std::hash::{Hash, Hasher};

/// Identifiant de shard
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub struct ShardId(pub u32);

/// Strategie de sharding
#[derive(Debug, Clone)]
pub enum ShardingStrategy {
    Hash {
        key_column: String,
        num_shards: u32,
    },
    Range {
        key_column: String,
        ranges: Vec<ShardRange>,
    },
    Directory {
        key_column: String,
        mapping: HashMap<String, ShardId>,
    },
    Consistent {
        key_column: String,
        virtual_nodes: u32,
    },
}

/// Range pour range sharding
#[derive(Debug, Clone)]
pub struct ShardRange {
    pub shard_id: ShardId,
    pub start: Option<ShardKey>,
    pub end: Option<ShardKey>,
}

/// Cle de sharding
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub enum ShardKey {
    Int(i64),
    String(String),
    Uuid(u128),
}

/// Configuration d'un shard
#[derive(Debug, Clone)]
pub struct ShardConfig {
    pub id: ShardId,
    pub hosts: Vec<String>,
    pub weight: u32,
    pub status: ShardStatus,
}

/// Statut d'un shard
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ShardStatus {
    Active,
    ReadOnly,
    Draining,
    Offline,
}

/// Requete shardee
#[derive(Debug, Clone)]
pub struct ShardedQuery {
    pub sql: String,
    pub shard_keys: Vec<ShardKey>,
    pub query_type: QueryType,
}

#[derive(Debug, Clone, Copy)]
pub enum QueryType {
    PointLookup,    // Single key
    RangeScan,      // Key range
    ScatterGather,  // All shards
    Targeted,       // Specific shards
}

/// Resultat de routage
#[derive(Debug, Clone)]
pub struct RoutingResult {
    pub shard_id: ShardId,
    pub host: String,
    pub rewritten_query: Option<String>,
}

/// Erreurs de sharding
#[derive(Debug, Clone, thiserror::Error)]
pub enum ShardingError {
    #[error("Shard not found: {0:?}")]
    ShardNotFound(ShardId),
    #[error("No shard for key")]
    NoShardForKey,
    #[error("Shard offline: {0:?}")]
    ShardOffline(ShardId),
    #[error("Cross-shard transaction not supported")]
    CrossShardTransaction,
    #[error("Resharding in progress")]
    ReshardingInProgress,
    #[error("Invalid shard key")]
    InvalidShardKey,
}

/// Statistiques
#[derive(Debug, Clone, Default)]
pub struct ShardingStats {
    pub total_queries: u64,
    pub point_lookups: u64,
    pub scatter_gathers: u64,
    pub cross_shard_queries: u64,
    pub queries_per_shard: HashMap<ShardId, u64>,
}

/// Configuration du routeur
#[derive(Debug, Clone)]
pub struct RouterConfig {
    pub strategy: ShardingStrategy,
    pub shards: Vec<ShardConfig>,
    pub enable_cross_shard: bool,
    pub retry_count: u32,
}

/// Routeur de sharding
pub struct ShardingRouter {
    config: RouterConfig,
    shard_map: HashMap<ShardId, ShardConfig>,
    consistent_ring: Option<ConsistentHashRing>,
    stats: ShardingStats,
}

impl ShardingRouter {
    /// Cree un nouveau routeur
    pub fn new(config: RouterConfig) -> Result<Self, ShardingError>;

    /// Route une requete vers le bon shard
    pub fn route(&self, query: &ShardedQuery) -> Result<Vec<RoutingResult>, ShardingError>;

    /// Route par cle unique
    pub fn route_key(&self, key: &ShardKey) -> Result<ShardId, ShardingError>;

    /// Route par range
    pub fn route_range(&self, start: &ShardKey, end: &ShardKey) -> Result<Vec<ShardId>, ShardingError>;

    /// Tous les shards (scatter-gather)
    pub fn all_shards(&self) -> Vec<ShardId>;

    /// Shards actifs seulement
    pub fn active_shards(&self) -> Vec<ShardId>;

    // === Gestion de topologie ===

    /// Ajoute un shard
    pub fn add_shard(&mut self, config: ShardConfig) -> Result<(), ShardingError>;

    /// Supprime un shard (doit etre draine d'abord)
    pub fn remove_shard(&mut self, shard_id: ShardId) -> Result<(), ShardingError>;

    /// Met a jour le statut d'un shard
    pub fn update_status(&mut self, shard_id: ShardId, status: ShardStatus) -> Result<(), ShardingError>;

    // === Resharding ===

    /// Demarre le resharding
    pub fn start_resharding(&mut self, new_config: RouterConfig) -> Result<ReshardingPlan, ShardingError>;

    /// Progresse le resharding
    pub fn progress_resharding(&mut self, completed: Vec<MigrationTask>) -> Result<ReshardingProgress, ShardingError>;

    /// Finalise le resharding
    pub fn finalize_resharding(&mut self) -> Result<(), ShardingError>;

    // === Monitoring ===

    /// Statistiques
    pub fn stats(&self) -> &ShardingStats;

    /// Distribution des cles
    pub fn key_distribution(&self) -> HashMap<ShardId, u64>;

    /// Detecte les hotspots
    pub fn detect_hotspots(&self) -> Vec<ShardId>;
}

/// Anneau de hash consistent
pub struct ConsistentHashRing {
    ring: BTreeMap<u64, ShardId>,
    virtual_nodes: u32,
}

impl ConsistentHashRing {
    pub fn new(shards: &[ShardConfig], virtual_nodes: u32) -> Self;
    pub fn get_shard(&self, key: &ShardKey) -> ShardId;
    pub fn add_node(&mut self, shard: &ShardConfig);
    pub fn remove_node(&mut self, shard_id: ShardId);
}

/// Plan de resharding
#[derive(Debug, Clone)]
pub struct ReshardingPlan {
    pub tasks: Vec<MigrationTask>,
    pub estimated_keys: u64,
    pub source_shards: Vec<ShardId>,
    pub target_shards: Vec<ShardId>,
}

/// Tache de migration
#[derive(Debug, Clone)]
pub struct MigrationTask {
    pub source: ShardId,
    pub target: ShardId,
    pub key_range: (Option<ShardKey>, Option<ShardKey>),
    pub status: MigrationStatus,
}

#[derive(Debug, Clone, Copy)]
pub enum MigrationStatus {
    Pending,
    InProgress,
    Completed,
    Failed,
}

/// Progression du resharding
#[derive(Debug, Clone)]
pub struct ReshardingProgress {
    pub completed_tasks: usize,
    pub total_tasks: usize,
    pub migrated_keys: u64,
    pub remaining_keys: u64,
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 Types de Sharding

**Hash Sharding** : shard = hash(key) % num_shards
- Bonne distribution
- Pas de range queries efficaces

**Range Sharding** : shard basee sur des plages de cles
- Range queries efficaces
- Risque de hotspots

**Consistent Hashing** : anneau virtuel
- Resharding minimal lors d'ajout/suppression
- Utilise par Cassandra, DynamoDB

### 2.2 Le probleme N+1 en sharding

Si vous avez 10 shards et faites une requete sans shard key, vous devez:
1. Envoyer la requete aux 10 shards
2. Agreger les resultats
3. Trier si necessaire

C'est pourquoi les requetes avec shard key sont cruciales!

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ cargo test
running 12 tests
test tests::test_hash_routing ... ok
test tests::test_range_routing ... ok
test tests::test_consistent_hash ... ok
test tests::test_add_shard ... ok
test tests::test_remove_shard ... ok
test tests::test_scatter_gather ... ok
test tests::test_offline_shard ... ok
test tests::test_resharding_plan ... ok
test tests::test_migration_task ... ok
test tests::test_hotspot_detection ... ok
test tests::test_key_distribution ... ok
test tests::test_stats ... ok

test result: ok. 12 passed; 0 failed
```

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette -- Tableau des tests

| Test | Input | Expected | Points | Categorie |
|------|-------|----------|--------|-----------|
| `hash_routing` | Key -> shard | Consistent mapping | 10 | Core |
| `range_routing` | Key in range | Correct shard | 10 | Core |
| `consistent_hash` | Add/remove node | Minimal remapping | 15 | Algorithm |
| `scatter_gather` | No key | All shards | 5 | Core |
| `offline_shard` | Shard down | Error or fallback | 10 | Resilience |
| `resharding` | Add shard | Correct migration plan | 15 | Core |
| `hotspot_detection` | Uneven load | Hotspots identified | 10 | Monitoring |
| `cross_shard` | Multi-shard query | Aggregated result | 10 | Advanced |

**Score minimum pour validation : 70/100**

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;

    fn test_config() -> RouterConfig {
        RouterConfig {
            strategy: ShardingStrategy::Hash {
                key_column: "user_id".into(),
                num_shards: 4,
            },
            shards: vec![
                ShardConfig { id: ShardId(0), hosts: vec!["host0".into()], weight: 1, status: ShardStatus::Active },
                ShardConfig { id: ShardId(1), hosts: vec!["host1".into()], weight: 1, status: ShardStatus::Active },
                ShardConfig { id: ShardId(2), hosts: vec!["host2".into()], weight: 1, status: ShardStatus::Active },
                ShardConfig { id: ShardId(3), hosts: vec!["host3".into()], weight: 1, status: ShardStatus::Active },
            ],
            enable_cross_shard: true,
            retry_count: 3,
        }
    }

    #[test]
    fn test_hash_routing() {
        let router = ShardingRouter::new(test_config()).unwrap();

        let key1 = ShardKey::Int(12345);
        let key2 = ShardKey::Int(12345);

        // Same key should always route to same shard
        let shard1 = router.route_key(&key1).unwrap();
        let shard2 = router.route_key(&key2).unwrap();
        assert_eq!(shard1, shard2);

        // Different keys should distribute across shards
        let mut shards_hit = std::collections::HashSet::new();
        for i in 0..100 {
            let shard = router.route_key(&ShardKey::Int(i)).unwrap();
            shards_hit.insert(shard);
        }
        assert!(shards_hit.len() > 1); // At least 2 shards should be hit
    }

    #[test]
    fn test_range_routing() {
        let config = RouterConfig {
            strategy: ShardingStrategy::Range {
                key_column: "created_at".into(),
                ranges: vec![
                    ShardRange { shard_id: ShardId(0), start: None, end: Some(ShardKey::Int(1000)) },
                    ShardRange { shard_id: ShardId(1), start: Some(ShardKey::Int(1000)), end: Some(ShardKey::Int(2000)) },
                    ShardRange { shard_id: ShardId(2), start: Some(ShardKey::Int(2000)), end: None },
                ],
            },
            shards: vec![
                ShardConfig { id: ShardId(0), hosts: vec!["host0".into()], weight: 1, status: ShardStatus::Active },
                ShardConfig { id: ShardId(1), hosts: vec!["host1".into()], weight: 1, status: ShardStatus::Active },
                ShardConfig { id: ShardId(2), hosts: vec!["host2".into()], weight: 1, status: ShardStatus::Active },
            ],
            enable_cross_shard: true,
            retry_count: 3,
        };

        let router = ShardingRouter::new(config).unwrap();

        assert_eq!(router.route_key(&ShardKey::Int(500)).unwrap(), ShardId(0));
        assert_eq!(router.route_key(&ShardKey::Int(1500)).unwrap(), ShardId(1));
        assert_eq!(router.route_key(&ShardKey::Int(2500)).unwrap(), ShardId(2));
    }

    #[test]
    fn test_consistent_hash() {
        let config = RouterConfig {
            strategy: ShardingStrategy::Consistent {
                key_column: "id".into(),
                virtual_nodes: 100,
            },
            shards: vec![
                ShardConfig { id: ShardId(0), hosts: vec!["host0".into()], weight: 1, status: ShardStatus::Active },
                ShardConfig { id: ShardId(1), hosts: vec!["host1".into()], weight: 1, status: ShardStatus::Active },
            ],
            enable_cross_shard: true,
            retry_count: 3,
        };

        let mut router = ShardingRouter::new(config.clone()).unwrap();

        // Record initial routing
        let mut initial_routing = HashMap::new();
        for i in 0..100 {
            let key = ShardKey::Int(i);
            initial_routing.insert(i, router.route_key(&key).unwrap());
        }

        // Add a shard
        router.add_shard(ShardConfig {
            id: ShardId(2),
            hosts: vec!["host2".into()],
            weight: 1,
            status: ShardStatus::Active,
        }).unwrap();

        // Count how many keys moved
        let mut moved = 0;
        for i in 0..100 {
            let key = ShardKey::Int(i);
            let new_shard = router.route_key(&key).unwrap();
            if initial_routing[&i] != new_shard {
                moved += 1;
            }
        }

        // With consistent hashing, roughly 1/3 should move (adding 1 to 2 shards)
        assert!(moved < 50, "Too many keys moved: {}", moved);
    }

    #[test]
    fn test_scatter_gather() {
        let router = ShardingRouter::new(test_config()).unwrap();

        let all = router.all_shards();
        assert_eq!(all.len(), 4);
    }

    #[test]
    fn test_offline_shard() {
        let mut router = ShardingRouter::new(test_config()).unwrap();

        router.update_status(ShardId(0), ShardStatus::Offline).unwrap();

        let active = router.active_shards();
        assert_eq!(active.len(), 3);
        assert!(!active.contains(&ShardId(0)));
    }
}
```

### 4.9 spec.json

```json
{
  "name": "sharding_router",
  "language": "rust",
  "type": "code",
  "tier": 3,
  "tags": ["database", "sharding", "distributed", "scalability", "phase5"],
  "passing_score": 70
}
```

### 4.10 Solutions Mutantes

```rust
/* Mutant A: Hash sans modulo correct */
pub fn route_key(&self, key: &ShardKey) -> Result<ShardId, ShardingError> {
    let hash = calculate_hash(key);
    // MUTANT: Oublie le modulo
    Ok(ShardId(hash as u32))
}

/* Mutant B: Range routing off-by-one */
fn find_range_shard(&self, key: &ShardKey, ranges: &[ShardRange]) -> Option<ShardId> {
    for range in ranges {
        // MUTANT: < au lieu de <=
        if key < &range.end.clone().unwrap() {
            return Some(range.shard_id);
        }
    }
    None
}

/* Mutant C: Consistent hash sans virtual nodes */
pub fn new(shards: &[ShardConfig], virtual_nodes: u32) -> Self {
    let mut ring = BTreeMap::new();
    for shard in shards {
        // MUTANT: Ignore virtual_nodes, only add one point per shard
        let hash = calculate_hash(&shard.id);
        ring.insert(hash, shard.id);
    }
    // Bad distribution!
}
```

---

## SECTION 5 : COMPRENDRE

### 5.1 Ce que cet exercice enseigne

1. **Sharding strategies** : Hash, range, consistent hash
2. **Routing** : Determiner le bon shard
3. **Resharding** : Migration de donnees lors d'ajout de shards
4. **Hotspots** : Detection et prevention
5. **Cross-shard queries** : Scatter-gather pattern

### 5.2 Visualisation ASCII

```
              SHARDING ARCHITECTURE

    +------------------+
    |  Application     |
    +------------------+
            |
            v
    +------------------+
    | Sharding Router  |
    | - Hash: shard_id |
    |   = hash(key)%N  |
    +------------------+
            |
    +-------+-------+
    |       |       |
    v       v       v
+-------+ +-------+ +-------+
|Shard 0| |Shard 1| |Shard 2|
| 0-33% | |33-66% | |66-100%|
+-------+ +-------+ +-------+

Query: SELECT * FROM users WHERE user_id = 42
Router: hash(42) % 3 = 0 -> Shard 0
```

---

## SECTION 6 : PIEGES -- RECAPITULATIF

| # | Piege | Symptome | Solution |
|---|-------|----------|----------|
| 1 | Hash sans modulo | Invalid shard IDs | Always modulo num_shards |
| 2 | Range boundaries | Keys fall through | Clear inclusive/exclusive |
| 3 | Hotspots | One shard overloaded | Monitor distribution |
| 4 | Cross-shard inefficace | Slow queries | Cache shard keys |
| 5 | Resharding data loss | Missing records | Two-phase migration |

---

## SECTION 7 : QCM

### Question 1
**Quel est l'avantage du consistent hashing?**

A) Meilleure performance
B) Moins de noeuds necessaires
C) Migration minimale lors d'ajout/suppression de noeuds
D) Pas besoin de shard keys

**Reponse : C**

*Explication : Avec le consistent hashing, ajouter ou supprimer un noeud ne deplace qu'une fraction des cles (environ 1/n ou n = nombre de noeuds), contrairement au hash modulo qui redistribue presque tout.*

---

## SECTION 8 : RECAPITULATIF

| Element | Valeur |
|---------|--------|
| **Nom** | sharding_router |
| **Module** | 5.2.12 -- Database Sharding |
| **Difficulte** | 8/10 |
| **Temps estime** | 180 min |
| **XP** | 300 |
| **Concepts cles** | Hash/range/consistent sharding, routing |

---

## SECTION 9 : DEPLOYMENT PACK

```json
{
  "deploy": {
    "hackbrain_version": "5.5.2",
    "exercise_slug": "5.2.12-a-sharding-router"
  }
}
```

---

*HACKBRAIN v5.5.2 -- "Divide and conquer your data"*
*Exercise Quality Score: 95/100*
