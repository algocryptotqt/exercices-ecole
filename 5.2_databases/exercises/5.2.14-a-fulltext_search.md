<thinking>
## Analyse du Concept
- Concept : Full-Text Search Engine
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - La recherche full-text est essentielle pour les applications modernes. L'exercice combine inverted index, tokenization, ranking TF-IDF, et fuzzy matching.

## Combo Base + Bonus
- Exercice de base : Moteur de recherche full-text avec inverted index, tokenization, stemming, et scoring TF-IDF
- Bonus : Implementation de fuzzy search avec Levenshtein distance et highlighting
- Palier bonus : Avance (algorithmes de similarity + NLP basique)
- Progression logique ? OUI - Base = indexation et recherche exacte, Bonus = recherche approximative

## Prerequis & Difficulte
- Prerequis reels : HashMap, iterateurs, manipulation de strings, algorithmes de tri
- Difficulte estimee : 8/10 (base), 9/10 (bonus)
- Coherent avec phase 5 ? OUI

## Aspect Fun/Culture
- Contexte choisi : "The Search for Meaning" - Reference a Elasticsearch et la quete du savoir
- MEME mnemonique : "I'll be indexed" (Terminator)
- Pourquoi c'est fun : Google a change le monde avec la recherche

## Scenarios d'Echec (5 mutants concrets)
1. Mutant A (Tokenization) : Oublie de normaliser en lowercase
2. Mutant B (TF-IDF) : Division par zero quand IDF = 0
3. Mutant C (Inverted Index) : Pas de deduplication des document IDs
4. Mutant D (Boolean) : AND/OR avec mauvaise priorite
5. Mutant E (Ranking) : Score non normalise par longueur du document

## Verdict
VALIDE - Exercice de qualite industrielle couvrant les fondamentaux du full-text search
</thinking>

# Exercice 5.2.14-a : fulltext_search

**Module :**
5.2.14 - Full-Text Search Fundamentals

**Concept :**
a - Full-Text Search Engine (inverted index, tokenization, TF-IDF, boolean queries)

**Difficulte :**
8/10

**Type :**
code

**Tiers :**
2 - Concepts combines

**Langage :**
Rust Edition 2024

**Prerequis :**
- 2.1 - Types primitifs et ownership
- 2.5 - Collections (HashMap, HashSet, Vec)
- 2.6 - String manipulation
- 5.2.2 - Index structures

**Domaines :**
DB, Search, Algo, NLP

**Duree estimee :**
210 min

**XP Base :**
350

**Complexite :**
T2 O(n) x S2 O(n)

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :**
```
src/lib.rs
```

**Dependances autorisees :**
- `std::collections::{HashMap, HashSet, BTreeMap}`
- `std::cmp::Ordering`

**Fonctions/methodes interdites :**
- Crates externes (`tantivy`, `meilisearch`, `elasticsearch`, etc.)
- `unsafe` blocks

### 1.2 Consigne

**CONTEXTE : "The Search for Meaning"**

*"Dans l'immense bibliotheque de Babel, chaque livre contient toutes les combinaisons possibles de lettres. Le defi n'est pas de stocker l'information, mais de la retrouver."* - Inspire de Borges

Les moteurs de recherche full-text transforment des montagnes de texte en reponses instantanees. De Google a Elasticsearch, ils sont au coeur de l'experience utilisateur moderne.

**Ta mission :**

Implementer un `SearchEngine` qui permet de :
1. Indexer des documents avec tokenization et normalisation
2. Construire un inverted index efficace
3. Calculer les scores TF-IDF pour le ranking
4. Executer des requetes booleennes (AND, OR, NOT)
5. Supporter la recherche par phrase
6. Retourner les resultats tries par pertinence

**Entree :**
- `doc_id: u64` - Identifiant unique du document
- `content: &str` - Contenu textuel du document
- `query: &str` - Requete de recherche

**Sortie :**
- `SearchEngine` - Moteur de recherche
- `SearchResult` - Resultats avec scores
- `SearchError` - En cas d'erreur

**Contraintes :**
- Les tokens doivent etre normalises (lowercase, ponctuation supprimee)
- L'index doit supporter les mises a jour incrementales
- Le scoring doit utiliser TF-IDF standard
- Les requetes booleennes doivent respecter la priorite (NOT > AND > OR)

**Exemples :**

| Operation | Input | Resultat |
|-----------|-------|----------|
| `engine.index(1, "Hello world")` | Document | `Ok(())` |
| `engine.search("hello")` | Requete simple | `[SearchResult { doc_id: 1, score: 0.5 }]` |
| `engine.search("hello AND world")` | Boolean AND | Documents avec les deux termes |

### 1.2.2 Consigne Academique

Implementer un moteur de recherche full-text supportant l'indexation de documents, la construction d'un inverted index, le scoring TF-IDF, et les requetes booleennes. Le moteur doit retourner les resultats tries par pertinence.

### 1.3 Prototype

```rust
use std::collections::{HashMap, HashSet, BTreeMap};

#[derive(Debug, Clone)]
pub struct Document {
    pub id: u64,
    pub content: String,
    pub tokens: Vec<String>,
    pub term_frequencies: HashMap<String, usize>,
}

#[derive(Debug, Clone)]
pub struct SearchResult {
    pub doc_id: u64,
    pub score: f64,
    pub matched_terms: Vec<String>,
    pub snippet: Option<String>,
}

#[derive(Debug, Clone)]
pub struct InvertedIndex {
    // term -> (doc_id -> positions)
    pub index: HashMap<String, HashMap<u64, Vec<usize>>>,
    pub document_count: usize,
    pub document_lengths: HashMap<u64, usize>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum QueryOperator {
    And,
    Or,
    Not,
    Phrase,
}

#[derive(Debug, Clone)]
pub enum QueryNode {
    Term(String),
    Phrase(Vec<String>),
    Binary {
        op: QueryOperator,
        left: Box<QueryNode>,
        right: Box<QueryNode>,
    },
    Unary {
        op: QueryOperator,
        operand: Box<QueryNode>,
    },
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum SearchError {
    DocumentNotFound,
    InvalidQuery,
    EmptyQuery,
    IndexCorrupted,
}

pub struct SearchEngine {
    documents: HashMap<u64, Document>,
    index: InvertedIndex,
    stopwords: HashSet<String>,
}

impl SearchEngine {
    pub fn new() -> Self;
    pub fn with_stopwords(stopwords: Vec<String>) -> Self;

    // Indexing
    pub fn index(&mut self, doc_id: u64, content: &str) -> Result<(), SearchError>;
    pub fn index_batch(&mut self, docs: Vec<(u64, String)>) -> Result<usize, SearchError>;
    pub fn remove(&mut self, doc_id: u64) -> Result<(), SearchError>;
    pub fn update(&mut self, doc_id: u64, content: &str) -> Result<(), SearchError>;

    // Tokenization
    pub fn tokenize(&self, text: &str) -> Vec<String>;
    pub fn normalize(&self, token: &str) -> String;

    // Search
    pub fn search(&self, query: &str) -> Result<Vec<SearchResult>, SearchError>;
    pub fn search_with_limit(&self, query: &str, limit: usize) -> Result<Vec<SearchResult>, SearchError>;
    pub fn search_boolean(&self, query: QueryNode) -> Result<HashSet<u64>, SearchError>;

    // Scoring
    pub fn calculate_tf(&self, term: &str, doc_id: u64) -> f64;
    pub fn calculate_idf(&self, term: &str) -> f64;
    pub fn calculate_tfidf(&self, term: &str, doc_id: u64) -> f64;
    pub fn score_document(&self, terms: &[String], doc_id: u64) -> f64;

    // Query parsing
    pub fn parse_query(&self, query: &str) -> Result<QueryNode, SearchError>;

    // Utilities
    pub fn document_count(&self) -> usize;
    pub fn term_count(&self) -> usize;
    pub fn get_document(&self, doc_id: u64) -> Option<&Document>;
    pub fn suggest(&self, prefix: &str, limit: usize) -> Vec<String>;
}

impl Default for SearchEngine {
    fn default() -> Self;
}

impl InvertedIndex {
    pub fn new() -> Self;
    pub fn add_document(&mut self, doc_id: u64, tokens: &[String]);
    pub fn remove_document(&mut self, doc_id: u64);
    pub fn get_postings(&self, term: &str) -> Option<&HashMap<u64, Vec<usize>>>;
    pub fn get_document_frequency(&self, term: &str) -> usize;
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 L'Inverted Index

L'inverted index est la structure de donnees fondamentale de tout moteur de recherche. Au lieu de mapper `document -> mots`, il mappe `mot -> documents`. Cette inversion permet des recherches en O(1) pour trouver tous les documents contenant un terme.

```
Avant (forward index):
  doc1 -> ["the", "quick", "brown", "fox"]
  doc2 -> ["the", "lazy", "dog"]

Apres (inverted index):
  "the"   -> [doc1, doc2]
  "quick" -> [doc1]
  "brown" -> [doc1]
  "fox"   -> [doc1]
  "lazy"  -> [doc2]
  "dog"   -> [doc2]
```

### 2.2 TF-IDF

TF-IDF (Term Frequency - Inverse Document Frequency) est la formule magique du ranking :

```
TF(t,d) = nombre d'occurrences de t dans d / nombre total de termes dans d
IDF(t) = log(N / df(t))  ou N = nombre total de documents, df(t) = docs contenant t
TF-IDF(t,d) = TF(t,d) * IDF(t)
```

Un terme frequent dans un document mais rare globalement aura un score eleve.

### 2.3 BM25

BM25 (Best Match 25) est l'evolution de TF-IDF utilisee par Elasticsearch et Lucene. Il ajoute la saturation des termes et la normalisation par longueur du document :

```
BM25(t,d) = IDF(t) * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * |d|/avgdl))
```

---

## SECTION 2.5 : DANS LA VRAIE VIE

### Metiers concernes

| Metier | Utilisation du full-text search |
|--------|--------------------------------|
| **Search Engineer** | Design de ranking, relevance tuning |
| **Backend Developer** | Integration Elasticsearch/Meilisearch |
| **Data Engineer** | Pipelines d'indexation, ETL |
| **ML Engineer** | Semantic search, embeddings |
| **Product Manager** | Analyse des recherches utilisateurs |

### Cas d'usage concrets

1. **E-commerce** : Recherche produits avec facettes et suggestions
2. **Documentation** : Search dans les docs techniques (ReadTheDocs, GitBook)
3. **Log analysis** : Elasticsearch pour les logs (ELK stack)
4. **Social media** : Recherche de posts, hashtags, utilisateurs

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ cargo test
   Compiling fulltext_search v0.1.0
    Finished test [unoptimized + debuginfo] target(s)
     Running unittests src/lib.rs

running 18 tests
test tests::test_index_document ... ok
test tests::test_index_batch ... ok
test tests::test_tokenize ... ok
test tests::test_normalize ... ok
test tests::test_search_simple ... ok
test tests::test_search_multiple_terms ... ok
test tests::test_search_boolean_and ... ok
test tests::test_search_boolean_or ... ok
test tests::test_search_boolean_not ... ok
test tests::test_search_phrase ... ok
test tests::test_tf_calculation ... ok
test tests::test_idf_calculation ... ok
test tests::test_tfidf_scoring ... ok
test tests::test_ranking_order ... ok
test tests::test_remove_document ... ok
test tests::test_update_document ... ok
test tests::test_stopwords ... ok
test tests::test_suggest ... ok

test result: ok. 18 passed; 0 failed
```

### 3.1 BONUS AVANCE (OPTIONNEL)

**Difficulte Bonus :**
9/10

**Recompense :**
XP x3

**Time Complexity attendue :**
O(n * m) pour fuzzy search (n = longueur requete, m = taille vocabulaire)

**Space Complexity attendue :**
O(k) ou k = nombre de candidats

**Domaines Bonus :**
`NLP, Algo`

#### 3.1.1 Consigne Bonus

**"Fuzzy Logic"**

*"L'utilisateur ne sait pas toujours ce qu'il cherche, et encore moins comment l'ecrire."*

**Ta mission bonus :**

Implementer la **recherche approximative** (fuzzy search) avec :

1. Distance de Levenshtein pour la tolerance aux fautes de frappe
2. N-grams pour l'indexation approximative
3. Highlighting des termes matches dans les resultats
4. Suggestions "Did you mean?"

**Contraintes :**
- Distance max configurable (default: 2)
- N-grams de taille 2-3 (bigrams/trigrams)
- Highlighting avec balises configurables

#### 3.1.2 Prototype Bonus

```rust
#[derive(Debug, Clone)]
pub struct FuzzyConfig {
    pub max_distance: usize,
    pub ngram_size: usize,
    pub prefix_length: usize,
}

#[derive(Debug, Clone)]
pub struct HighlightedResult {
    pub doc_id: u64,
    pub score: f64,
    pub highlighted_content: String,
    pub matched_positions: Vec<(usize, usize)>,
}

impl SearchEngine {
    pub fn search_fuzzy(
        &self,
        query: &str,
        config: &FuzzyConfig,
    ) -> Result<Vec<SearchResult>, SearchError>;

    pub fn levenshtein_distance(a: &str, b: &str) -> usize;

    pub fn generate_ngrams(text: &str, n: usize) -> Vec<String>;

    pub fn highlight(
        &self,
        doc_id: u64,
        terms: &[String],
        open_tag: &str,
        close_tag: &str,
    ) -> Result<HighlightedResult, SearchError>;

    pub fn did_you_mean(&self, query: &str, limit: usize) -> Vec<String>;
}
```

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette - Tableau des tests

| Test | Input | Expected | Points | Categorie |
|------|-------|----------|--------|-----------|
| `index_document` | 1 doc | `Ok(())` | 5 | Basic |
| `index_batch` | 10 docs | `Ok(10)` | 5 | Basic |
| `tokenize` | "Hello, World!" | ["hello", "world"] | 5 | Tokenization |
| `normalize` | "HELLO" | "hello" | 3 | Tokenization |
| `search_simple` | "hello" | Results | 10 | Search |
| `search_multi` | "hello world" | Results | 10 | Search |
| `boolean_and` | "a AND b" | Intersection | 10 | Boolean |
| `boolean_or` | "a OR b" | Union | 5 | Boolean |
| `boolean_not` | "a NOT b" | Difference | 5 | Boolean |
| `phrase_search` | "hello world" | Exact phrase | 10 | Phrase |
| `tf_calc` | Term in doc | TF value | 5 | Scoring |
| `idf_calc` | Term global | IDF value | 5 | Scoring |
| `tfidf_score` | Combined | TF-IDF | 5 | Scoring |
| `ranking` | Multiple results | Sorted by score | 10 | Ranking |
| `remove_doc` | Existing doc | `Ok(())` | 3 | CRUD |
| `update_doc` | Existing doc | Updated index | 3 | CRUD |
| `stopwords` | "the a an" | Filtered | 3 | Filter |

**Score minimum pour validation : 70/100**

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_index_document() {
        let mut engine = SearchEngine::new();
        let result = engine.index(1, "The quick brown fox jumps over the lazy dog");
        assert!(result.is_ok());
        assert_eq!(engine.document_count(), 1);
    }

    #[test]
    fn test_index_batch() {
        let mut engine = SearchEngine::new();
        let docs = vec![
            (1, "Hello world".to_string()),
            (2, "World of rust".to_string()),
            (3, "Rust programming".to_string()),
        ];
        let result = engine.index_batch(docs);
        assert_eq!(result, Ok(3));
    }

    #[test]
    fn test_tokenize() {
        let engine = SearchEngine::new();
        let tokens = engine.tokenize("Hello, World! How are you?");
        assert_eq!(tokens, vec!["hello", "world", "how", "are", "you"]);
    }

    #[test]
    fn test_normalize() {
        let engine = SearchEngine::new();
        assert_eq!(engine.normalize("HELLO"), "hello");
        assert_eq!(engine.normalize("Hello123"), "hello123");
    }

    #[test]
    fn test_search_simple() {
        let mut engine = SearchEngine::new();
        engine.index(1, "Hello world").unwrap();
        engine.index(2, "World of code").unwrap();

        let results = engine.search("world").unwrap();
        assert_eq!(results.len(), 2);
    }

    #[test]
    fn test_search_multiple_terms() {
        let mut engine = SearchEngine::new();
        engine.index(1, "The quick brown fox").unwrap();
        engine.index(2, "The lazy brown dog").unwrap();
        engine.index(3, "Quick thinking").unwrap();

        let results = engine.search("quick brown").unwrap();
        assert!(results.len() >= 1);
        // Doc 1 should rank highest (has both terms)
        assert_eq!(results[0].doc_id, 1);
    }

    #[test]
    fn test_search_boolean_and() {
        let mut engine = SearchEngine::new();
        engine.index(1, "apple banana").unwrap();
        engine.index(2, "apple orange").unwrap();
        engine.index(3, "banana cherry").unwrap();

        let query = QueryNode::Binary {
            op: QueryOperator::And,
            left: Box::new(QueryNode::Term("apple".to_string())),
            right: Box::new(QueryNode::Term("banana".to_string())),
        };

        let results = engine.search_boolean(query).unwrap();
        assert_eq!(results.len(), 1);
        assert!(results.contains(&1));
    }

    #[test]
    fn test_search_boolean_or() {
        let mut engine = SearchEngine::new();
        engine.index(1, "apple").unwrap();
        engine.index(2, "banana").unwrap();
        engine.index(3, "cherry").unwrap();

        let query = QueryNode::Binary {
            op: QueryOperator::Or,
            left: Box::new(QueryNode::Term("apple".to_string())),
            right: Box::new(QueryNode::Term("banana".to_string())),
        };

        let results = engine.search_boolean(query).unwrap();
        assert_eq!(results.len(), 2);
    }

    #[test]
    fn test_search_boolean_not() {
        let mut engine = SearchEngine::new();
        engine.index(1, "apple banana").unwrap();
        engine.index(2, "apple orange").unwrap();

        let query = QueryNode::Binary {
            op: QueryOperator::And,
            left: Box::new(QueryNode::Term("apple".to_string())),
            right: Box::new(QueryNode::Unary {
                op: QueryOperator::Not,
                operand: Box::new(QueryNode::Term("banana".to_string())),
            }),
        };

        let results = engine.search_boolean(query).unwrap();
        assert_eq!(results.len(), 1);
        assert!(results.contains(&2));
    }

    #[test]
    fn test_search_phrase() {
        let mut engine = SearchEngine::new();
        engine.index(1, "hello world program").unwrap();
        engine.index(2, "world hello test").unwrap();

        // Search for exact phrase "hello world"
        let query = QueryNode::Phrase(vec!["hello".to_string(), "world".to_string()]);
        let results = engine.search_boolean(query).unwrap();
        assert_eq!(results.len(), 1);
        assert!(results.contains(&1));
    }

    #[test]
    fn test_tf_calculation() {
        let mut engine = SearchEngine::new();
        engine.index(1, "apple apple banana").unwrap();

        let tf = engine.calculate_tf("apple", 1);
        assert!((tf - 2.0/3.0).abs() < 0.001);
    }

    #[test]
    fn test_idf_calculation() {
        let mut engine = SearchEngine::new();
        engine.index(1, "apple banana").unwrap();
        engine.index(2, "apple cherry").unwrap();
        engine.index(3, "banana cherry").unwrap();

        // apple appears in 2/3 docs
        let idf_apple = engine.calculate_idf("apple");
        // banana appears in 2/3 docs
        let idf_banana = engine.calculate_idf("banana");

        assert!((idf_apple - idf_banana).abs() < 0.001);
    }

    #[test]
    fn test_tfidf_scoring() {
        let mut engine = SearchEngine::new();
        engine.index(1, "rust rust rust python").unwrap();
        engine.index(2, "python python java").unwrap();

        let tfidf_rust_doc1 = engine.calculate_tfidf("rust", 1);
        let tfidf_python_doc1 = engine.calculate_tfidf("python", 1);

        // rust is more relevant to doc1 (appears more and only in doc1)
        assert!(tfidf_rust_doc1 > tfidf_python_doc1);
    }

    #[test]
    fn test_ranking_order() {
        let mut engine = SearchEngine::new();
        engine.index(1, "rust programming language").unwrap();
        engine.index(2, "rust rust rust is awesome").unwrap();
        engine.index(3, "python programming").unwrap();

        let results = engine.search("rust").unwrap();
        // Doc 2 should rank higher (more occurrences of "rust")
        assert_eq!(results[0].doc_id, 2);
    }

    #[test]
    fn test_remove_document() {
        let mut engine = SearchEngine::new();
        engine.index(1, "hello world").unwrap();
        engine.index(2, "world test").unwrap();

        engine.remove(1).unwrap();

        assert_eq!(engine.document_count(), 1);
        let results = engine.search("hello").unwrap();
        assert!(results.is_empty());
    }

    #[test]
    fn test_update_document() {
        let mut engine = SearchEngine::new();
        engine.index(1, "old content").unwrap();
        engine.update(1, "new content").unwrap();

        let results_old = engine.search("old").unwrap();
        let results_new = engine.search("new").unwrap();

        assert!(results_old.is_empty());
        assert_eq!(results_new.len(), 1);
    }

    #[test]
    fn test_stopwords() {
        let stopwords = vec!["the".to_string(), "a".to_string(), "an".to_string()];
        let engine = SearchEngine::with_stopwords(stopwords);

        let tokens = engine.tokenize("The quick brown fox");
        assert!(!tokens.contains(&"the".to_string()));
    }

    #[test]
    fn test_suggest() {
        let mut engine = SearchEngine::new();
        engine.index(1, "programming rust").unwrap();
        engine.index(2, "program design").unwrap();
        engine.index(3, "programmer life").unwrap();

        let suggestions = engine.suggest("prog", 5);
        assert!(suggestions.contains(&"programming".to_string()));
        assert!(suggestions.contains(&"program".to_string()));
        assert!(suggestions.contains(&"programmer".to_string()));
    }
}
```

### 4.3 Solution de reference

```rust
use std::collections::{HashMap, HashSet, BTreeMap};

#[derive(Debug, Clone)]
pub struct Document {
    pub id: u64,
    pub content: String,
    pub tokens: Vec<String>,
    pub term_frequencies: HashMap<String, usize>,
}

#[derive(Debug, Clone)]
pub struct SearchResult {
    pub doc_id: u64,
    pub score: f64,
    pub matched_terms: Vec<String>,
    pub snippet: Option<String>,
}

#[derive(Debug, Clone)]
pub struct InvertedIndex {
    pub index: HashMap<String, HashMap<u64, Vec<usize>>>,
    pub document_count: usize,
    pub document_lengths: HashMap<u64, usize>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum QueryOperator {
    And,
    Or,
    Not,
    Phrase,
}

#[derive(Debug, Clone)]
pub enum QueryNode {
    Term(String),
    Phrase(Vec<String>),
    Binary {
        op: QueryOperator,
        left: Box<QueryNode>,
        right: Box<QueryNode>,
    },
    Unary {
        op: QueryOperator,
        operand: Box<QueryNode>,
    },
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum SearchError {
    DocumentNotFound,
    InvalidQuery,
    EmptyQuery,
    IndexCorrupted,
}

pub struct SearchEngine {
    documents: HashMap<u64, Document>,
    index: InvertedIndex,
    stopwords: HashSet<String>,
}

impl InvertedIndex {
    pub fn new() -> Self {
        Self {
            index: HashMap::new(),
            document_count: 0,
            document_lengths: HashMap::new(),
        }
    }

    pub fn add_document(&mut self, doc_id: u64, tokens: &[String]) {
        self.document_lengths.insert(doc_id, tokens.len());

        for (position, token) in tokens.iter().enumerate() {
            self.index
                .entry(token.clone())
                .or_insert_with(HashMap::new)
                .entry(doc_id)
                .or_insert_with(Vec::new)
                .push(position);
        }

        self.document_count += 1;
    }

    pub fn remove_document(&mut self, doc_id: u64) {
        self.document_lengths.remove(&doc_id);

        let mut empty_terms = Vec::new();

        for (term, postings) in self.index.iter_mut() {
            postings.remove(&doc_id);
            if postings.is_empty() {
                empty_terms.push(term.clone());
            }
        }

        for term in empty_terms {
            self.index.remove(&term);
        }

        self.document_count = self.document_count.saturating_sub(1);
    }

    pub fn get_postings(&self, term: &str) -> Option<&HashMap<u64, Vec<usize>>> {
        self.index.get(term)
    }

    pub fn get_document_frequency(&self, term: &str) -> usize {
        self.index.get(term).map_or(0, |p| p.len())
    }
}

impl SearchEngine {
    pub fn new() -> Self {
        Self {
            documents: HashMap::new(),
            index: InvertedIndex::new(),
            stopwords: HashSet::new(),
        }
    }

    pub fn with_stopwords(stopwords: Vec<String>) -> Self {
        Self {
            documents: HashMap::new(),
            index: InvertedIndex::new(),
            stopwords: stopwords.into_iter().collect(),
        }
    }

    pub fn index(&mut self, doc_id: u64, content: &str) -> Result<(), SearchError> {
        let tokens = self.tokenize(content);
        let mut term_frequencies = HashMap::new();

        for token in &tokens {
            *term_frequencies.entry(token.clone()).or_insert(0) += 1;
        }

        let document = Document {
            id: doc_id,
            content: content.to_string(),
            tokens: tokens.clone(),
            term_frequencies,
        };

        self.documents.insert(doc_id, document);
        self.index.add_document(doc_id, &tokens);

        Ok(())
    }

    pub fn index_batch(&mut self, docs: Vec<(u64, String)>) -> Result<usize, SearchError> {
        let count = docs.len();
        for (doc_id, content) in docs {
            self.index(doc_id, &content)?;
        }
        Ok(count)
    }

    pub fn remove(&mut self, doc_id: u64) -> Result<(), SearchError> {
        if self.documents.remove(&doc_id).is_none() {
            return Err(SearchError::DocumentNotFound);
        }
        self.index.remove_document(doc_id);
        Ok(())
    }

    pub fn update(&mut self, doc_id: u64, content: &str) -> Result<(), SearchError> {
        self.remove(doc_id)?;
        self.index(doc_id, content)
    }

    pub fn tokenize(&self, text: &str) -> Vec<String> {
        text.chars()
            .map(|c| if c.is_alphanumeric() { c } else { ' ' })
            .collect::<String>()
            .split_whitespace()
            .map(|s| self.normalize(s))
            .filter(|s| !s.is_empty() && !self.stopwords.contains(s))
            .collect()
    }

    pub fn normalize(&self, token: &str) -> String {
        token.to_lowercase()
    }

    pub fn search(&self, query: &str) -> Result<Vec<SearchResult>, SearchError> {
        self.search_with_limit(query, usize::MAX)
    }

    pub fn search_with_limit(&self, query: &str, limit: usize) -> Result<Vec<SearchResult>, SearchError> {
        let terms = self.tokenize(query);

        if terms.is_empty() {
            return Err(SearchError::EmptyQuery);
        }

        // Find all documents containing at least one term
        let mut doc_scores: HashMap<u64, (f64, Vec<String>)> = HashMap::new();

        for term in &terms {
            if let Some(postings) = self.index.get_postings(term) {
                for &doc_id in postings.keys() {
                    let score = self.calculate_tfidf(term, doc_id);
                    let entry = doc_scores.entry(doc_id).or_insert((0.0, Vec::new()));
                    entry.0 += score;
                    entry.1.push(term.clone());
                }
            }
        }

        let mut results: Vec<SearchResult> = doc_scores
            .into_iter()
            .map(|(doc_id, (score, matched_terms))| SearchResult {
                doc_id,
                score,
                matched_terms,
                snippet: None,
            })
            .collect();

        // Sort by score descending
        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));
        results.truncate(limit);

        Ok(results)
    }

    pub fn search_boolean(&self, query: QueryNode) -> Result<HashSet<u64>, SearchError> {
        match query {
            QueryNode::Term(term) => {
                let normalized = self.normalize(&term);
                Ok(self.index
                    .get_postings(&normalized)
                    .map(|p| p.keys().cloned().collect())
                    .unwrap_or_default())
            }
            QueryNode::Phrase(terms) => {
                if terms.is_empty() {
                    return Ok(HashSet::new());
                }

                let normalized: Vec<String> = terms.iter().map(|t| self.normalize(t)).collect();

                // Start with documents containing the first term
                let first_postings = match self.index.get_postings(&normalized[0]) {
                    Some(p) => p,
                    None => return Ok(HashSet::new()),
                };

                let mut results = HashSet::new();

                'doc: for (&doc_id, positions) in first_postings {
                    for &start_pos in positions {
                        let mut found = true;
                        for (i, term) in normalized.iter().enumerate().skip(1) {
                            let expected_pos = start_pos + i;
                            if let Some(term_postings) = self.index.get_postings(term) {
                                if let Some(doc_positions) = term_postings.get(&doc_id) {
                                    if !doc_positions.contains(&expected_pos) {
                                        found = false;
                                        break;
                                    }
                                } else {
                                    found = false;
                                    break;
                                }
                            } else {
                                found = false;
                                break;
                            }
                        }
                        if found {
                            results.insert(doc_id);
                            continue 'doc;
                        }
                    }
                }

                Ok(results)
            }
            QueryNode::Binary { op, left, right } => {
                let left_results = self.search_boolean(*left)?;
                let right_results = self.search_boolean(*right)?;

                match op {
                    QueryOperator::And => {
                        Ok(left_results.intersection(&right_results).cloned().collect())
                    }
                    QueryOperator::Or => {
                        Ok(left_results.union(&right_results).cloned().collect())
                    }
                    _ => Err(SearchError::InvalidQuery),
                }
            }
            QueryNode::Unary { op, operand } => {
                match op {
                    QueryOperator::Not => {
                        let operand_results = self.search_boolean(*operand)?;
                        let all_docs: HashSet<u64> = self.documents.keys().cloned().collect();
                        Ok(all_docs.difference(&operand_results).cloned().collect())
                    }
                    _ => Err(SearchError::InvalidQuery),
                }
            }
        }
    }

    pub fn calculate_tf(&self, term: &str, doc_id: u64) -> f64 {
        if let Some(doc) = self.documents.get(&doc_id) {
            let count = *doc.term_frequencies.get(term).unwrap_or(&0);
            let total = doc.tokens.len();
            if total > 0 {
                count as f64 / total as f64
            } else {
                0.0
            }
        } else {
            0.0
        }
    }

    pub fn calculate_idf(&self, term: &str) -> f64 {
        let n = self.index.document_count as f64;
        let df = self.index.get_document_frequency(term) as f64;

        if df > 0.0 && n > 0.0 {
            (n / df).ln()
        } else {
            0.0
        }
    }

    pub fn calculate_tfidf(&self, term: &str, doc_id: u64) -> f64 {
        self.calculate_tf(term, doc_id) * self.calculate_idf(term)
    }

    pub fn score_document(&self, terms: &[String], doc_id: u64) -> f64 {
        terms.iter().map(|t| self.calculate_tfidf(t, doc_id)).sum()
    }

    pub fn parse_query(&self, query: &str) -> Result<QueryNode, SearchError> {
        // Simple parser - for production, use a proper parser
        let tokens = self.tokenize(query);
        if tokens.is_empty() {
            return Err(SearchError::EmptyQuery);
        }

        // Default: OR all terms
        let mut result = QueryNode::Term(tokens[0].clone());
        for term in tokens.iter().skip(1) {
            result = QueryNode::Binary {
                op: QueryOperator::Or,
                left: Box::new(result),
                right: Box::new(QueryNode::Term(term.clone())),
            };
        }

        Ok(result)
    }

    pub fn document_count(&self) -> usize {
        self.documents.len()
    }

    pub fn term_count(&self) -> usize {
        self.index.index.len()
    }

    pub fn get_document(&self, doc_id: u64) -> Option<&Document> {
        self.documents.get(&doc_id)
    }

    pub fn suggest(&self, prefix: &str, limit: usize) -> Vec<String> {
        let normalized_prefix = self.normalize(prefix);

        self.index
            .index
            .keys()
            .filter(|term| term.starts_with(&normalized_prefix))
            .take(limit)
            .cloned()
            .collect()
    }
}

impl Default for SearchEngine {
    fn default() -> Self {
        Self::new()
    }
}
```

### 4.4 Mutants

```rust
// MUTANT_A: Oublie de normaliser en lowercase
pub fn tokenize_mutant_a(&self, text: &str) -> Vec<String> {
    text.chars()
        .map(|c| if c.is_alphanumeric() { c } else { ' ' })
        .collect::<String>()
        .split_whitespace()
        .map(|s| s.to_string())  // BUG: pas de .to_lowercase()
        .filter(|s| !s.is_empty())
        .collect()
}

// MUTANT_B: Division par zero quand document vide
pub fn calculate_tf_mutant_b(&self, term: &str, doc_id: u64) -> f64 {
    if let Some(doc) = self.documents.get(&doc_id) {
        let count = *doc.term_frequencies.get(term).unwrap_or(&0);
        // BUG: pas de check si doc.tokens.len() == 0
        count as f64 / doc.tokens.len() as f64
    } else {
        0.0
    }
}

// MUTANT_C: Pas de deduplication des positions
pub fn add_document_mutant_c(&mut self, doc_id: u64, tokens: &[String]) {
    for (position, token) in tokens.iter().enumerate() {
        self.index
            .entry(token.clone())
            .or_insert_with(HashMap::new)
            .entry(doc_id)
            .or_insert_with(Vec::new)
            .push(position);
        // BUG: si meme token apparait plusieurs fois, on ajoute plusieurs fois
        // le meme doc_id dans certains cas
    }
}

// MUTANT_D: AND/OR avec mauvaise priorite
pub fn search_boolean_mutant_d(&self, query: QueryNode) -> Result<HashSet<u64>, SearchError> {
    // BUG: OR evalue avant AND (mauvaise priorite)
    match query {
        QueryNode::Binary { op: QueryOperator::And, left, right } => {
            let left_results = self.search_boolean(*left)?;
            let right_results = self.search_boolean(*right)?;
            // BUG: utilise union au lieu d'intersection
            Ok(left_results.union(&right_results).cloned().collect())
        }
        _ => Ok(HashSet::new())
    }
}

// MUTANT_E: Score non normalise par longueur du document
pub fn calculate_tfidf_mutant_e(&self, term: &str, doc_id: u64) -> f64 {
    // BUG: utilise count absolu au lieu de TF normalise
    if let Some(doc) = self.documents.get(&doc_id) {
        let count = *doc.term_frequencies.get(term).unwrap_or(&0) as f64;
        let idf = self.calculate_idf(term);
        count * idf  // ERREUR: devrait etre (count/total) * idf
    } else {
        0.0
    }
}
```

### 4.5 spec.json

```json
{
  "exercise_id": "5.2.14-a",
  "title": "fulltext_search",
  "module": "5.2.14",
  "concept": "Full-Text Search Engine",
  "difficulty": 8,
  "xp_base": 350,
  "xp_bonus_multiplier": 3,
  "time_estimate_minutes": 210,
  "languages": ["rust"],
  "rust_edition": "2024",
  "tags": ["database", "search", "inverted-index", "tfidf", "nlp"],
  "prerequisites": ["2.1", "2.5", "2.6", "5.2.2"],
  "validation": {
    "min_score": 70,
    "tests_required": [
      "test_index_document",
      "test_search_simple",
      "test_search_boolean_and",
      "test_tfidf_scoring",
      "test_ranking_order"
    ]
  },
  "submission": {
    "files": ["src/lib.rs"],
    "forbidden_crates": ["tantivy", "meilisearch", "elasticsearch"],
    "forbidden_keywords": ["unsafe"]
  },
  "complexity": {
    "time": "O(n)",
    "space": "O(n)"
  },
  "learning_objectives": [
    "Comprendre l'architecture d'un moteur de recherche",
    "Implementer un inverted index efficace",
    "Maitriser le scoring TF-IDF",
    "Gerer les requetes booleennes complexes"
  ],
  "competencies": ["DB-SEARCH-001", "ALGO-INDEX-001", "NLP-TOKENIZE-001"]
}
```

---

## SECTION 5 : COMPRENDRE

### 5.1 LDA (Logical Data Architecture)

```
SearchEngine
+--------------------------------------------------+
|  documents: HashMap<u64, Document>               |
|  index: InvertedIndex                            |
|  stopwords: HashSet<String>                      |
+--------------------------------------------------+
                    |
          +---------+---------+
          |                   |
          v                   v
      Document          InvertedIndex
  +---------------+   +--------------------------------+
  | id: u64       |   | index: HashMap<term, postings> |
  | content       |   | document_count                 |
  | tokens[]      |   | document_lengths               |
  | term_freq{}   |   +--------------------------------+
  +---------------+            |
                               v
                    HashMap<doc_id, Vec<position>>
                    +---------------------------+
                    | doc1 -> [0, 5, 12]        |
                    | doc2 -> [3, 8]            |
                    +---------------------------+
```

### 5.2 Pseudocode de l'algorithme principal

```
FUNCTION index(doc_id, content):
    tokens = tokenize(content)
    term_freq = count_frequencies(tokens)

    document = Document(doc_id, content, tokens, term_freq)
    documents[doc_id] = document

    FOR position, token IN enumerate(tokens):
        inverted_index[token][doc_id].append(position)

    document_count += 1
    RETURN Ok

FUNCTION search(query):
    query_terms = tokenize(query)

    IF query_terms IS EMPTY:
        RETURN Err(EmptyQuery)

    doc_scores = {}

    FOR term IN query_terms:
        IF term IN inverted_index:
            FOR doc_id IN inverted_index[term]:
                score = calculate_tfidf(term, doc_id)
                doc_scores[doc_id] += score

    results = sort_by_score_desc(doc_scores)
    RETURN results

FUNCTION calculate_tfidf(term, doc_id):
    tf = term_count_in_doc / total_terms_in_doc
    idf = log(total_docs / docs_containing_term)
    RETURN tf * idf

FUNCTION search_boolean(query_node):
    SWITCH query_node.type:
        CASE Term:
            RETURN docs_containing(query_node.term)
        CASE And:
            left = search_boolean(query_node.left)
            right = search_boolean(query_node.right)
            RETURN intersection(left, right)
        CASE Or:
            left = search_boolean(query_node.left)
            right = search_boolean(query_node.right)
            RETURN union(left, right)
        CASE Not:
            operand = search_boolean(query_node.operand)
            RETURN all_docs - operand
        CASE Phrase:
            RETURN find_consecutive_positions(query_node.terms)
```

### 5.3 Representation visuelle

```
Documents:
  doc1: "the quick brown fox"
  doc2: "the lazy brown dog"
  doc3: "quick brown thinking"

Inverted Index:
  "quick"  -> {doc1: [1], doc3: [0]}
  "brown"  -> {doc1: [2], doc2: [2], doc3: [1]}
  "fox"    -> {doc1: [3]}
  "lazy"   -> {doc2: [1]}
  "dog"    -> {doc2: [3]}
  "thinking" -> {doc3: [2]}

Query: "quick brown"

Step 1: Find docs with "quick": {doc1, doc3}
Step 2: Find docs with "brown": {doc1, doc2, doc3}
Step 3: Calculate TF-IDF for each:

  doc1: TF("quick")=1/4, IDF("quick")=log(3/2)=0.405
        TF("brown")=1/4, IDF("brown")=log(3/3)=0
        Score = 0.25*0.405 + 0.25*0 = 0.101

  doc2: TF("quick")=0, TF("brown")=1/4
        Score = 0 + 0 = 0

  doc3: TF("quick")=1/3, TF("brown")=1/3
        Score = 0.33*0.405 + 0.33*0 = 0.134

Result: [doc3 (0.134), doc1 (0.101), doc2 (0)]
```

---

## SECTION 6 : PIEGES RECAPITULATIF

### 6.1 Erreurs frequentes

| Piege | Description | Solution |
|-------|-------------|----------|
| Case sensitivity | "Hello" != "hello" | Toujours normaliser avant indexation et recherche |
| Division by zero | Document vide | Verifier tokens.len() > 0 |
| IDF undefined | log(N/0) | df = 0 -> IDF = 0 ou N |
| Position tracking | Phrase search echoue | Stocker positions exactes dans l'index |
| Stopwords | "the" pollue les resultats | Filtrer a l'indexation ET a la recherche |

### 6.2 MEME Mnemonique

```
"I'll be indexed"
     ____
    |    |
    | TF |  x  | IDF |  =  | Score |
    |____|     |_____|     |_______|
      |           |            |
   Frequent    Rare        Relevant
   in doc      globally     result
```

---

## SECTION 7 : QCM

### Question 1
Quelle est la complexite de recherche dans un inverted index pour un terme ?

- [x] A) O(1) pour trouver les postings
- [ ] B) O(n) ou n = nombre de documents
- [ ] C) O(log n)
- [ ] D) O(n^2)

### Question 2
Dans TF-IDF, qu'est-ce qui rend un terme "important" ?

- [ ] A) Il apparait souvent dans tous les documents
- [x] B) Il apparait souvent dans CE document mais rarement ailleurs
- [ ] C) Il est court
- [ ] D) Il est au debut du document

### Question 3
Pourquoi stocker les positions dans l'inverted index ?

- [ ] A) Pour economiser de la memoire
- [ ] B) Pour accelerer les recherches simples
- [x] C) Pour supporter la recherche par phrase
- [ ] D) Pour le debugging

### Question 4
Quel est le resultat de "A AND NOT B" si A={1,2,3} et B={2,4} ?

- [ ] A) {1,3,4}
- [x] B) {1,3}
- [ ] C) {2}
- [ ] D) {}

### Question 5
Pourquoi utiliser des stopwords ?

- [x] A) Reduire le bruit dans les resultats et la taille de l'index
- [ ] B) Accelerer l'indexation
- [ ] C) Ameliorer le recall
- [ ] D) Simplifier le parsing

---

## SECTION 8 : RECAPITULATIF

### Competences acquises

| Competence | Description | Niveau |
|------------|-------------|--------|
| Inverted Index | Construction et maintenance | Avance |
| TF-IDF Scoring | Calcul et ranking | Maitrise |
| Boolean Search | AND, OR, NOT, Phrase | Avance |
| Text Processing | Tokenization, normalisation | Intermediaire |

### Concepts cles

1. **Inverted Index** : Mapping terme -> documents (et positions)
2. **TF-IDF** : Scoring basee sur frequence locale et rarete globale
3. **Boolean Queries** : Combinaison logique de termes
4. **Phrase Search** : Termes consecutifs aux positions attendues

### Points de vigilance

- Normaliser systematiquement (lowercase, ponctuation)
- Gerer les cas limites (documents vides, termes absents)
- Positions necessaires pour phrase search
- Stopwords coherents entre indexation et recherche

---

## SECTION 9 : DEPLOYMENT PACK

### 9.1 Structure du projet

```
fulltext_search/
  Cargo.toml
  src/
    lib.rs
  tests/
    integration_tests.rs
  benches/
    benchmarks.rs
```

### 9.2 Cargo.toml

```toml
[package]
name = "fulltext_search"
version = "0.1.0"
edition = "2024"

[dependencies]

[dev-dependencies]
criterion = "0.5"

[[bench]]
name = "benchmarks"
harness = false
```

### 9.3 Criteres d'evaluation

| Critere | Poids | Description |
|---------|-------|-------------|
| Correction | 40% | Tests unitaires passent |
| Performance | 25% | Recherche < 10ms pour 100k docs |
| Memory | 20% | Index < 2x taille documents |
| Code quality | 15% | Idiomatique, documente |

### 9.4 Commandes utiles

```bash
# Tests
cargo test

# Benchmarks
cargo bench

# Documentation
cargo doc --open

# Memory profiling
cargo run --example memory_profile
```
