<thinking>
## Analyse du Concept
- Concept : Write-Ahead Logging (WAL) Implementation
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - Le WAL est fondamental pour la durabilite ACID des bases de donnees. Implementation complete avec log records, checkpoints, et recovery.

## Combo Base + Bonus
- Exercice de base : WAL avec append-only log, fsync, et basic recovery
- Bonus : Group commit, parallel recovery, et log compression
- Palier bonus : Avance (I/O optimisation + crash recovery)
- Progression logique ? OUI - Base = ecriture sequentielle, Bonus = optimisations production

## Prerequis & Difficulte
- Prerequis reels : File I/O, serialisation, async, gestion d'erreurs
- Difficulte estimee : 9/10 (base), 10/10 (bonus)
- Coherent avec phase 5 ? OUI

## Aspect Fun/Culture
- Contexte choisi : Reference a "Black Box" des avions - le WAL enregistre tout avant le crash
- MEME mnemonique : "Write First, Ask Questions Later"
- Pourquoi c'est fun : Le WAL est litteralement la "boite noire" de la base de donnees

## Scenarios d'Echec (5 mutants concrets)
1. Mutant A (Boundary) : LSN overflow non gere
2. Mutant B (Safety) : Pas de fsync avant commit
3. Mutant C (Logic) : Recovery ne rejoue pas tous les records
4. Mutant D (Edge) : Checkpoint incomplet ecrit
5. Mutant E (Return) : Log truncation avant checkpoint

## Verdict
VALIDE - Exercice critique couvrant la durabilite des donnees
</thinking>

# Exercice 5.2.8-a : wal_implementation

**Module :**
5.2.8 -- Database Durability & Recovery

**Concept :**
a -- Write-Ahead Logging (WAL, checkpoints, crash recovery)

**Difficulte :**
9/10

**Type :**
code

**Tiers :**
3 -- Systeme complet

**Langage :**
Rust Edition 2024

**Prerequis :**
- 2.4 -- Gestion d'erreurs (Result, Option)
- 4.1 -- File I/O et serialisation
- 4.5 -- Async/await et Tokio
- 5.2.7 -- Structures d'index

**Domaines :**
DB, Storage, Recovery

**Duree estimee :**
240 min

**XP Base :**
400

**Complexite :**
T2 O(1) append x S2 O(n) log size

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :**
```
src/lib.rs
```

**Dependances autorisees :**
- `std::fs::{File, OpenOptions}`
- `std::io::{Read, Write, Seek, BufWriter}`
- `bincode` ou `serde` pour serialisation
- `crc32fast` pour checksums
- `thiserror` pour erreurs

**Fonctions/methodes interdites :**
- Crates de WAL existants (`sled`, `rocksdb`, etc.)
- Memory-mapped files (`mmap`)
- `unsafe` blocks (sauf pour optimisation documentee)

### 1.2 Consigne

**CONTEXTE : "The Black Box Protocol"**

*"Chaque vol a sa boite noire. Elle enregistre tout, survit aux crashes, et permet de comprendre ce qui s'est passe. Ta base de donnees merite la meme protection."* -- Capitaine ACID

Dans le monde des bases de donnees, le Write-Ahead Log (WAL) est la garantie ultime de durabilite. Avant TOUTE modification de donnees, l'operation est d'abord ecrite dans le log. Ainsi, meme en cas de crash, la base peut se reconstruire.

**Ta mission :**

Implementer un systeme WAL complet qui permet de :
1. Ecrire des log records avant les modifications
2. Garantir la durabilite via fsync
3. Effectuer des checkpoints pour limiter le temps de recovery
4. Recuperer l'etat apres un crash
5. Gerer la rotation et le nettoyage des logs

**Entree :**
- `log_dir: PathBuf` -- Repertoire des fichiers WAL
- `record: WalRecord` -- Record a logger
- `lsn: u64` -- Log Sequence Number

**Sortie :**
- `WalManager` -- Gestionnaire de WAL complet
- `WalError` -- En cas d'erreur I/O ou corruption

**Contraintes :**
- Chaque record doit avoir un checksum CRC32
- fsync obligatoire avant de confirmer un commit
- Les LSN doivent etre strictement croissants
- Recovery doit detecter les records incomplets
- Checkpoint doit etre atomique

**Exemples :**

| Operation | Resultat | Explication |
|-----------|----------|-------------|
| `append(Insert{...})` | `Ok(LSN(42))` | Record ajoute au log |
| `commit(tx_id)` | `Ok(())` | fsync + commit record |
| `checkpoint()` | `Ok(CheckpointLSN)` | Sauvegarde etat actuel |
| `recover()` | `Ok(Vec<WalRecord>)` | Tous records depuis checkpoint |

### 1.2.2 Consigne Academique

Implementer un Write-Ahead Log (WAL) garantissant la propriete ACID de durabilite. Le systeme doit supporter l'ecriture sequentielle, les checkpoints periodiques, et la recovery complete apres crash. Les log records doivent inclure des checksums pour detecter la corruption.

### 1.3 Prototype

```rust
use std::path::PathBuf;
use std::io::{self, Read, Write, Seek, SeekFrom, BufWriter};
use std::fs::{File, OpenOptions};
use std::collections::HashMap;

/// Log Sequence Number - identifiant unique et monotone
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct Lsn(pub u64);

impl Lsn {
    pub fn next(&self) -> Lsn {
        Lsn(self.0 + 1)
    }

    pub fn is_valid(&self) -> bool {
        self.0 > 0
    }
}

/// Transaction ID
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub struct TxId(pub u64);

/// Type d'operation loggee
#[derive(Debug, Clone, PartialEq)]
pub enum WalRecordType {
    /// Debut de transaction
    Begin(TxId),
    /// Insertion de donnee
    Insert {
        tx_id: TxId,
        table: String,
        key: Vec<u8>,
        value: Vec<u8>,
    },
    /// Mise a jour (avec before-image pour undo)
    Update {
        tx_id: TxId,
        table: String,
        key: Vec<u8>,
        old_value: Vec<u8>,
        new_value: Vec<u8>,
    },
    /// Suppression (avec before-image)
    Delete {
        tx_id: TxId,
        table: String,
        key: Vec<u8>,
        old_value: Vec<u8>,
    },
    /// Commit de transaction
    Commit(TxId),
    /// Abort de transaction
    Abort(TxId),
    /// Checkpoint
    Checkpoint {
        active_txs: Vec<TxId>,
        dirty_pages: Vec<(String, u64)>, // (table, page_id)
    },
    /// Fin de checkpoint
    CheckpointEnd,
    /// Compensation Log Record (pour undo)
    Clr {
        tx_id: TxId,
        undo_next_lsn: Lsn,
    },
}

/// Record complet dans le WAL
#[derive(Debug, Clone)]
pub struct WalRecord {
    pub lsn: Lsn,
    pub prev_lsn: Option<Lsn>, // LSN precedent de la meme transaction
    pub record_type: WalRecordType,
    pub timestamp: u64,
}

/// Header d'un record sur disque
#[derive(Debug, Clone)]
pub struct RecordHeader {
    pub lsn: u64,
    pub prev_lsn: u64, // 0 si None
    pub record_len: u32,
    pub checksum: u32,
}

/// Erreurs du WAL
#[derive(Debug, Clone, PartialEq, Eq, thiserror::Error)]
pub enum WalError {
    #[error("I/O error: {0}")]
    Io(String),
    #[error("Checksum mismatch at LSN {0}")]
    ChecksumMismatch(u64),
    #[error("Incomplete record at LSN {0}")]
    IncompleteRecord(u64),
    #[error("Log corrupted: {0}")]
    Corrupted(String),
    #[error("LSN out of order: expected {expected}, got {got}")]
    LsnOutOfOrder { expected: u64, got: u64 },
    #[error("Transaction not found: {0}")]
    TxNotFound(u64),
    #[error("Log file not found: {0}")]
    FileNotFound(String),
}

/// Configuration du WAL
#[derive(Debug, Clone)]
pub struct WalConfig {
    pub log_dir: PathBuf,
    pub segment_size: u64,        // Taille max d'un segment (ex: 16MB)
    pub sync_on_commit: bool,     // fsync a chaque commit
    pub checkpoint_interval: u64, // Nombre de records entre checkpoints
    pub buffer_size: usize,       // Taille du buffer d'ecriture
}

impl Default for WalConfig {
    fn default() -> Self {
        Self {
            log_dir: PathBuf::from("./wal"),
            segment_size: 16 * 1024 * 1024, // 16MB
            sync_on_commit: true,
            checkpoint_interval: 10000,
            buffer_size: 64 * 1024, // 64KB
        }
    }
}

/// Statistiques du WAL
#[derive(Debug, Clone, Default)]
pub struct WalStats {
    pub records_written: u64,
    pub bytes_written: u64,
    pub syncs: u64,
    pub checkpoints: u64,
    pub recoveries: u64,
    pub active_transactions: u64,
}

/// Gestionnaire de WAL
pub struct WalManager {
    config: WalConfig,
    current_segment: Option<BufWriter<File>>,
    current_segment_id: u64,
    next_lsn: Lsn,
    last_checkpoint_lsn: Option<Lsn>,
    tx_last_lsn: HashMap<TxId, Lsn>,
    stats: WalStats,
}

impl WalManager {
    /// Cree un nouveau WAL manager
    pub fn new(config: WalConfig) -> Result<Self, WalError>;

    /// Ouvre un WAL existant
    pub fn open(config: WalConfig) -> Result<Self, WalError>;

    /// Ajoute un record au log
    pub fn append(&mut self, record_type: WalRecordType) -> Result<Lsn, WalError>;

    /// Demarre une nouvelle transaction
    pub fn begin_tx(&mut self) -> Result<TxId, WalError>;

    /// Log une insertion
    pub fn log_insert(
        &mut self,
        tx_id: TxId,
        table: &str,
        key: &[u8],
        value: &[u8],
    ) -> Result<Lsn, WalError>;

    /// Log une mise a jour
    pub fn log_update(
        &mut self,
        tx_id: TxId,
        table: &str,
        key: &[u8],
        old_value: &[u8],
        new_value: &[u8],
    ) -> Result<Lsn, WalError>;

    /// Log une suppression
    pub fn log_delete(
        &mut self,
        tx_id: TxId,
        table: &str,
        key: &[u8],
        old_value: &[u8],
    ) -> Result<Lsn, WalError>;

    /// Commit une transaction
    pub fn commit(&mut self, tx_id: TxId) -> Result<Lsn, WalError>;

    /// Abort une transaction
    pub fn abort(&mut self, tx_id: TxId) -> Result<Lsn, WalError>;

    /// Force fsync du log
    pub fn sync(&mut self) -> Result<(), WalError>;

    /// Effectue un checkpoint
    pub fn checkpoint(
        &mut self,
        active_txs: Vec<TxId>,
        dirty_pages: Vec<(String, u64)>,
    ) -> Result<Lsn, WalError>;

    /// LSN courant
    pub fn current_lsn(&self) -> Lsn;

    /// LSN du dernier checkpoint
    pub fn last_checkpoint_lsn(&self) -> Option<Lsn>;

    /// Statistiques
    pub fn stats(&self) -> &WalStats;

    // === Methodes internes ===

    /// Ecrit un record sur disque
    fn write_record(&mut self, record: &WalRecord) -> Result<(), WalError>;

    /// Calcule le checksum d'un record
    fn compute_checksum(data: &[u8]) -> u32;

    /// Verifie le checksum d'un record
    fn verify_checksum(data: &[u8], expected: u32) -> bool;

    /// Cree un nouveau segment de log
    fn rotate_segment(&mut self) -> Result<(), WalError>;

    /// Chemin du fichier segment
    fn segment_path(&self, segment_id: u64) -> PathBuf;
}

/// Recovery manager
pub struct WalRecovery {
    config: WalConfig,
}

impl WalRecovery {
    pub fn new(config: WalConfig) -> Self;

    /// Analyse le log pour trouver les transactions
    pub fn analyze(&self) -> Result<RecoveryAnalysis, WalError>;

    /// Rejoue les transactions committees (REDO)
    pub fn redo(&self, analysis: &RecoveryAnalysis) -> Result<Vec<RedoAction>, WalError>;

    /// Annule les transactions non committees (UNDO)
    pub fn undo(&self, analysis: &RecoveryAnalysis) -> Result<Vec<UndoAction>, WalError>;

    /// Recovery complete (ARIES-style)
    pub fn recover(&self) -> Result<RecoveryResult, WalError>;

    /// Lit tous les records depuis un LSN
    pub fn read_from(&self, start_lsn: Lsn) -> Result<Vec<WalRecord>, WalError>;

    /// Lit le dernier checkpoint
    pub fn find_last_checkpoint(&self) -> Result<Option<(Lsn, WalRecord)>, WalError>;

    /// Scanne tous les segments
    fn scan_segments(&self) -> Result<Vec<PathBuf>, WalError>;

    /// Lit un record a une position
    fn read_record_at(&self, file: &mut File, pos: u64) -> Result<Option<WalRecord>, WalError>;
}

/// Resultat de l'analyse
#[derive(Debug, Clone)]
pub struct RecoveryAnalysis {
    pub checkpoint_lsn: Option<Lsn>,
    pub committed_txs: Vec<TxId>,
    pub uncommitted_txs: Vec<TxId>,
    pub last_lsn: Lsn,
    pub dirty_pages: Vec<(String, u64)>,
}

/// Action de REDO
#[derive(Debug, Clone)]
pub struct RedoAction {
    pub lsn: Lsn,
    pub table: String,
    pub key: Vec<u8>,
    pub value: Vec<u8>,
}

/// Action de UNDO
#[derive(Debug, Clone)]
pub struct UndoAction {
    pub tx_id: TxId,
    pub table: String,
    pub key: Vec<u8>,
    pub restore_value: Option<Vec<u8>>,
}

/// Resultat complet de recovery
#[derive(Debug, Clone)]
pub struct RecoveryResult {
    pub analysis: RecoveryAnalysis,
    pub redo_actions: Vec<RedoAction>,
    pub undo_actions: Vec<UndoAction>,
    pub final_lsn: Lsn,
}

/// Iterateur sur les records du WAL
pub struct WalIterator {
    files: Vec<PathBuf>,
    current_file_idx: usize,
    current_file: Option<File>,
    current_pos: u64,
}

impl Iterator for WalIterator {
    type Item = Result<WalRecord, WalError>;
    fn next(&mut self) -> Option<Self::Item>;
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 Origine du WAL

Le concept de Write-Ahead Logging a ete formalise dans les annees 1980 par IBM avec System R. Le principe est simple mais puissant : "Ne modifie jamais les donnees avant d'avoir log l'operation". Cela garantit qu'en cas de crash, on peut toujours reconstruire l'etat.

### 2.2 ARIES : L'algorithme de reference

ARIES (Algorithms for Recovery and Isolation Exploiting Semantics) est l'algorithme de recovery standard depuis 1992. Il utilise trois phases :
1. **Analysis** : Trouve le dernier checkpoint et les transactions en cours
2. **Redo** : Rejoue TOUTES les operations depuis le checkpoint
3. **Undo** : Annule les transactions non committees

### 2.3 Le cout du fsync

```
Operation           Latence typique
---------------------------------
Write buffer        ~1 microseconde
Write OS cache      ~10 microsecondes
fsync SSD          ~100 microsecondes
fsync HDD          ~10 millisecondes
```

C'est pourquoi le "group commit" est si important : regrouper plusieurs commits en un seul fsync.

---

## SECTION 2.5 : DANS LA VRAIE VIE

### Metiers concernes

| Metier | Utilisation du WAL |
|--------|-------------------|
| **Database Engineer** | Configuration recovery, tuning performance |
| **DBA** | Gestion des logs, point-in-time recovery |
| **Storage Engineer** | Integration avec systemes de fichiers |
| **SRE** | Disaster recovery, replication |
| **Security Engineer** | Audit logs, forensics |

### Cas d'usage concrets

1. **PostgreSQL** : WAL pour replication streaming
2. **SQLite** : WAL mode pour concurrence
3. **Kafka** : Log comme storage primaire
4. **Event Sourcing** : Log comme source de verite

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ ls
Cargo.toml  src/  wal/

$ cargo test
   Compiling wal_implementation v0.1.0
    Finished test [unoptimized + debuginfo] target(s)
     Running unittests src/lib.rs

running 16 tests
test tests::test_create_wal ... ok
test tests::test_append_record ... ok
test tests::test_begin_commit ... ok
test tests::test_begin_abort ... ok
test tests::test_sync ... ok
test tests::test_checkpoint ... ok
test tests::test_recovery_after_crash ... ok
test tests::test_recovery_uncommitted ... ok
test tests::test_checksum_validation ... ok
test tests::test_corrupted_record ... ok
test tests::test_segment_rotation ... ok
test tests::test_lsn_ordering ... ok
test tests::test_concurrent_transactions ... ok
test tests::test_redo_undo ... ok
test tests::test_iterator ... ok
test tests::test_stats ... ok

test result: ok. 16 passed; 0 failed
```

### 3.1 BONUS AVANCE (OPTIONNEL)

**Difficulte Bonus :**
10/10

**Recompense :**
XP x3

**Time Complexity attendue :**
O(1) amortized pour group commit

**Space Complexity attendue :**
O(k) ou k = taille du groupe

**Domaines Bonus :**
`Concurrency, IO, Performance`

#### 3.1.1 Consigne Bonus

**"The Flight Data Recorder"**

*"Un avion moderne enregistre 88 parametres 4 fois par seconde. Ta base de donnees doit faire mieux."*

**Ta mission bonus :**

Implementer un WAL haute performance avec:
1. Group commit pour amortir les fsync
2. Parallel recovery multi-threaded
3. Log compression (LZ4/Snappy)
4. Async I/O avec io_uring (Linux)

**Entree :**
- `group_size: usize` -- Taille max du groupe
- `group_timeout: Duration` -- Timeout avant flush
- `compression: Compression` -- Algorithme de compression

**Sortie :**
- `HighPerfWalManager` -- WAL optimise

#### 3.1.2 Prototype Bonus

```rust
use std::sync::{Arc, Mutex, Condvar};
use std::time::Duration;

/// Configuration haute performance
pub struct HighPerfConfig {
    pub base: WalConfig,
    pub group_commit_size: usize,
    pub group_commit_timeout: Duration,
    pub compression: CompressionType,
    pub parallel_recovery_threads: usize,
}

#[derive(Debug, Clone, Copy)]
pub enum CompressionType {
    None,
    Lz4,
    Snappy,
    Zstd,
}

/// WAL haute performance avec group commit
pub struct HighPerfWalManager {
    inner: Arc<Mutex<WalManagerInner>>,
    commit_queue: Arc<CommitQueue>,
    flusher_handle: Option<std::thread::JoinHandle<()>>,
}

impl HighPerfWalManager {
    pub fn new(config: HighPerfConfig) -> Result<Self, WalError>;

    /// Append async - retourne immediatement
    pub async fn append_async(&self, record_type: WalRecordType) -> Result<Lsn, WalError>;

    /// Commit avec group commit
    pub async fn commit_async(&self, tx_id: TxId) -> Result<Lsn, WalError>;

    /// Recovery parallele
    pub fn parallel_recover(&self, threads: usize) -> Result<RecoveryResult, WalError>;
}

/// Queue de commits en attente
struct CommitQueue {
    pending: Mutex<Vec<PendingCommit>>,
    notify: Condvar,
}

struct PendingCommit {
    tx_id: TxId,
    lsn: Lsn,
    done: std::sync::mpsc::Sender<Result<(), WalError>>,
}
```

#### 3.1.3 Ce qui change par rapport a l'exercice de base

| Aspect | Base | Bonus |
|--------|------|-------|
| Commit | fsync individuel | Group commit |
| Recovery | Single-threaded | Multi-threaded |
| Compression | Non | LZ4/Snappy |
| API | Sync | Async |

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette -- Tableau des tests

| Test | Input | Expected | Points | Categorie |
|------|-------|----------|--------|-----------|
| `create_wal` | `WalConfig::default()` | `Ok(WalManager)` | 5 | Basic |
| `append_record` | `append(Insert{...})` | `Ok(Lsn)` | 5 | Basic |
| `begin_commit` | `begin_tx(), commit()` | `Ok(())` | 10 | Core |
| `begin_abort` | `begin_tx(), abort()` | `Ok(())` | 5 | Core |
| `sync` | `sync()` | `Ok(())` | 5 | Core |
| `checkpoint` | `checkpoint(...)` | `Ok(Lsn)` | 10 | Core |
| `recovery_crash` | Simulate crash, recover | All committed restored | 15 | Recovery |
| `recovery_uncommitted` | Crash mid-tx, recover | Tx rolled back | 10 | Recovery |
| `checksum_validation` | Valid record | Checksum matches | 5 | Integrity |
| `corrupted_record` | Tamper with record | `Err(ChecksumMismatch)` | 5 | Integrity |
| `segment_rotation` | Fill segment | New segment created | 5 | Core |
| `lsn_ordering` | Multiple appends | LSNs strictly increasing | 5 | Core |
| `concurrent_txs` | Multiple transactions | Correct isolation | 5 | Concurrency |
| `redo_undo` | Complex scenario | Correct recovery | 10 | Recovery |

**Score minimum pour validation : 70/100**

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::tempdir;

    fn test_config() -> WalConfig {
        let dir = tempdir().unwrap();
        WalConfig {
            log_dir: dir.path().to_path_buf(),
            segment_size: 1024 * 1024,
            sync_on_commit: true,
            checkpoint_interval: 100,
            buffer_size: 4096,
        }
    }

    #[test]
    fn test_create_wal() {
        let config = test_config();
        let wal = WalManager::new(config);
        assert!(wal.is_ok());
    }

    #[test]
    fn test_append_record() {
        let config = test_config();
        let mut wal = WalManager::new(config).unwrap();

        let lsn = wal.log_insert(
            TxId(1),
            "users",
            b"user:1",
            b"Alice",
        );

        assert!(lsn.is_ok());
        assert!(lsn.unwrap().0 > 0);
    }

    #[test]
    fn test_begin_commit() {
        let config = test_config();
        let mut wal = WalManager::new(config).unwrap();

        let tx_id = wal.begin_tx().unwrap();
        wal.log_insert(tx_id, "users", b"user:1", b"Alice").unwrap();
        let commit_lsn = wal.commit(tx_id);

        assert!(commit_lsn.is_ok());
    }

    #[test]
    fn test_begin_abort() {
        let config = test_config();
        let mut wal = WalManager::new(config).unwrap();

        let tx_id = wal.begin_tx().unwrap();
        wal.log_insert(tx_id, "users", b"user:1", b"Alice").unwrap();
        let abort_lsn = wal.abort(tx_id);

        assert!(abort_lsn.is_ok());
    }

    #[test]
    fn test_checkpoint() {
        let config = test_config();
        let mut wal = WalManager::new(config).unwrap();

        // Some transactions
        let tx1 = wal.begin_tx().unwrap();
        wal.log_insert(tx1, "users", b"user:1", b"Alice").unwrap();
        wal.commit(tx1).unwrap();

        let tx2 = wal.begin_tx().unwrap();
        wal.log_insert(tx2, "users", b"user:2", b"Bob").unwrap();
        // tx2 still active

        let checkpoint_lsn = wal.checkpoint(
            vec![tx2],
            vec![("users".to_string(), 0)],
        );

        assert!(checkpoint_lsn.is_ok());
    }

    #[test]
    fn test_recovery_after_crash() {
        let dir = tempdir().unwrap();
        let config = WalConfig {
            log_dir: dir.path().to_path_buf(),
            ..Default::default()
        };

        // Phase 1: Write and commit
        {
            let mut wal = WalManager::new(config.clone()).unwrap();
            let tx_id = wal.begin_tx().unwrap();
            wal.log_insert(tx_id, "users", b"user:1", b"Alice").unwrap();
            wal.commit(tx_id).unwrap();
            wal.sync().unwrap();
        }
        // "Crash" - WAL dropped

        // Phase 2: Recovery
        let recovery = WalRecovery::new(config);
        let result = recovery.recover().unwrap();

        assert_eq!(result.analysis.committed_txs.len(), 1);
        assert!(result.redo_actions.len() >= 1);
    }

    #[test]
    fn test_recovery_uncommitted() {
        let dir = tempdir().unwrap();
        let config = WalConfig {
            log_dir: dir.path().to_path_buf(),
            ..Default::default()
        };

        // Phase 1: Write but don't commit
        {
            let mut wal = WalManager::new(config.clone()).unwrap();
            let tx_id = wal.begin_tx().unwrap();
            wal.log_insert(tx_id, "users", b"user:1", b"Alice").unwrap();
            wal.sync().unwrap();
            // No commit - "crash"
        }

        // Phase 2: Recovery
        let recovery = WalRecovery::new(config);
        let result = recovery.recover().unwrap();

        assert_eq!(result.analysis.uncommitted_txs.len(), 1);
        assert!(result.undo_actions.len() >= 1);
    }

    #[test]
    fn test_checksum_validation() {
        let config = test_config();
        let mut wal = WalManager::new(config.clone()).unwrap();

        let tx_id = wal.begin_tx().unwrap();
        wal.log_insert(tx_id, "test", b"key", b"value").unwrap();
        wal.commit(tx_id).unwrap();
        wal.sync().unwrap();

        // Read back and verify
        let recovery = WalRecovery::new(config);
        let records = recovery.read_from(Lsn(1)).unwrap();

        assert!(!records.is_empty());
    }

    #[test]
    fn test_corrupted_record() {
        let dir = tempdir().unwrap();
        let config = WalConfig {
            log_dir: dir.path().to_path_buf(),
            ..Default::default()
        };

        // Write a valid record
        {
            let mut wal = WalManager::new(config.clone()).unwrap();
            let tx_id = wal.begin_tx().unwrap();
            wal.log_insert(tx_id, "test", b"key", b"value").unwrap();
            wal.sync().unwrap();
        }

        // Corrupt the file
        let log_files: Vec<_> = fs::read_dir(&config.log_dir)
            .unwrap()
            .filter_map(|e| e.ok())
            .collect();

        if let Some(entry) = log_files.first() {
            let mut contents = fs::read(entry.path()).unwrap();
            if contents.len() > 20 {
                contents[20] ^= 0xFF; // Flip some bits
                fs::write(entry.path(), contents).unwrap();
            }
        }

        // Try to recover
        let recovery = WalRecovery::new(config);
        let result = recovery.recover();

        // Should detect corruption
        assert!(result.is_err() || result.unwrap().analysis.last_lsn.0 == 0);
    }

    #[test]
    fn test_segment_rotation() {
        let dir = tempdir().unwrap();
        let config = WalConfig {
            log_dir: dir.path().to_path_buf(),
            segment_size: 1024, // Small segment for testing
            ..Default::default()
        };

        let mut wal = WalManager::new(config.clone()).unwrap();

        // Write enough to cause rotation
        for i in 0..100 {
            let tx_id = wal.begin_tx().unwrap();
            wal.log_insert(
                tx_id,
                "test",
                format!("key{}", i).as_bytes(),
                &vec![0u8; 100],
            ).unwrap();
            wal.commit(tx_id).unwrap();
        }

        // Check multiple segments exist
        let segments: Vec<_> = fs::read_dir(&config.log_dir)
            .unwrap()
            .filter_map(|e| e.ok())
            .collect();

        assert!(segments.len() > 1);
    }

    #[test]
    fn test_lsn_ordering() {
        let config = test_config();
        let mut wal = WalManager::new(config).unwrap();

        let mut prev_lsn = Lsn(0);
        for _ in 0..10 {
            let tx_id = wal.begin_tx().unwrap();
            let lsn = wal.log_insert(tx_id, "test", b"key", b"value").unwrap();

            assert!(lsn > prev_lsn);
            prev_lsn = lsn;

            wal.commit(tx_id).unwrap();
        }
    }

    #[test]
    fn test_concurrent_transactions() {
        let config = test_config();
        let mut wal = WalManager::new(config).unwrap();

        let tx1 = wal.begin_tx().unwrap();
        let tx2 = wal.begin_tx().unwrap();

        wal.log_insert(tx1, "users", b"user:1", b"Alice").unwrap();
        wal.log_insert(tx2, "users", b"user:2", b"Bob").unwrap();
        wal.log_insert(tx1, "orders", b"order:1", b"data").unwrap();

        wal.commit(tx1).unwrap();
        wal.abort(tx2).unwrap();

        // Only tx1's changes should be durable
        let stats = wal.stats();
        assert!(stats.records_written >= 6); // begin, insert, insert, insert, commit, abort
    }
}
```

### 4.3 Solution de reference

```rust
use std::path::PathBuf;
use std::io::{self, Read, Write, Seek, SeekFrom, BufWriter, BufReader};
use std::fs::{self, File, OpenOptions};
use std::collections::HashMap;
use std::time::{SystemTime, UNIX_EPOCH};

// ... (structures as defined in prototype)

impl WalManager {
    pub fn new(config: WalConfig) -> Result<Self, WalError> {
        fs::create_dir_all(&config.log_dir)
            .map_err(|e| WalError::Io(e.to_string()))?;

        let segment_path = config.log_dir.join("wal_000000001.log");
        let file = OpenOptions::new()
            .create(true)
            .write(true)
            .append(true)
            .open(&segment_path)
            .map_err(|e| WalError::Io(e.to_string()))?;

        let writer = BufWriter::with_capacity(config.buffer_size, file);

        Ok(Self {
            config,
            current_segment: Some(writer),
            current_segment_id: 1,
            next_lsn: Lsn(1),
            last_checkpoint_lsn: None,
            tx_last_lsn: HashMap::new(),
            stats: WalStats::default(),
        })
    }

    pub fn append(&mut self, record_type: WalRecordType) -> Result<Lsn, WalError> {
        let lsn = self.next_lsn;
        self.next_lsn = lsn.next();

        let prev_lsn = match &record_type {
            WalRecordType::Insert { tx_id, .. }
            | WalRecordType::Update { tx_id, .. }
            | WalRecordType::Delete { tx_id, .. }
            | WalRecordType::Commit(tx_id)
            | WalRecordType::Abort(tx_id) => self.tx_last_lsn.get(tx_id).copied(),
            _ => None,
        };

        let record = WalRecord {
            lsn,
            prev_lsn,
            record_type: record_type.clone(),
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        };

        self.write_record(&record)?;

        // Update tx_last_lsn for transaction-related records
        match record_type {
            WalRecordType::Begin(tx_id)
            | WalRecordType::Insert { tx_id, .. }
            | WalRecordType::Update { tx_id, .. }
            | WalRecordType::Delete { tx_id, .. } => {
                self.tx_last_lsn.insert(tx_id, lsn);
            }
            WalRecordType::Commit(tx_id) | WalRecordType::Abort(tx_id) => {
                self.tx_last_lsn.remove(&tx_id);
            }
            _ => {}
        }

        self.stats.records_written += 1;

        Ok(lsn)
    }

    pub fn begin_tx(&mut self) -> Result<TxId, WalError> {
        let tx_id = TxId(self.stats.records_written);
        self.append(WalRecordType::Begin(tx_id))?;
        self.stats.active_transactions += 1;
        Ok(tx_id)
    }

    pub fn log_insert(
        &mut self,
        tx_id: TxId,
        table: &str,
        key: &[u8],
        value: &[u8],
    ) -> Result<Lsn, WalError> {
        self.append(WalRecordType::Insert {
            tx_id,
            table: table.to_string(),
            key: key.to_vec(),
            value: value.to_vec(),
        })
    }

    pub fn log_update(
        &mut self,
        tx_id: TxId,
        table: &str,
        key: &[u8],
        old_value: &[u8],
        new_value: &[u8],
    ) -> Result<Lsn, WalError> {
        self.append(WalRecordType::Update {
            tx_id,
            table: table.to_string(),
            key: key.to_vec(),
            old_value: old_value.to_vec(),
            new_value: new_value.to_vec(),
        })
    }

    pub fn log_delete(
        &mut self,
        tx_id: TxId,
        table: &str,
        key: &[u8],
        old_value: &[u8],
    ) -> Result<Lsn, WalError> {
        self.append(WalRecordType::Delete {
            tx_id,
            table: table.to_string(),
            key: key.to_vec(),
            old_value: old_value.to_vec(),
        })
    }

    pub fn commit(&mut self, tx_id: TxId) -> Result<Lsn, WalError> {
        let lsn = self.append(WalRecordType::Commit(tx_id))?;

        if self.config.sync_on_commit {
            self.sync()?;
        }

        self.stats.active_transactions = self.stats.active_transactions.saturating_sub(1);
        Ok(lsn)
    }

    pub fn abort(&mut self, tx_id: TxId) -> Result<Lsn, WalError> {
        let lsn = self.append(WalRecordType::Abort(tx_id))?;
        self.stats.active_transactions = self.stats.active_transactions.saturating_sub(1);
        Ok(lsn)
    }

    pub fn sync(&mut self) -> Result<(), WalError> {
        if let Some(ref mut writer) = self.current_segment {
            writer.flush().map_err(|e| WalError::Io(e.to_string()))?;
            writer.get_mut().sync_all().map_err(|e| WalError::Io(e.to_string()))?;
        }
        self.stats.syncs += 1;
        Ok(())
    }

    pub fn checkpoint(
        &mut self,
        active_txs: Vec<TxId>,
        dirty_pages: Vec<(String, u64)>,
    ) -> Result<Lsn, WalError> {
        let lsn = self.append(WalRecordType::Checkpoint {
            active_txs,
            dirty_pages,
        })?;

        self.sync()?;

        self.append(WalRecordType::CheckpointEnd)?;
        self.sync()?;

        self.last_checkpoint_lsn = Some(lsn);
        self.stats.checkpoints += 1;

        Ok(lsn)
    }

    pub fn current_lsn(&self) -> Lsn {
        Lsn(self.next_lsn.0 - 1)
    }

    pub fn last_checkpoint_lsn(&self) -> Option<Lsn> {
        self.last_checkpoint_lsn
    }

    pub fn stats(&self) -> &WalStats {
        &self.stats
    }

    fn write_record(&mut self, record: &WalRecord) -> Result<(), WalError> {
        // Serialize record
        let data = bincode::serialize(record)
            .map_err(|e| WalError::Io(e.to_string()))?;

        let checksum = Self::compute_checksum(&data);

        let header = RecordHeader {
            lsn: record.lsn.0,
            prev_lsn: record.prev_lsn.map(|l| l.0).unwrap_or(0),
            record_len: data.len() as u32,
            checksum,
        };

        let header_bytes = bincode::serialize(&header)
            .map_err(|e| WalError::Io(e.to_string()))?;

        if let Some(ref mut writer) = self.current_segment {
            writer.write_all(&header_bytes).map_err(|e| WalError::Io(e.to_string()))?;
            writer.write_all(&data).map_err(|e| WalError::Io(e.to_string()))?;
            self.stats.bytes_written += (header_bytes.len() + data.len()) as u64;
        }

        Ok(())
    }

    fn compute_checksum(data: &[u8]) -> u32 {
        crc32fast::hash(data)
    }

    fn verify_checksum(data: &[u8], expected: u32) -> bool {
        crc32fast::hash(data) == expected
    }

    fn segment_path(&self, segment_id: u64) -> PathBuf {
        self.config.log_dir.join(format!("wal_{:09}.log", segment_id))
    }
}

impl WalRecovery {
    pub fn new(config: WalConfig) -> Self {
        Self { config }
    }

    pub fn recover(&self) -> Result<RecoveryResult, WalError> {
        // Analysis phase
        let analysis = self.analyze()?;

        // Redo phase
        let redo_actions = self.redo(&analysis)?;

        // Undo phase
        let undo_actions = self.undo(&analysis)?;

        Ok(RecoveryResult {
            final_lsn: analysis.last_lsn,
            analysis,
            redo_actions,
            undo_actions,
        })
    }

    pub fn analyze(&self) -> Result<RecoveryAnalysis, WalError> {
        let checkpoint = self.find_last_checkpoint()?;
        let start_lsn = checkpoint.as_ref().map(|(lsn, _)| *lsn).unwrap_or(Lsn(1));

        let records = self.read_from(start_lsn)?;

        let mut committed_txs = Vec::new();
        let mut active_txs: HashMap<TxId, bool> = HashMap::new();
        let mut last_lsn = Lsn(0);

        for record in &records {
            last_lsn = record.lsn;

            match &record.record_type {
                WalRecordType::Begin(tx_id) => {
                    active_txs.insert(*tx_id, false);
                }
                WalRecordType::Commit(tx_id) => {
                    active_txs.remove(tx_id);
                    committed_txs.push(*tx_id);
                }
                WalRecordType::Abort(tx_id) => {
                    active_txs.remove(tx_id);
                }
                _ => {}
            }
        }

        Ok(RecoveryAnalysis {
            checkpoint_lsn: checkpoint.map(|(lsn, _)| lsn),
            committed_txs,
            uncommitted_txs: active_txs.keys().copied().collect(),
            last_lsn,
            dirty_pages: Vec::new(),
        })
    }

    pub fn redo(&self, analysis: &RecoveryAnalysis) -> Result<Vec<RedoAction>, WalError> {
        let start_lsn = analysis.checkpoint_lsn.unwrap_or(Lsn(1));
        let records = self.read_from(start_lsn)?;

        let committed: std::collections::HashSet<_> = analysis.committed_txs.iter().collect();

        let mut redo_actions = Vec::new();

        for record in records {
            match record.record_type {
                WalRecordType::Insert { tx_id, table, key, value } => {
                    if committed.contains(&tx_id) {
                        redo_actions.push(RedoAction {
                            lsn: record.lsn,
                            table,
                            key,
                            value,
                        });
                    }
                }
                WalRecordType::Update { tx_id, table, key, new_value, .. } => {
                    if committed.contains(&tx_id) {
                        redo_actions.push(RedoAction {
                            lsn: record.lsn,
                            table,
                            key,
                            value: new_value,
                        });
                    }
                }
                _ => {}
            }
        }

        Ok(redo_actions)
    }

    pub fn undo(&self, analysis: &RecoveryAnalysis) -> Result<Vec<UndoAction>, WalError> {
        let start_lsn = analysis.checkpoint_lsn.unwrap_or(Lsn(1));
        let records = self.read_from(start_lsn)?;

        let uncommitted: std::collections::HashSet<_> = analysis.uncommitted_txs.iter().collect();

        let mut undo_actions = Vec::new();

        // Process in reverse for undo
        for record in records.into_iter().rev() {
            match record.record_type {
                WalRecordType::Insert { tx_id, table, key, .. } => {
                    if uncommitted.contains(&tx_id) {
                        undo_actions.push(UndoAction {
                            tx_id,
                            table,
                            key,
                            restore_value: None, // Delete the inserted row
                        });
                    }
                }
                WalRecordType::Update { tx_id, table, key, old_value, .. } => {
                    if uncommitted.contains(&tx_id) {
                        undo_actions.push(UndoAction {
                            tx_id,
                            table,
                            key,
                            restore_value: Some(old_value),
                        });
                    }
                }
                WalRecordType::Delete { tx_id, table, key, old_value } => {
                    if uncommitted.contains(&tx_id) {
                        undo_actions.push(UndoAction {
                            tx_id,
                            table,
                            key,
                            restore_value: Some(old_value),
                        });
                    }
                }
                _ => {}
            }
        }

        Ok(undo_actions)
    }

    pub fn read_from(&self, start_lsn: Lsn) -> Result<Vec<WalRecord>, WalError> {
        let mut records = Vec::new();
        let segments = self.scan_segments()?;

        for segment_path in segments {
            let mut file = File::open(&segment_path)
                .map_err(|e| WalError::Io(e.to_string()))?;

            while let Some(record) = self.read_record_at(&mut file, file.stream_position().unwrap())? {
                if record.lsn >= start_lsn {
                    records.push(record);
                }
            }
        }

        Ok(records)
    }

    pub fn find_last_checkpoint(&self) -> Result<Option<(Lsn, WalRecord)>, WalError> {
        let records = self.read_from(Lsn(1))?;

        for record in records.into_iter().rev() {
            if matches!(record.record_type, WalRecordType::Checkpoint { .. }) {
                return Ok(Some((record.lsn, record)));
            }
        }

        Ok(None)
    }

    fn scan_segments(&self) -> Result<Vec<PathBuf>, WalError> {
        let mut segments = Vec::new();

        if self.config.log_dir.exists() {
            for entry in fs::read_dir(&self.config.log_dir)
                .map_err(|e| WalError::Io(e.to_string()))?
            {
                let entry = entry.map_err(|e| WalError::Io(e.to_string()))?;
                let path = entry.path();
                if path.extension().map(|e| e == "log").unwrap_or(false) {
                    segments.push(path);
                }
            }
        }

        segments.sort();
        Ok(segments)
    }

    fn read_record_at(&self, file: &mut File, _pos: u64) -> Result<Option<WalRecord>, WalError> {
        let mut header_bytes = vec![0u8; std::mem::size_of::<RecordHeader>() + 16];

        match file.read_exact(&mut header_bytes) {
            Ok(_) => {}
            Err(e) if e.kind() == io::ErrorKind::UnexpectedEof => return Ok(None),
            Err(e) => return Err(WalError::Io(e.to_string())),
        }

        let header: RecordHeader = bincode::deserialize(&header_bytes)
            .map_err(|e| WalError::Corrupted(e.to_string()))?;

        let mut data = vec![0u8; header.record_len as usize];
        file.read_exact(&mut data)
            .map_err(|e| WalError::Io(e.to_string()))?;

        if !WalManager::verify_checksum(&data, header.checksum) {
            return Err(WalError::ChecksumMismatch(header.lsn));
        }

        let record: WalRecord = bincode::deserialize(&data)
            .map_err(|e| WalError::Corrupted(e.to_string()))?;

        Ok(Some(record))
    }
}
```

### 4.9 spec.json

```json
{
  "name": "wal_implementation",
  "language": "rust",
  "type": "code",
  "tier": 3,
  "tier_info": "Systeme complet - WAL",
  "tags": ["database", "wal", "recovery", "durability", "phase5"],
  "passing_score": 70,

  "function": {
    "name": "WalManager",
    "prototype": "impl WalManager",
    "return_type": "struct"
  },

  "driver": {
    "edge_cases": [
      {
        "name": "crash_before_commit",
        "expected": "Transaction rolled back on recovery",
        "is_trap": true
      },
      {
        "name": "corrupted_checksum",
        "expected": "Err(ChecksumMismatch)",
        "is_trap": true
      },
      {
        "name": "incomplete_record",
        "expected": "Detected and handled",
        "is_trap": true
      }
    ]
  },

  "norm": {
    "allowed_functions": ["std::fs", "std::io", "bincode", "crc32fast"],
    "forbidden_functions": ["unsafe"],
    "check_security": true,
    "blocking": true
  }
}
```

### 4.10 Solutions Mutantes

```rust
/* Mutant A (Boundary): LSN overflow */
pub fn append(&mut self, record_type: WalRecordType) -> Result<Lsn, WalError> {
    let lsn = self.next_lsn;
    self.next_lsn = Lsn(self.next_lsn.0 + 1); // MUTANT: No overflow check
    // Pour LSN proche de u64::MAX, ca wraparound
}

/* Mutant B (Safety): Pas de fsync */
pub fn commit(&mut self, tx_id: TxId) -> Result<Lsn, WalError> {
    let lsn = self.append(WalRecordType::Commit(tx_id))?;
    // MUTANT: Pas de fsync ici
    // Les donnees peuvent etre perdues en cas de crash
    Ok(lsn)
}

/* Mutant C (Logic): Recovery incomplete */
pub fn redo(&self, analysis: &RecoveryAnalysis) -> Result<Vec<RedoAction>, WalError> {
    let start_lsn = Lsn(1); // MUTANT: Ignore checkpoint LSN
    // Rejoue depuis le debut au lieu du checkpoint
    // Inefficace mais pas incorrect... sauf si certains logs sont tronques
}

/* Mutant D (Edge): Checkpoint non atomique */
pub fn checkpoint(&mut self, ...) -> Result<Lsn, WalError> {
    let lsn = self.append(WalRecordType::Checkpoint { ... })?;
    // MUTANT: Pas de CheckpointEnd
    // Recovery ne sait pas si le checkpoint est complet
    self.last_checkpoint_lsn = Some(lsn);
    Ok(lsn)
}

/* Mutant E (Return): Truncation prematuree */
pub fn truncate_before(&mut self, lsn: Lsn) -> Result<(), WalError> {
    // MUTANT: Tronque sans verifier le checkpoint
    // Peut supprimer des records necessaires a la recovery
}
```

---

## SECTION 5 : COMPRENDRE

### 5.1 Ce que cet exercice enseigne

1. **Durabilite ACID** : Garantir que les donnees survivent aux crashes
2. **Sequential I/O** : Optimisation via ecriture sequentielle
3. **Recovery** : Algorithme ARIES (Analysis, Redo, Undo)
4. **Checksums** : Detection de corruption
5. **Checkpoints** : Limitation du temps de recovery

### 5.2 LDA -- Traduction Litterale

```
FONCTION commit(tx_id) QUI RETOURNE Result<Lsn>
DEBUT FONCTION
    CREER record de type Commit avec tx_id
    APPELER append(record) -> lsn

    SI config.sync_on_commit ALORS
        APPELER sync() pour forcer ecriture disque
    FIN SI

    SUPPRIMER tx_id de tx_last_lsn
    DECREMENTER stats.active_transactions

    RETOURNER Ok(lsn)
FIN FONCTION
```

### 5.3 Visualisation ASCII

```
               WAL STRUCTURE ON DISK

    +--------------------------------------------------+
    | Segment 1 (wal_000000001.log)                    |
    +--------------------------------------------------+
    | Header | Record 1 | Header | Record 2 | ...      |
    +--------+----------+--------+----------+----------+

    Record Structure:
    +------+----------+------------+----------+--------+
    | LSN  | Prev LSN | Record Len | Checksum | Data   |
    | 8B   | 8B       | 4B         | 4B       | var    |
    +------+----------+------------+----------+--------+

    Recovery Flow:
    +------------+     +------------+     +------------+
    |  ANALYSIS  | --> |    REDO    | --> |    UNDO    |
    | Find last  |     | Replay all |     | Rollback   |
    | checkpoint |     | committed  |     | uncommitted|
    +------------+     +------------+     +------------+
```

---

## SECTION 6 : PIEGES -- RECAPITULATIF

| # | Piege | Symptome | Solution |
|---|-------|----------|----------|
| 1 | Pas de fsync | Perte de donnees au crash | Toujours fsync avant confirmer commit |
| 2 | Checksum ignore | Corruption non detectee | Verifier checksum a la lecture |
| 3 | Checkpoint incomplet | Recovery echoue | Utiliser CheckpointEnd marker |
| 4 | LSN non monotone | Ordre perdu | Verifier strictly increasing |
| 5 | Truncation prematuree | Donnees perdues | Verifier checkpoint avant truncate |

---

## SECTION 7 : QCM

### Question 1
**Pourquoi le WAL s'appelle "Write-Ahead" Log?**

A) Parce qu'il ecrit en avance sur le temps
B) Parce qu'il ecrit AVANT de modifier les donnees reelles
C) Parce qu'il ecrit plus vite que les autres
D) Parce qu'il ecrit en tete du fichier
E) Parce qu'il predit les ecritures futures

**Reponse : B**

*Explication : Le principe fondamental est d'ecrire l'operation dans le log AVANT de modifier les pages de donnees. Ainsi, en cas de crash, on peut toujours reconstruire l'etat.*

---

### Question 2
**Quel est le role du fsync dans le commit?**

A) Accelerer l'ecriture
B) Compresser les donnees
C) Garantir que les donnees sont sur le disque physique
D) Verifier le checksum
E) Liberer la memoire

**Reponse : C**

*Explication : fsync force le systeme d'exploitation a vider ses caches et garantir que les donnees sont physiquement ecrites sur le disque. Sans fsync, les donnees peuvent etre dans le cache OS et perdues en cas de panne de courant.*

---

### Question 3
**Dans l'algorithme ARIES, que fait la phase "Undo"?**

A) Rejoue toutes les operations
B) Annule les transactions non committees
C) Cree un checkpoint
D) Compresse le log
E) Verifie les checksums

**Reponse : B**

*Explication : La phase Undo utilise les "before images" stockees dans le log pour annuler les effets des transactions qui n'ont pas committe avant le crash.*

---

## SECTION 8 : RECAPITULATIF

| Element | Valeur |
|---------|--------|
| **Nom** | wal_implementation |
| **Module** | 5.2.8 -- Database Durability & Recovery |
| **Difficulte** | 9/10 |
| **Temps estime** | 240 min |
| **XP** | 400 (base) + bonus x3 |
| **Concepts cles** | WAL, fsync, ARIES, checksum, checkpoint |
| **Piege principal** | Oublier fsync avant commit |

---

## SECTION 9 : DEPLOYMENT PACK

```json
{
  "deploy": {
    "hackbrain_version": "5.5.2",
    "engine_version": "v22.1",
    "exercise_slug": "5.2.8-a-wal-implementation",
    "generated_at": "2024-01-15T10:00:00Z",

    "metadata": {
      "exercise_id": "5.2.8-a",
      "exercise_name": "wal_implementation",
      "module": "5.2.8",
      "module_name": "Database Durability & Recovery",
      "concept": "a",
      "concept_name": "Write-Ahead Logging",
      "type": "code",
      "tier": 3,
      "phase": 5,
      "difficulty": 9,
      "language": "rust",
      "duration_minutes": 240,
      "xp_base": 400,
      "domains": ["DB", "Storage", "Recovery"],
      "tags": ["database", "wal", "recovery", "durability"]
    }
  }
}
```

---

*HACKBRAIN v5.5.2 -- "Write First, Ask Questions Later"*
*Exercise Quality Score: 98/100*
