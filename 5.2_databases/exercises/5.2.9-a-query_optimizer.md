<thinking>
## Analyse du Concept
- Concept : Query Optimizer Implementation
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - L'optimiseur de requetes est le cerveau du SGBD. Implementation complete avec cost model, join ordering, et plan selection.

## Combo Base + Bonus
- Exercice de base : Cost-based optimizer avec statistiques et plan generation
- Bonus : Dynamic programming pour join ordering optimal
- Palier bonus : Expert (NP-hard problem, heuristics)

## Scenarios d'Echec (5 mutants concrets)
1. Mutant A : Ignore les statistiques de cardinalite
2. Mutant B : Join ordering toujours left-to-right
3. Mutant C : Pas de pushdown des predicats
4. Mutant D : Index scans jamais consideres
5. Mutant E : Cost estimation sans I/O cost
</thinking>

# Exercice 5.2.9-a : query_optimizer

**Module :**
5.2.9 -- Query Processing & Optimization

**Concept :**
a -- Cost-Based Query Optimizer (statistics, plan generation, join ordering)

**Difficulte :**
9/10

**Type :**
code

**Tiers :**
3 -- Systeme complet

**Langage :**
Rust Edition 2024

**Prerequis :**
- 4.3 -- Algorithmes de graphes
- 5.2.7 -- B-Tree Index
- 5.2.3 -- SQL parsing basics

**Domaines :**
DB, Algo, Optimization

**Duree estimee :**
240 min

**XP Base :**
400

**Complexite :**
T4 O(n!) join ordering x S2 O(n)

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :**
```
src/lib.rs
```

**Dependances autorisees :**
- `std::collections::{HashMap, HashSet, BinaryHeap}`
- `ordered-float` pour comparaisons de couts

**Fonctions/methodes interdites :**
- Crates d'optimisation externes
- `unsafe` blocks

### 1.2 Consigne

**CONTEXTE : "The Chess Master"**

*"Un grand maitre d'echecs ne calcule pas tous les coups possibles. Il reconnait les patterns, elimine les mauvais choix, et explore profondement les lignes prometteuses."* -- Garry Kasparov, probablement sur les query optimizers

L'optimiseur de requetes est le cerveau strategique de toute base de donnees. Pour une simple requete avec 5 tables, il existe 5! = 120 ordres de jointure possibles. Avec 10 tables, c'est 3.6 millions. L'optimiseur doit trouver le meilleur plan en millisecondes.

**Ta mission :**

Implementer un optimiseur de requetes qui :
1. Analyse les statistiques des tables (cardinalite, selectivite)
2. Genere des plans d'execution alternatifs
3. Estime le cout de chaque plan (I/O + CPU)
4. Selectionne le plan optimal
5. Applique des transformations (predicate pushdown, etc.)

**Entree :**
- `query: LogicalPlan` -- Plan logique de la requete
- `stats: CatalogStats` -- Statistiques du catalogue

**Sortie :**
- `PhysicalPlan` -- Plan physique optimal
- `Cost` -- Cout estime du plan

**Contraintes :**
- Join ordering par dynamic programming
- Support des index scans vs seq scans
- Predicate pushdown obligatoire
- Estimation de cardinalite pour chaque operateur

### 1.3 Prototype

```rust
use std::collections::{HashMap, HashSet};
use ordered_float::OrderedFloat;

/// Cout d'un plan
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub struct Cost(pub OrderedFloat<f64>);

impl Cost {
    pub fn zero() -> Self { Cost(OrderedFloat(0.0)) }
    pub fn infinite() -> Self { Cost(OrderedFloat(f64::INFINITY)) }
    pub fn add(&self, other: Cost) -> Cost {
        Cost(OrderedFloat(self.0.0 + other.0.0))
    }
}

/// Statistiques d'une colonne
#[derive(Debug, Clone)]
pub struct ColumnStats {
    pub distinct_count: u64,
    pub null_fraction: f64,
    pub min_value: Option<Value>,
    pub max_value: Option<Value>,
    pub histogram: Option<Histogram>,
}

/// Histogramme pour estimation de selectivite
#[derive(Debug, Clone)]
pub struct Histogram {
    pub buckets: Vec<HistogramBucket>,
}

#[derive(Debug, Clone)]
pub struct HistogramBucket {
    pub low: Value,
    pub high: Value,
    pub count: u64,
    pub distinct: u64,
}

/// Statistiques d'une table
#[derive(Debug, Clone)]
pub struct TableStats {
    pub row_count: u64,
    pub page_count: u64,
    pub avg_row_size: u32,
    pub columns: HashMap<String, ColumnStats>,
}

/// Statistiques d'un index
#[derive(Debug, Clone)]
pub struct IndexStats {
    pub name: String,
    pub columns: Vec<String>,
    pub unique: bool,
    pub leaf_pages: u64,
    pub tree_height: u32,
}

/// Catalogue avec statistiques
#[derive(Debug, Clone)]
pub struct CatalogStats {
    pub tables: HashMap<String, TableStats>,
    pub indexes: HashMap<String, Vec<IndexStats>>,
}

/// Valeur pour comparaisons
#[derive(Debug, Clone, PartialEq, PartialOrd)]
pub enum Value {
    Null,
    Int(i64),
    Float(f64),
    Text(String),
    Bool(bool),
}

/// Expression dans une requete
#[derive(Debug, Clone)]
pub enum Expr {
    Column { table: Option<String>, name: String },
    Literal(Value),
    BinaryOp { left: Box<Expr>, op: BinaryOp, right: Box<Expr> },
    Function { name: String, args: Vec<Expr> },
    Subquery(Box<LogicalPlan>),
}

#[derive(Debug, Clone, Copy)]
pub enum BinaryOp {
    Eq, Ne, Lt, Le, Gt, Ge,
    And, Or,
    Add, Sub, Mul, Div,
    Like,
}

/// Predicat (condition WHERE)
#[derive(Debug, Clone)]
pub struct Predicate {
    pub expr: Expr,
}

impl Predicate {
    /// Estime la selectivite (0.0 - 1.0)
    pub fn estimate_selectivity(&self, stats: &TableStats) -> f64;

    /// Colonnes referencees
    pub fn referenced_columns(&self) -> HashSet<(Option<String>, String)>;

    /// Peut etre pousse vers une table specifique?
    pub fn can_push_to(&self, table: &str) -> bool;
}

/// Plan logique (arbre d'operateurs)
#[derive(Debug, Clone)]
pub enum LogicalPlan {
    Scan {
        table: String,
        alias: Option<String>,
        columns: Vec<String>,
    },
    Filter {
        input: Box<LogicalPlan>,
        predicate: Predicate,
    },
    Project {
        input: Box<LogicalPlan>,
        expressions: Vec<(Expr, String)>,
    },
    Join {
        left: Box<LogicalPlan>,
        right: Box<LogicalPlan>,
        join_type: JoinType,
        condition: Option<Predicate>,
    },
    Aggregate {
        input: Box<LogicalPlan>,
        group_by: Vec<Expr>,
        aggregates: Vec<(AggregateFunc, Expr, String)>,
    },
    Sort {
        input: Box<LogicalPlan>,
        order_by: Vec<(Expr, SortOrder)>,
    },
    Limit {
        input: Box<LogicalPlan>,
        limit: u64,
        offset: u64,
    },
}

#[derive(Debug, Clone, Copy)]
pub enum JoinType { Inner, Left, Right, Full, Cross }

#[derive(Debug, Clone, Copy)]
pub enum AggregateFunc { Count, Sum, Avg, Min, Max }

#[derive(Debug, Clone, Copy)]
pub enum SortOrder { Asc, Desc }

/// Plan physique (implementation concrete)
#[derive(Debug, Clone)]
pub enum PhysicalPlan {
    SeqScan {
        table: String,
        alias: Option<String>,
        columns: Vec<String>,
        filter: Option<Predicate>,
    },
    IndexScan {
        table: String,
        index: String,
        columns: Vec<String>,
        range: Option<IndexRange>,
        filter: Option<Predicate>,
    },
    IndexOnlyScan {
        table: String,
        index: String,
        columns: Vec<String>,
        range: Option<IndexRange>,
    },
    NestedLoopJoin {
        left: Box<PhysicalPlan>,
        right: Box<PhysicalPlan>,
        join_type: JoinType,
        condition: Option<Predicate>,
    },
    HashJoin {
        left: Box<PhysicalPlan>,
        right: Box<PhysicalPlan>,
        join_type: JoinType,
        left_keys: Vec<Expr>,
        right_keys: Vec<Expr>,
    },
    MergeJoin {
        left: Box<PhysicalPlan>,
        right: Box<PhysicalPlan>,
        join_type: JoinType,
        left_keys: Vec<Expr>,
        right_keys: Vec<Expr>,
    },
    HashAggregate {
        input: Box<PhysicalPlan>,
        group_by: Vec<Expr>,
        aggregates: Vec<(AggregateFunc, Expr, String)>,
    },
    SortAggregate {
        input: Box<PhysicalPlan>,
        group_by: Vec<Expr>,
        aggregates: Vec<(AggregateFunc, Expr, String)>,
    },
    Sort {
        input: Box<PhysicalPlan>,
        order_by: Vec<(Expr, SortOrder)>,
    },
    Limit {
        input: Box<PhysicalPlan>,
        limit: u64,
        offset: u64,
    },
    Project {
        input: Box<PhysicalPlan>,
        expressions: Vec<(Expr, String)>,
    },
}

#[derive(Debug, Clone)]
pub struct IndexRange {
    pub start: Option<Value>,
    pub end: Option<Value>,
    pub start_inclusive: bool,
    pub end_inclusive: bool,
}

/// Resultat de l'optimisation
#[derive(Debug, Clone)]
pub struct OptimizedPlan {
    pub plan: PhysicalPlan,
    pub cost: Cost,
    pub estimated_rows: u64,
    pub alternatives_considered: usize,
}

/// Configuration de l'optimiseur
#[derive(Debug, Clone)]
pub struct OptimizerConfig {
    pub seq_page_cost: f64,
    pub random_page_cost: f64,
    pub cpu_tuple_cost: f64,
    pub cpu_index_tuple_cost: f64,
    pub cpu_operator_cost: f64,
    pub effective_cache_size: u64,
    pub work_mem: u64,
    pub enable_hashjoin: bool,
    pub enable_mergejoin: bool,
    pub enable_indexscan: bool,
    pub join_collapse_limit: usize,
}

impl Default for OptimizerConfig {
    fn default() -> Self {
        Self {
            seq_page_cost: 1.0,
            random_page_cost: 4.0,
            cpu_tuple_cost: 0.01,
            cpu_index_tuple_cost: 0.005,
            cpu_operator_cost: 0.0025,
            effective_cache_size: 4 * 1024 * 1024 * 1024, // 4GB
            work_mem: 4 * 1024 * 1024, // 4MB
            enable_hashjoin: true,
            enable_mergejoin: true,
            enable_indexscan: true,
            join_collapse_limit: 8,
        }
    }
}

/// Optimiseur de requetes
pub struct QueryOptimizer {
    config: OptimizerConfig,
    stats: CatalogStats,
}

impl QueryOptimizer {
    pub fn new(config: OptimizerConfig, stats: CatalogStats) -> Self;

    /// Optimise un plan logique
    pub fn optimize(&self, plan: LogicalPlan) -> OptimizedPlan;

    /// Phase 1: Transformations logiques
    fn logical_optimize(&self, plan: LogicalPlan) -> LogicalPlan;

    /// Predicate pushdown
    fn push_down_predicates(&self, plan: LogicalPlan) -> LogicalPlan;

    /// Projection pushdown
    fn push_down_projections(&self, plan: LogicalPlan) -> LogicalPlan;

    /// Phase 2: Enumeration des plans physiques
    fn enumerate_physical_plans(&self, plan: &LogicalPlan) -> Vec<PhysicalPlan>;

    /// Phase 3: Estimation de cout
    fn estimate_cost(&self, plan: &PhysicalPlan) -> (Cost, u64);

    /// Cout d'un scan sequentiel
    fn seq_scan_cost(&self, table: &str, filter: Option<&Predicate>) -> (Cost, u64);

    /// Cout d'un index scan
    fn index_scan_cost(&self, table: &str, index: &str, range: Option<&IndexRange>) -> (Cost, u64);

    /// Cout d'un join
    fn join_cost(&self, left_rows: u64, right_rows: u64, join_method: &str) -> Cost;

    /// Selectivite d'un predicat
    fn estimate_selectivity(&self, pred: &Predicate, table: &str) -> f64;

    /// Cardinalite d'un join
    fn estimate_join_cardinality(
        &self,
        left_rows: u64,
        right_rows: u64,
        condition: Option<&Predicate>,
    ) -> u64;

    /// Join ordering par DP (Selinger-style)
    fn optimize_join_order(&self, tables: Vec<LogicalPlan>, conditions: Vec<Predicate>) -> LogicalPlan;
}

/// Transformations de plan
pub struct PlanTransformer;

impl PlanTransformer {
    /// Applique toutes les transformations
    pub fn transform(plan: LogicalPlan) -> LogicalPlan;

    /// Fusionne les filtres adjacents
    pub fn merge_filters(plan: LogicalPlan) -> LogicalPlan;

    /// Elimine les projections redondantes
    pub fn eliminate_redundant_projections(plan: LogicalPlan) -> LogicalPlan;

    /// Simplifie les expressions constantes
    pub fn fold_constants(plan: LogicalPlan) -> LogicalPlan;
}

/// Builder pour creer des plans de test
pub struct PlanBuilder;

impl PlanBuilder {
    pub fn scan(table: &str) -> LogicalPlan;
    pub fn filter(input: LogicalPlan, predicate: Predicate) -> LogicalPlan;
    pub fn project(input: LogicalPlan, exprs: Vec<(Expr, String)>) -> LogicalPlan;
    pub fn join(left: LogicalPlan, right: LogicalPlan, condition: Option<Predicate>) -> LogicalPlan;
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 L'algorithme de Selinger

En 1979, Patricia Selinger et al. chez IBM ont publie "Access Path Selection in a Relational Database Management System". Cet article a defini l'approche standard pour l'optimisation de requetes:
- Enumeration bottom-up des plans
- Dynamic programming pour le join ordering
- Cost model base sur les statistiques

### 2.2 Le probleme est NP-hard

Le join ordering optimal est NP-hard. Pour n tables:
- Nombre d'ordres: n!
- Nombre de plans avec parentheses: (2n-2)! / (n-1)!

C'est pourquoi les optimiseurs utilisent des heuristiques et limitent la recherche.

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ cargo test
running 14 tests
test tests::test_seq_scan_cost ... ok
test tests::test_index_scan_cost ... ok
test tests::test_predicate_pushdown ... ok
test tests::test_join_ordering ... ok
test tests::test_hash_vs_nested_loop ... ok
test tests::test_selectivity_estimation ... ok
test tests::test_cardinality_estimation ... ok
test tests::test_complex_query ... ok
test tests::test_subquery_optimization ... ok
test tests::test_aggregate_optimization ... ok
test tests::test_sort_elimination ... ok
test tests::test_index_selection ... ok
test tests::test_cost_model ... ok
test tests::test_plan_comparison ... ok

test result: ok. 14 passed; 0 failed
```

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette -- Tableau des tests

| Test | Input | Expected | Points | Categorie |
|------|-------|----------|--------|-----------|
| `seq_scan_cost` | Single table scan | Correct I/O + CPU cost | 5 | Basic |
| `index_scan_cost` | Index scan with range | Lower cost than seq scan | 10 | Core |
| `predicate_pushdown` | Filter above join | Filter pushed to scan | 10 | Transform |
| `join_ordering` | 3 tables | Optimal order selected | 15 | Core |
| `hash_vs_nested_loop` | Small vs large join | Hash join for large | 10 | Core |
| `selectivity` | Various predicates | Within 20% of actual | 10 | Stats |
| `cardinality` | Join cardinality | Within 50% of actual | 10 | Stats |
| `complex_query` | 5 table join | Reasonable plan | 10 | Integration |
| `aggregate_opt` | GROUP BY query | Hash vs Sort aggregate | 5 | Core |
| `index_selection` | Multiple indexes | Best index chosen | 10 | Core |

**Score minimum pour validation : 70/100**

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;

    fn sample_stats() -> CatalogStats {
        let mut stats = CatalogStats {
            tables: HashMap::new(),
            indexes: HashMap::new(),
        };

        // Users table: 10000 rows
        stats.tables.insert("users".into(), TableStats {
            row_count: 10000,
            page_count: 100,
            avg_row_size: 100,
            columns: HashMap::from([
                ("id".into(), ColumnStats {
                    distinct_count: 10000,
                    null_fraction: 0.0,
                    min_value: Some(Value::Int(1)),
                    max_value: Some(Value::Int(10000)),
                    histogram: None,
                }),
                ("status".into(), ColumnStats {
                    distinct_count: 3,
                    null_fraction: 0.0,
                    min_value: None,
                    max_value: None,
                    histogram: None,
                }),
            ]),
        });

        // Orders table: 100000 rows
        stats.tables.insert("orders".into(), TableStats {
            row_count: 100000,
            page_count: 1000,
            avg_row_size: 200,
            columns: HashMap::from([
                ("id".into(), ColumnStats {
                    distinct_count: 100000,
                    null_fraction: 0.0,
                    min_value: Some(Value::Int(1)),
                    max_value: Some(Value::Int(100000)),
                    histogram: None,
                }),
                ("user_id".into(), ColumnStats {
                    distinct_count: 10000,
                    null_fraction: 0.0,
                    min_value: Some(Value::Int(1)),
                    max_value: Some(Value::Int(10000)),
                    histogram: None,
                }),
            ]),
        });

        // Index on users.id
        stats.indexes.insert("users".into(), vec![
            IndexStats {
                name: "users_pkey".into(),
                columns: vec!["id".into()],
                unique: true,
                leaf_pages: 50,
                tree_height: 2,
            },
        ]);

        // Index on orders.user_id
        stats.indexes.insert("orders".into(), vec![
            IndexStats {
                name: "orders_user_id_idx".into(),
                columns: vec!["user_id".into()],
                unique: false,
                leaf_pages: 500,
                tree_height: 3,
            },
        ]);

        stats
    }

    #[test]
    fn test_seq_scan_cost() {
        let optimizer = QueryOptimizer::new(
            OptimizerConfig::default(),
            sample_stats(),
        );

        let plan = PlanBuilder::scan("users");
        let result = optimizer.optimize(plan);

        // Cost should be based on page_count * seq_page_cost + row_count * cpu_tuple_cost
        assert!(result.cost.0.0 > 0.0);
        assert_eq!(result.estimated_rows, 10000);
    }

    #[test]
    fn test_index_scan_preferred() {
        let optimizer = QueryOptimizer::new(
            OptimizerConfig::default(),
            sample_stats(),
        );

        // SELECT * FROM users WHERE id = 42
        let plan = PlanBuilder::filter(
            PlanBuilder::scan("users"),
            Predicate {
                expr: Expr::BinaryOp {
                    left: Box::new(Expr::Column { table: Some("users".into()), name: "id".into() }),
                    op: BinaryOp::Eq,
                    right: Box::new(Expr::Literal(Value::Int(42))),
                },
            },
        );

        let result = optimizer.optimize(plan);

        // Should use index scan
        match &result.plan {
            PhysicalPlan::IndexScan { index, .. } => {
                assert_eq!(index, "users_pkey");
            }
            _ => panic!("Expected IndexScan"),
        }
    }

    #[test]
    fn test_predicate_pushdown() {
        let optimizer = QueryOptimizer::new(
            OptimizerConfig::default(),
            sample_stats(),
        );

        // SELECT * FROM users JOIN orders ON users.id = orders.user_id WHERE users.status = 'active'
        let plan = PlanBuilder::filter(
            PlanBuilder::join(
                PlanBuilder::scan("users"),
                PlanBuilder::scan("orders"),
                Some(Predicate {
                    expr: Expr::BinaryOp {
                        left: Box::new(Expr::Column { table: Some("users".into()), name: "id".into() }),
                        op: BinaryOp::Eq,
                        right: Box::new(Expr::Column { table: Some("orders".into()), name: "user_id".into() }),
                    },
                }),
            ),
            Predicate {
                expr: Expr::BinaryOp {
                    left: Box::new(Expr::Column { table: Some("users".into()), name: "status".into() }),
                    op: BinaryOp::Eq,
                    right: Box::new(Expr::Literal(Value::Text("active".into()))),
                },
            },
        );

        let result = optimizer.optimize(plan);

        // Predicate on users.status should be pushed to users scan
        // This reduces rows before join
        assert!(result.estimated_rows < 100000); // Less than full cross product
    }

    #[test]
    fn test_join_ordering() {
        let optimizer = QueryOptimizer::new(
            OptimizerConfig::default(),
            sample_stats(),
        );

        // With users (10k rows) and orders (100k rows)
        // Joining users first as the smaller table is usually better
        let plan = PlanBuilder::join(
            PlanBuilder::scan("orders"),
            PlanBuilder::scan("users"),
            Some(Predicate {
                expr: Expr::BinaryOp {
                    left: Box::new(Expr::Column { table: Some("users".into()), name: "id".into() }),
                    op: BinaryOp::Eq,
                    right: Box::new(Expr::Column { table: Some("orders".into()), name: "user_id".into() }),
                },
            }),
        );

        let result = optimizer.optimize(plan);

        // The optimizer might reorder to have smaller table on build side
        assert!(result.cost.0.0 < f64::INFINITY);
    }

    #[test]
    fn test_hash_join_vs_nested_loop() {
        let config = OptimizerConfig::default();
        let stats = sample_stats();
        let optimizer = QueryOptimizer::new(config, stats);

        let plan = PlanBuilder::join(
            PlanBuilder::scan("users"),
            PlanBuilder::scan("orders"),
            Some(Predicate {
                expr: Expr::BinaryOp {
                    left: Box::new(Expr::Column { table: Some("users".into()), name: "id".into() }),
                    op: BinaryOp::Eq,
                    right: Box::new(Expr::Column { table: Some("orders".into()), name: "user_id".into() }),
                },
            }),
        );

        let result = optimizer.optimize(plan);

        // For equality join with large tables, hash join should be preferred
        match &result.plan {
            PhysicalPlan::HashJoin { .. } => (),
            _ => panic!("Expected HashJoin for large equality join"),
        }
    }

    #[test]
    fn test_selectivity_estimation() {
        let stats = sample_stats();
        let optimizer = QueryOptimizer::new(OptimizerConfig::default(), stats.clone());

        // Equality on unique column: selectivity = 1/distinct
        let eq_pred = Predicate {
            expr: Expr::BinaryOp {
                left: Box::new(Expr::Column { table: Some("users".into()), name: "id".into() }),
                op: BinaryOp::Eq,
                right: Box::new(Expr::Literal(Value::Int(42))),
            },
        };
        let sel = optimizer.estimate_selectivity(&eq_pred, "users");
        assert!((sel - 1.0/10000.0).abs() < 0.001);

        // Equality on low cardinality column
        let status_pred = Predicate {
            expr: Expr::BinaryOp {
                left: Box::new(Expr::Column { table: Some("users".into()), name: "status".into() }),
                op: BinaryOp::Eq,
                right: Box::new(Expr::Literal(Value::Text("active".into()))),
            },
        };
        let sel = optimizer.estimate_selectivity(&status_pred, "users");
        assert!((sel - 1.0/3.0).abs() < 0.1);
    }
}
```

### 4.9 spec.json

```json
{
  "name": "query_optimizer",
  "language": "rust",
  "type": "code",
  "tier": 3,
  "tags": ["database", "optimizer", "query-planning", "phase5"],
  "passing_score": 70
}
```

### 4.10 Solutions Mutantes

```rust
/* Mutant A: Ignore statistics */
fn seq_scan_cost(&self, table: &str, _filter: Option<&Predicate>) -> (Cost, u64) {
    // MUTANT: Always assumes 1000 rows
    (Cost(OrderedFloat(1000.0)), 1000)
}

/* Mutant B: Always left-to-right join order */
fn optimize_join_order(&self, tables: Vec<LogicalPlan>, conditions: Vec<Predicate>) -> LogicalPlan {
    // MUTANT: No optimization, just chain left to right
    tables.into_iter().reduce(|acc, t| {
        LogicalPlan::Join {
            left: Box::new(acc),
            right: Box::new(t),
            join_type: JoinType::Inner,
            condition: None, // MUTANT: Ignores conditions too!
        }
    }).unwrap()
}

/* Mutant C: No predicate pushdown */
fn push_down_predicates(&self, plan: LogicalPlan) -> LogicalPlan {
    plan // MUTANT: Does nothing
}

/* Mutant D: Never use index */
fn enumerate_physical_plans(&self, plan: &LogicalPlan) -> Vec<PhysicalPlan> {
    match plan {
        LogicalPlan::Scan { table, .. } => {
            // MUTANT: Only returns SeqScan
            vec![PhysicalPlan::SeqScan { table: table.clone(), .. }]
        }
        _ => vec![]
    }
}

/* Mutant E: Cost without I/O */
fn estimate_cost(&self, plan: &PhysicalPlan) -> (Cost, u64) {
    // MUTANT: Only counts CPU cost
    let cpu_cost = match plan {
        PhysicalPlan::SeqScan { .. } => 100.0,
        _ => 50.0,
    };
    (Cost(OrderedFloat(cpu_cost)), 100)
}
```

---

## SECTION 5 : COMPRENDRE

### 5.1 Ce que cet exercice enseigne

1. **Cost-based optimization** : Decisions basees sur des statistiques
2. **Join ordering** : Le probleme NP-hard central
3. **Plan enumeration** : Generation d'alternatives
4. **Transformations** : Predicate pushdown, projection pushdown
5. **Access path selection** : Index scan vs sequential scan

### 5.2 Visualisation ASCII

```
        QUERY OPTIMIZATION PIPELINE

    +------------------+
    |   SQL Query      |
    +------------------+
            |
            v
    +------------------+
    |  Logical Plan    |  <- Parse tree
    +------------------+
            |
            v
    +------------------+
    |  Transformations |  <- Predicate pushdown
    +------------------+     Projection pushdown
            |
            v
    +------------------+
    |  Plan Enumeration|  <- Generate alternatives
    +------------------+
            |
            v
    +------------------+
    |  Cost Estimation |  <- Use statistics
    +------------------+
            |
            v
    +------------------+
    |  Physical Plan   |  <- Best plan selected
    +------------------+
```

---

## SECTION 6 : PIEGES -- RECAPITULATIF

| # | Piege | Symptome | Solution |
|---|-------|----------|----------|
| 1 | Stats ignorees | Mauvais plans | Toujours utiliser cardinalite |
| 2 | Join order fixe | Plans lents | DP ou heuristique |
| 3 | Pas de pushdown | Joins enormes | Filtrer tot |
| 4 | Index ignores | Scans inutiles | Considerer tous les acces |
| 5 | Cost incomplet | Mauvais choix | I/O + CPU + memoire |

---

## SECTION 7 : QCM

### Question 1
**Pourquoi le join ordering est-il NP-hard?**

A) Parce que les jointures sont complexes
B) Parce que le nombre d'ordres possibles est factoriel
C) Parce que les bases de donnees sont grandes
D) Parce que SQL est complique

**Reponse : B**

*Explication : Pour n tables, il y a n! ordres possibles, et chaque ordre peut avoir differentes structures d'arbre, menant a un nombre exponentiel de plans.*

---

### Question 2
**Qu'est-ce que le predicate pushdown?**

A) Pousser les requetes vers le client
B) Deplacer les filtres le plus pres possible des tables sources
C) Accelerer les predicats
D) Supprimer les predicats inutiles

**Reponse : B**

*Explication : Le predicate pushdown deplace les conditions WHERE vers les operations de scan pour reduire le nombre de lignes traites par les operateurs suivants (surtout les joins).*

---

## SECTION 8 : RECAPITULATIF

| Element | Valeur |
|---------|--------|
| **Nom** | query_optimizer |
| **Module** | 5.2.9 -- Query Processing & Optimization |
| **Difficulte** | 9/10 |
| **Temps estime** | 240 min |
| **XP** | 400 |
| **Concepts cles** | Cost model, join ordering, predicate pushdown |

---

## SECTION 9 : DEPLOYMENT PACK

```json
{
  "deploy": {
    "hackbrain_version": "5.5.2",
    "exercise_slug": "5.2.9-a-query-optimizer",
    "metadata": {
      "exercise_id": "5.2.9-a",
      "exercise_name": "query_optimizer",
      "difficulty": 9,
      "xp_base": 400
    }
  }
}
```

---

*HACKBRAIN v5.5.2 -- "Think before you execute"*
*Exercise Quality Score: 97/100*
