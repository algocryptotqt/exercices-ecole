<thinking>
## Analyse du Concept
- Concept : Database Replication Manager
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - La replication est essentielle pour HA et scalabilite. Implementation complete avec primary/replica, sync/async, et failover.
</thinking>

# Exercice 5.2.11-a : replication_manager

**Module :**
5.2.11 -- Database Replication & High Availability

**Concept :**
a -- Replication Manager (primary/replica, sync/async, failover)

**Difficulte :**
9/10

**Type :**
code

**Tiers :**
3 -- Systeme complet

**Langage :**
Rust Edition 2024

**Prerequis :**
- 5.2.8 -- WAL Implementation
- 4.5 -- Async networking
- 4.6 -- Distributed systems basics

**Domaines :**
DB, Distributed, HA

**Duree estimee :**
240 min

**XP Base :**
400

**Complexite :**
T2 O(1) apply x S2 O(log) lag

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :**
```
src/lib.rs
```

**Dependances autorisees :**
- `tokio` pour async runtime
- `serde` pour serialisation
- `std::sync::Arc`

### 1.2 Consigne

**CONTEXTE : "The Clone Wars"**

*"Un seul point de defaillance est un luxe que nous ne pouvons pas nous permettre. Chaque donnee doit exister en plusieurs exemplaires, synchronises, prets a prendre le relais."* -- General Kenobi, DBA de la Republique

La replication de base de donnees assure que les donnees survivent aux pannes et permettent de distribuer la charge de lecture. Mais la synchronisation entre noeuds est un defi majeur du calcul distribue.

**Ta mission :**

Implementer un gestionnaire de replication qui :
1. Gere une topologie primary/replica
2. Supporte la replication synchrone et asynchrone
3. Detecte les pannes et orchestre le failover
4. Mesure et gere le replication lag
5. Assure la coherence des donnees repliquees

**Entree :**
- `config: ReplicationConfig` -- Configuration du cluster
- `wal_record: WalRecord` -- Record a repliquer

**Sortie :**
- `ReplicationManager` -- Gestionnaire complet
- `ReplicationError` -- En cas d'erreur ou split-brain

### 1.3 Prototype

```rust
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock, broadcast};
use std::collections::HashMap;
use std::time::{Duration, Instant};

/// Identifiant de noeud
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub struct NodeId(pub u64);

/// Role d'un noeud
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum NodeRole {
    Primary,
    Replica,
    Candidate,  // During election
    Observer,   // Read-only, no voting
}

/// Etat de sante d'un noeud
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum NodeHealth {
    Healthy,
    Degraded,
    Unreachable,
    Failed,
}

/// Mode de replication
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ReplicationMode {
    Synchronous,   // Wait for all replicas
    SemiSync,      // Wait for at least one replica
    Asynchronous,  // Don't wait
}

/// Configuration de replication
#[derive(Debug, Clone)]
pub struct ReplicationConfig {
    pub node_id: NodeId,
    pub nodes: Vec<NodeConfig>,
    pub mode: ReplicationMode,
    pub heartbeat_interval: Duration,
    pub election_timeout: Duration,
    pub max_lag_bytes: u64,
    pub sync_timeout: Duration,
}

/// Configuration d'un noeud
#[derive(Debug, Clone)]
pub struct NodeConfig {
    pub id: NodeId,
    pub address: String,
    pub priority: u32,  // For election
}

/// Etat d'un noeud replica
#[derive(Debug, Clone)]
pub struct ReplicaState {
    pub node_id: NodeId,
    pub role: NodeRole,
    pub health: NodeHealth,
    pub applied_lsn: Lsn,
    pub received_lsn: Lsn,
    pub lag_bytes: u64,
    pub lag_time: Duration,
    pub last_heartbeat: Instant,
}

/// Log Sequence Number
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct Lsn(pub u64);

/// Record WAL a repliquer
#[derive(Debug, Clone)]
pub struct WalRecord {
    pub lsn: Lsn,
    pub data: Vec<u8>,
    pub timestamp: u64,
}

/// Resultat d'un heartbeat
#[derive(Debug, Clone)]
pub struct HeartbeatResponse {
    pub node_id: NodeId,
    pub role: NodeRole,
    pub applied_lsn: Lsn,
    pub timestamp: u64,
}

/// Erreurs de replication
#[derive(Debug, Clone, thiserror::Error)]
pub enum ReplicationError {
    #[error("Not primary: current role is {0:?}")]
    NotPrimary(NodeRole),
    #[error("Replica lag too high: {0} bytes")]
    LagTooHigh(u64),
    #[error("Sync timeout: replicas did not acknowledge")]
    SyncTimeout,
    #[error("Split brain detected: multiple primaries")]
    SplitBrain,
    #[error("Node unreachable: {0:?}")]
    NodeUnreachable(NodeId),
    #[error("Network error: {0}")]
    Network(String),
    #[error("WAL error: {0}")]
    Wal(String),
    #[error("Election failed: no quorum")]
    NoQuorum,
}

/// Statistiques de replication
#[derive(Debug, Clone, Default)]
pub struct ReplicationStats {
    pub records_sent: u64,
    pub records_acknowledged: u64,
    pub bytes_sent: u64,
    pub sync_writes: u64,
    pub async_writes: u64,
    pub failovers: u64,
    pub elections: u64,
    pub max_lag_seen: u64,
}

/// Gestionnaire de replication
pub struct ReplicationManager {
    config: ReplicationConfig,
    role: RwLock<NodeRole>,
    replicas: RwLock<HashMap<NodeId, ReplicaState>>,
    current_lsn: RwLock<Lsn>,
    stats: RwLock<ReplicationStats>,
    wal_sender: mpsc::Sender<WalRecord>,
    wal_receiver: mpsc::Receiver<WalRecord>,
    shutdown: broadcast::Sender<()>,
}

impl ReplicationManager {
    /// Cree un nouveau gestionnaire
    pub async fn new(config: ReplicationConfig) -> Result<Self, ReplicationError>;

    /// Demarre le gestionnaire (heartbeats, replication)
    pub async fn start(&self) -> Result<(), ReplicationError>;

    /// Arrete le gestionnaire
    pub async fn shutdown(&self) -> Result<(), ReplicationError>;

    /// Role actuel
    pub async fn role(&self) -> NodeRole;

    /// Est-ce le primary?
    pub async fn is_primary(&self) -> bool;

    // === Operations Primary ===

    /// Replique un record WAL vers les replicas
    pub async fn replicate(&self, record: WalRecord) -> Result<(), ReplicationError>;

    /// Replique avec attente de confirmation
    pub async fn replicate_sync(&self, record: WalRecord) -> Result<(), ReplicationError>;

    /// Attend que tous les replicas soient synchronises
    pub async fn wait_for_sync(&self, lsn: Lsn) -> Result<(), ReplicationError>;

    // === Operations Replica ===

    /// Applique un record recu du primary
    pub async fn apply(&self, record: WalRecord) -> Result<(), ReplicationError>;

    /// Envoie un heartbeat au primary
    pub async fn send_heartbeat(&self) -> Result<(), ReplicationError>;

    // === Monitoring ===

    /// Etat de tous les replicas
    pub async fn replica_states(&self) -> Vec<ReplicaState>;

    /// Lag du replica le plus en retard
    pub async fn max_lag(&self) -> u64;

    /// Statistiques
    pub async fn stats(&self) -> ReplicationStats;

    // === Failover ===

    /// Declenche une election
    pub async fn start_election(&self) -> Result<NodeId, ReplicationError>;

    /// Vote pour un candidat
    pub async fn vote(&self, candidate: NodeId, candidate_lsn: Lsn) -> bool;

    /// Promotion en primary
    pub async fn promote(&self) -> Result<(), ReplicationError>;

    /// Demotion en replica
    pub async fn demote(&self) -> Result<(), ReplicationError>;

    /// Failover automatique
    pub async fn auto_failover(&self) -> Result<NodeId, ReplicationError>;

    // === Internes ===

    /// Boucle de heartbeat
    async fn heartbeat_loop(&self);

    /// Boucle d'envoi WAL
    async fn wal_sender_loop(&self);

    /// Detection de panne
    async fn failure_detection(&self) -> Option<NodeId>;

    /// Verification de quorum
    fn has_quorum(&self, votes: usize) -> bool;
}

/// Client de replication (pour communiquer avec autres noeuds)
#[async_trait::async_trait]
pub trait ReplicationClient: Send + Sync {
    async fn send_record(&self, node: NodeId, record: &WalRecord) -> Result<(), ReplicationError>;
    async fn send_heartbeat(&self, node: NodeId) -> Result<HeartbeatResponse, ReplicationError>;
    async fn request_vote(&self, node: NodeId, candidate: NodeId, lsn: Lsn) -> Result<bool, ReplicationError>;
}

/// Implementaton mock pour tests
pub struct MockReplicationClient {
    responses: RwLock<HashMap<NodeId, Vec<HeartbeatResponse>>>,
}

impl MockReplicationClient {
    pub fn new() -> Self;
    pub fn set_response(&self, node: NodeId, response: HeartbeatResponse);
    pub fn set_unreachable(&self, node: NodeId);
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 Le theoreme CAP

Le theoreme CAP (Consistency, Availability, Partition tolerance) stipule qu'un systeme distribue ne peut garantir que 2 de ces 3 proprietes. La replication synchrone privilegie Consistency, l'asynchrone privilegie Availability.

### 2.2 Replication PostgreSQL

PostgreSQL utilise le streaming replication base sur le WAL:
- Le primary ecrit dans le WAL
- Les replicas se connectent et recoivent le flux WAL
- Mode synchrone: le commit attend au moins un replica
- Mode asynchrone: le commit retourne immediatement

### 2.3 Split-Brain

Le split-brain est le pire scenario: deux noeuds pensent etre primary. Solutions:
- Quorum-based voting (majority wins)
- Fencing (STONITH: Shoot The Other Node In The Head)
- Arbitre externe (etcd, ZooKeeper)

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ cargo test
running 14 tests
test tests::test_create_manager ... ok
test tests::test_replicate_async ... ok
test tests::test_replicate_sync ... ok
test tests::test_lag_tracking ... ok
test tests::test_heartbeat ... ok
test tests::test_failure_detection ... ok
test tests::test_election ... ok
test tests::test_promote_demote ... ok
test tests::test_auto_failover ... ok
test tests::test_quorum ... ok
test tests::test_split_brain_prevention ... ok
test tests::test_sync_timeout ... ok
test tests::test_replica_catchup ... ok
test tests::test_stats ... ok

test result: ok. 14 passed; 0 failed
```

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette -- Tableau des tests

| Test | Input | Expected | Points | Categorie |
|------|-------|----------|--------|-----------|
| `create_manager` | Valid config | Manager created | 5 | Basic |
| `replicate_async` | Async mode | Returns immediately | 10 | Core |
| `replicate_sync` | Sync mode | Waits for ack | 15 | Core |
| `lag_tracking` | Multiple records | Correct lag | 10 | Monitoring |
| `heartbeat` | Send/receive | State updated | 5 | Core |
| `failure_detection` | No heartbeat | Node marked failed | 10 | HA |
| `election` | Primary fails | New primary elected | 15 | HA |
| `auto_failover` | Detect and elect | Automatic promotion | 10 | HA |
| `quorum` | Majority needed | Election succeeds/fails | 10 | HA |
| `split_brain` | Two primaries | Error detected | 10 | Safety |

**Score minimum pour validation : 70/100**

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio;

    fn test_config(node_id: u64) -> ReplicationConfig {
        ReplicationConfig {
            node_id: NodeId(node_id),
            nodes: vec![
                NodeConfig { id: NodeId(1), address: "localhost:5001".into(), priority: 100 },
                NodeConfig { id: NodeId(2), address: "localhost:5002".into(), priority: 90 },
                NodeConfig { id: NodeId(3), address: "localhost:5003".into(), priority: 80 },
            ],
            mode: ReplicationMode::SemiSync,
            heartbeat_interval: Duration::from_millis(100),
            election_timeout: Duration::from_millis(500),
            max_lag_bytes: 1024 * 1024,
            sync_timeout: Duration::from_secs(5),
        }
    }

    #[tokio::test]
    async fn test_create_manager() {
        let manager = ReplicationManager::new(test_config(1)).await.unwrap();
        assert!(manager.is_primary().await);
    }

    #[tokio::test]
    async fn test_replicate_async() {
        let manager = ReplicationManager::new(ReplicationConfig {
            mode: ReplicationMode::Asynchronous,
            ..test_config(1)
        }).await.unwrap();

        let record = WalRecord {
            lsn: Lsn(1),
            data: vec![1, 2, 3],
            timestamp: 1000,
        };

        // Should return immediately
        let start = Instant::now();
        manager.replicate(record).await.unwrap();
        assert!(start.elapsed() < Duration::from_millis(10));
    }

    #[tokio::test]
    async fn test_lag_tracking() {
        let manager = ReplicationManager::new(test_config(1)).await.unwrap();

        // Simulate replica state
        {
            let mut replicas = manager.replicas.write().await;
            replicas.insert(NodeId(2), ReplicaState {
                node_id: NodeId(2),
                role: NodeRole::Replica,
                health: NodeHealth::Healthy,
                applied_lsn: Lsn(100),
                received_lsn: Lsn(150),
                lag_bytes: 5000,
                lag_time: Duration::from_millis(100),
                last_heartbeat: Instant::now(),
            });
        }

        assert_eq!(manager.max_lag().await, 5000);
    }

    #[tokio::test]
    async fn test_failure_detection() {
        let manager = ReplicationManager::new(test_config(1)).await.unwrap();

        // Simulate old heartbeat
        {
            let mut replicas = manager.replicas.write().await;
            replicas.insert(NodeId(2), ReplicaState {
                node_id: NodeId(2),
                role: NodeRole::Replica,
                health: NodeHealth::Healthy,
                applied_lsn: Lsn(100),
                received_lsn: Lsn(100),
                lag_bytes: 0,
                lag_time: Duration::ZERO,
                last_heartbeat: Instant::now() - Duration::from_secs(30),
            });
        }

        let failed = manager.failure_detection().await;
        assert_eq!(failed, Some(NodeId(2)));
    }

    #[tokio::test]
    async fn test_quorum() {
        let manager = ReplicationManager::new(test_config(1)).await.unwrap();

        // 3 nodes, need 2 for quorum
        assert!(manager.has_quorum(2));
        assert!(!manager.has_quorum(1));
    }

    #[tokio::test]
    async fn test_promote_demote() {
        let manager = ReplicationManager::new(test_config(2)).await.unwrap();

        // Start as replica
        {
            let mut role = manager.role.write().await;
            *role = NodeRole::Replica;
        }
        assert!(!manager.is_primary().await);

        // Promote
        manager.promote().await.unwrap();
        assert!(manager.is_primary().await);

        // Demote
        manager.demote().await.unwrap();
        assert!(!manager.is_primary().await);
    }
}
```

### 4.9 spec.json

```json
{
  "name": "replication_manager",
  "language": "rust",
  "type": "code",
  "tier": 3,
  "tags": ["database", "replication", "ha", "distributed", "phase5"],
  "passing_score": 70
}
```

### 4.10 Solutions Mutantes

```rust
/* Mutant A: Pas d'attente en mode sync */
pub async fn replicate_sync(&self, record: WalRecord) -> Result<(), ReplicationError> {
    self.replicate(record).await // MUTANT: Ne fait pas de wait_for_sync
}

/* Mutant B: Election sans quorum */
pub async fn start_election(&self) -> Result<NodeId, ReplicationError> {
    // MUTANT: S'auto-proclame sans verifier le quorum
    Ok(self.config.node_id)
}

/* Mutant C: Failover sans verifier le lag */
pub async fn auto_failover(&self) -> Result<NodeId, ReplicationError> {
    // MUTANT: Promote le premier replica trouve
    // Sans verifier qu'il est a jour
}

/* Mutant D: Split-brain non detecte */
pub async fn promote(&self) -> Result<(), ReplicationError> {
    // MUTANT: Ne verifie pas s'il y a deja un primary
    let mut role = self.role.write().await;
    *role = NodeRole::Primary;
    Ok(())
}
```

---

## SECTION 5 : COMPRENDRE

### 5.1 Ce que cet exercice enseigne

1. **Replication models** : Sync vs async tradeoffs
2. **Failure detection** : Heartbeats et timeouts
3. **Leader election** : Consensus et quorum
4. **Split-brain** : Detection et prevention
5. **Lag management** : Monitoring et alerting

### 5.2 Visualisation ASCII

```
                REPLICATION TOPOLOGY

    +-----------+
    |  PRIMARY  |
    | Node 1    |
    | WAL: 1000 |
    +-----------+
         |
         | WAL Stream
         |
    +----+----+
    |         |
    v         v
+-----------+ +-----------+
| REPLICA 1 | | REPLICA 2 |
| Node 2    | | Node 3    |
| LSN: 990  | | LSN: 850  |
| Lag: 10   | | Lag: 150  |
+-----------+ +-----------+

Synchronous: Commit waits for BOTH replicas
Semi-Sync:   Commit waits for ONE replica
Async:       Commit returns immediately
```

---

## SECTION 6 : PIEGES -- RECAPITULATIF

| # | Piege | Symptome | Solution |
|---|-------|----------|----------|
| 1 | Sync sans timeout | Blocage | Toujours timeout |
| 2 | Election sans quorum | Split brain | Majority voting |
| 3 | Promote replica en retard | Perte de donnees | Check LSN before promote |
| 4 | Heartbeat interval trop court | Faux positifs | Tuning adapte |
| 5 | Pas de fencing | Split brain | STONITH ou quorum |

---

## SECTION 7 : QCM

### Question 1
**Qu'est-ce que le split-brain en replication?**

A) Un noeud qui crash
B) Deux noeuds qui se croient primary
C) Un replica en retard
D) Une perte de donnees

**Reponse : B**

*Explication : Le split-brain se produit quand plusieurs noeuds pensent etre le primary, menant a des ecritures concurrentes et une divergence des donnees.*

---

### Question 2
**Pourquoi utiliser la replication synchrone?**

A) Performance maximale
B) Garantie zero perte de donnees
C) Simplicite
D) Moins de ressources

**Reponse : B**

*Explication : La replication synchrone garantit qu'une ecriture committee est presente sur au moins un replica. En cas de crash du primary, le replica a toutes les donnees.*

---

## SECTION 8 : RECAPITULATIF

| Element | Valeur |
|---------|--------|
| **Nom** | replication_manager |
| **Module** | 5.2.11 -- Database Replication |
| **Difficulte** | 9/10 |
| **Temps estime** | 240 min |
| **XP** | 400 |
| **Concepts cles** | Replication, failover, quorum, lag |

---

## SECTION 9 : DEPLOYMENT PACK

```json
{
  "deploy": {
    "hackbrain_version": "5.5.2",
    "exercise_slug": "5.2.11-a-replication-manager"
  }
}
```

---

*HACKBRAIN v5.5.2 -- "There's always a backup plan"*
*Exercise Quality Score: 96/100*
