<thinking>
## Analyse du Concept
- Concept : Time-Series Store
- Phase demandee : 5 (Advanced Systems)
- Adapte ? OUI - Le stockage de series temporelles est crucial pour le monitoring, IoT, et les metriques. L'exercice combine structures de donnees efficaces, compression, et requetes temporelles.

## Combo Base + Bonus
- Exercice de base : Store de series temporelles avec ingestion, compression, downsampling, et requetes par intervalle
- Bonus : Implementation de retention policies avec TTL automatique et aggregations continues
- Palier bonus : Avance (gestion memoire + algorithmes de compression)
- Progression logique ? OUI - Base = stockage et requetes, Bonus = politiques avancees

## Prerequis & Difficulte
- Prerequis reels : BTreeMap, gestion du temps (chrono), compression delta, agregations
- Difficulte estimee : 8/10 (base), 9/10 (bonus)
- Coherent avec phase 5 ? OUI

## Aspect Fun/Culture
- Contexte choisi : "Prometheus Unbound" - Reference au monitoring et a la mythologie
- MEME mnemonique : "Time is a flat circle" (True Detective)
- Pourquoi c'est fun : Les series temporelles sont partout dans le monitoring moderne

## Scenarios d'Echec (5 mutants concrets)
1. Mutant A (Boundary) : Timestamps negatifs acceptes sans erreur
2. Mutant B (Compression) : Delta encoding qui deborde sur les grandes valeurs
3. Mutant C (Query) : Range query inclusive/exclusive inversee
4. Mutant D (Downsampling) : Moyenne calculee avec division entiere
5. Mutant E (Retention) : TTL qui ne prend pas en compte le fuseau horaire

## Verdict
VALIDE - Exercice de qualite industrielle couvrant les fondamentaux du stockage time-series
</thinking>

# Exercice 5.2.13-a : timeseries_store

**Module :**
5.2.13 - Time-Series Database Fundamentals

**Concept :**
a - Time-Series Storage (ingestion, compression, downsampling, queries)

**Difficulte :**
8/10

**Type :**
code

**Tiers :**
2 - Concepts combines

**Langage :**
Rust Edition 2024

**Prerequis :**
- 2.1 - Types primitifs et ownership
- 2.5 - Collections (BTreeMap, Vec)
- 2.7 - Gestion du temps (std::time, chrono)
- 5.2.1 - Structures de stockage

**Domaines :**
DB, Perf, Algo

**Duree estimee :**
180 min

**XP Base :**
300

**Complexite :**
T2 O(log n) x S2 O(n)

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :**
```
src/lib.rs
```

**Dependances autorisees :**
- `std::collections::{BTreeMap, HashMap, VecDeque}`
- `std::time::{Duration, SystemTime, UNIX_EPOCH}`

**Fonctions/methodes interdites :**
- Crates externes (`influxdb`, `timescaledb`, etc.)
- `unsafe` blocks

### 1.2 Consigne

**CONTEXTE : "Prometheus Unbound"**

*"Dans l'Olympe moderne du monitoring, chaque metrique est une goutte dans l'ocean du temps. Prometheus collecte, stocke, et interroge ces donnees pour prevenir les catastrophes avant qu'elles ne surviennent."*

Les bases de donnees time-series sont optimisees pour stocker et interroger des donnees horodatees. Elles sont au coeur de tout systeme de monitoring, IoT, ou analyse financiere.

**Ta mission :**

Implementer un `TimeSeriesStore` qui permet de :
1. Ingerer des points de donnees (timestamp, valeur, tags)
2. Compresser les donnees avec delta encoding
3. Effectuer du downsampling (reduction de resolution)
4. Executer des requetes par intervalle de temps
5. Calculer des agregations (min, max, avg, sum, count)
6. Gerer la retention des donnees (TTL)

**Entree :**
- `metric: &str` - Nom de la metrique
- `timestamp: u64` - Timestamp Unix en millisecondes
- `value: f64` - Valeur de la metrique
- `tags: HashMap<String, String>` - Labels/tags associes

**Sortie :**
- `TimeSeriesStore` - Structure de stockage
- `QueryResult` - Resultat des requetes
- `TimeSeriesError` - En cas d'erreur

**Contraintes :**
- Les timestamps doivent etre positifs
- Les donnees doivent etre stockees de maniere ordonnee
- La compression doit etre sans perte pour les requetes
- Le downsampling utilise des fenetres de temps fixes

**Exemples :**

| Operation | Input | Resultat |
|-----------|-------|----------|
| `store.write("cpu", 1000, 45.5, tags)` | Point de donnee | `Ok(())` |
| `store.query("cpu", 0, 2000)` | Intervalle | `Vec<DataPoint>` |
| `store.aggregate("cpu", 0, 2000, Avg)` | Agregation | `Ok(45.5)` |

### 1.2.2 Consigne Academique

Implementer une structure `TimeSeriesStore` pour le stockage efficace de donnees temporelles. La structure doit supporter l'ingestion haute performance, la compression delta, le downsampling configurable, et les requetes par intervalle avec agregations.

### 1.3 Prototype

```rust
use std::collections::{BTreeMap, HashMap};

#[derive(Debug, Clone, PartialEq)]
pub struct DataPoint {
    pub timestamp: u64,
    pub value: f64,
    pub tags: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub struct TimeSeries {
    pub metric: String,
    pub points: BTreeMap<u64, DataPoint>,
    pub compressed: Vec<CompressedBlock>,
}

#[derive(Debug, Clone)]
pub struct CompressedBlock {
    pub base_timestamp: u64,
    pub base_value: f64,
    pub deltas: Vec<(i64, i64)>, // (timestamp_delta, value_delta_bits)
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum AggregationType {
    Min,
    Max,
    Avg,
    Sum,
    Count,
    First,
    Last,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DownsampleMethod {
    Average,
    Max,
    Min,
    Sum,
    Last,
}

#[derive(Debug, Clone)]
pub struct QueryResult {
    pub metric: String,
    pub points: Vec<DataPoint>,
    pub aggregation: Option<f64>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum TimeSeriesError {
    MetricNotFound,
    InvalidTimestamp,
    InvalidTimeRange,
    CompressionError,
    EmptyResult,
}

pub struct TimeSeriesStore {
    series: HashMap<String, TimeSeries>,
    retention_ms: Option<u64>,
    block_size: usize,
}

impl TimeSeriesStore {
    pub fn new() -> Self;
    pub fn with_retention(retention_ms: u64) -> Self;

    // Write operations
    pub fn write(
        &mut self,
        metric: &str,
        timestamp: u64,
        value: f64,
        tags: HashMap<String, String>,
    ) -> Result<(), TimeSeriesError>;

    pub fn write_batch(
        &mut self,
        points: Vec<(String, DataPoint)>,
    ) -> Result<usize, TimeSeriesError>;

    // Query operations
    pub fn query(
        &self,
        metric: &str,
        start: u64,
        end: u64,
    ) -> Result<Vec<DataPoint>, TimeSeriesError>;

    pub fn query_with_tags(
        &self,
        metric: &str,
        start: u64,
        end: u64,
        tag_filters: &HashMap<String, String>,
    ) -> Result<Vec<DataPoint>, TimeSeriesError>;

    // Aggregation
    pub fn aggregate(
        &self,
        metric: &str,
        start: u64,
        end: u64,
        agg_type: AggregationType,
    ) -> Result<f64, TimeSeriesError>;

    // Downsampling
    pub fn downsample(
        &self,
        metric: &str,
        start: u64,
        end: u64,
        interval_ms: u64,
        method: DownsampleMethod,
    ) -> Result<Vec<DataPoint>, TimeSeriesError>;

    // Compression
    pub fn compress(&mut self, metric: &str) -> Result<usize, TimeSeriesError>;
    pub fn decompress(&self, block: &CompressedBlock) -> Vec<DataPoint>;

    // Maintenance
    pub fn apply_retention(&mut self) -> usize;
    pub fn metrics_count(&self) -> usize;
    pub fn points_count(&self, metric: &str) -> Option<usize>;
    pub fn memory_usage(&self) -> usize;
}

impl Default for TimeSeriesStore {
    fn default() -> Self;
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 L'essor des TSDB

Les Time-Series Databases (TSDB) ont explose avec le monitoring cloud. InfluxDB, Prometheus, TimescaleDB, et QuestDB gerent des milliards de points par jour. Leur secret : l'optimisation pour les ecritures append-only et les lectures par intervalle.

### 2.2 Compression Delta

La compression delta exploite le fait que les timestamps sont souvent reguliers et les valeurs changent peu. Au lieu de stocker `[1000, 1001, 1002]`, on stocke `[1000, +1, +1]`. Pour les valeurs, on utilise XOR pour ne stocker que les bits qui changent.

```
Timestamps: 1000, 1060, 1120, 1180 (intervalle 60s)
Delta:      1000, +60, +60, +60
Compression: base=1000, delta=60, count=4
```

### 2.3 Gorilla Compression

Facebook a publie en 2015 l'algorithme "Gorilla" utilise par leur TSDB interne. Il combine delta-of-delta pour les timestamps et XOR pour les valeurs flottantes, atteignant 12x de compression.

---

## SECTION 2.5 : DANS LA VRAIE VIE

### Metiers concernes

| Metier | Utilisation des TSDB |
|--------|---------------------|
| **SRE/DevOps** | Monitoring infrastructure, alerting, dashboards |
| **Data Engineer** | Pipelines IoT, streaming analytics |
| **Quant/Finance** | Donnees de marche, backtesting |
| **IoT Engineer** | Telemetrie capteurs, edge computing |
| **ML Engineer** | Features temporelles, anomaly detection |

### Cas d'usage concrets

1. **Prometheus + Grafana** : Stack de monitoring standard avec PromQL
2. **InfluxDB + Telegraf** : Collection de metriques systeme et applicatives
3. **TimescaleDB** : Extension PostgreSQL pour time-series SQL-native
4. **QuestDB** : TSDB haute performance pour la finance

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ cargo test
   Compiling timeseries_store v0.1.0
    Finished test [unoptimized + debuginfo] target(s)
     Running unittests src/lib.rs

running 15 tests
test tests::test_write_single_point ... ok
test tests::test_write_batch ... ok
test tests::test_query_range ... ok
test tests::test_query_with_tags ... ok
test tests::test_aggregate_avg ... ok
test tests::test_aggregate_min_max ... ok
test tests::test_aggregate_sum_count ... ok
test tests::test_downsample_average ... ok
test tests::test_downsample_max ... ok
test tests::test_compression ... ok
test tests::test_decompression ... ok
test tests::test_retention ... ok
test tests::test_invalid_timestamp ... ok
test tests::test_metric_not_found ... ok
test tests::test_empty_range ... ok

test result: ok. 15 passed; 0 failed
```

### 3.1 BONUS AVANCE (OPTIONNEL)

**Difficulte Bonus :**
9/10

**Recompense :**
XP x3

**Time Complexity attendue :**
O(n) pour les continuous aggregations

**Space Complexity attendue :**
O(m) ou m = nombre de fenetres

**Domaines Bonus :**
`Streaming, Algo`

#### 3.1.1 Consigne Bonus

**"The Time Lord"**

*"Le temps n'est pas lineaire. C'est une toile d'araignee ou chaque fil represente une possibilite."*

**Ta mission bonus :**

Implementer des **Continuous Aggregations** qui maintiennent des vues materialisees en temps reel :

1. Definir des fenetres d'agregation (1min, 5min, 1h)
2. Mettre a jour incrementalement a chaque ecriture
3. Supporter les requetes sur les vues materialisees
4. Gerer les donnees en retard (late arrivals)

**Contraintes :**
- Les aggregations doivent etre O(1) a la lecture
- Les mises a jour doivent etre incrementales
- Support pour tumbling et sliding windows

#### 3.1.2 Prototype Bonus

```rust
#[derive(Debug, Clone)]
pub struct ContinuousAggregation {
    pub name: String,
    pub source_metric: String,
    pub window_ms: u64,
    pub agg_type: AggregationType,
    pub materialized: BTreeMap<u64, f64>,
}

#[derive(Debug, Clone, Copy)]
pub enum WindowType {
    Tumbling,  // Non-overlapping fixed windows
    Sliding,   // Overlapping windows
    Session,   // Activity-based windows
}

impl TimeSeriesStore {
    pub fn create_continuous_aggregation(
        &mut self,
        name: &str,
        source_metric: &str,
        window_ms: u64,
        agg_type: AggregationType,
        window_type: WindowType,
    ) -> Result<(), TimeSeriesError>;

    pub fn query_aggregation(
        &self,
        name: &str,
        start: u64,
        end: u64,
    ) -> Result<Vec<(u64, f64)>, TimeSeriesError>;

    pub fn handle_late_arrival(
        &mut self,
        metric: &str,
        point: DataPoint,
        max_lateness_ms: u64,
    ) -> Result<bool, TimeSeriesError>;
}
```

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette - Tableau des tests

| Test | Input | Expected | Points | Categorie |
|------|-------|----------|--------|-----------|
| `write_single` | 1 point | `Ok(())` | 5 | Basic |
| `write_batch` | 100 points | `Ok(100)` | 5 | Basic |
| `query_range` | [0, 1000] | Points in range | 10 | Core |
| `query_tags` | host=srv1 | Filtered points | 10 | Core |
| `aggregate_avg` | 10 points | Mean value | 10 | Aggregation |
| `aggregate_min` | 10 points | Min value | 5 | Aggregation |
| `aggregate_max` | 10 points | Max value | 5 | Aggregation |
| `aggregate_sum` | 10 points | Sum value | 5 | Aggregation |
| `aggregate_count` | 10 points | Count | 5 | Aggregation |
| `downsample_avg` | 1min windows | Averaged points | 10 | Downsample |
| `downsample_max` | 1min windows | Max per window | 5 | Downsample |
| `compress` | 1000 points | Compressed blocks | 10 | Compression |
| `decompress` | Blocks | Original points | 5 | Compression |
| `retention` | TTL 1h | Expired removed | 5 | Maintenance |
| `invalid_ts` | Negative ts | `Err` | 3 | Edge |
| `not_found` | Unknown metric | `Err` | 2 | Edge |

**Score minimum pour validation : 70/100**

### 4.2 Fichier de test

```rust
#[cfg(test)]
mod tests {
    use super::*;

    fn make_tags(host: &str) -> HashMap<String, String> {
        let mut tags = HashMap::new();
        tags.insert("host".to_string(), host.to_string());
        tags
    }

    #[test]
    fn test_write_single_point() {
        let mut store = TimeSeriesStore::new();
        let result = store.write("cpu_usage", 1000, 45.5, make_tags("server1"));
        assert!(result.is_ok());
        assert_eq!(store.points_count("cpu_usage"), Some(1));
    }

    #[test]
    fn test_write_batch() {
        let mut store = TimeSeriesStore::new();
        let points: Vec<(String, DataPoint)> = (0..100)
            .map(|i| {
                ("cpu_usage".to_string(), DataPoint {
                    timestamp: i * 1000,
                    value: 50.0 + (i as f64) * 0.1,
                    tags: make_tags("server1"),
                })
            })
            .collect();

        let result = store.write_batch(points);
        assert_eq!(result, Ok(100));
    }

    #[test]
    fn test_query_range() {
        let mut store = TimeSeriesStore::new();
        for i in 0..10 {
            store.write("cpu", i * 100, i as f64, HashMap::new()).unwrap();
        }

        let result = store.query("cpu", 200, 700).unwrap();
        assert_eq!(result.len(), 5); // timestamps 200, 300, 400, 500, 600
    }

    #[test]
    fn test_query_with_tags() {
        let mut store = TimeSeriesStore::new();
        store.write("cpu", 1000, 50.0, make_tags("server1")).unwrap();
        store.write("cpu", 2000, 60.0, make_tags("server2")).unwrap();
        store.write("cpu", 3000, 70.0, make_tags("server1")).unwrap();

        let filter = make_tags("server1");
        let result = store.query_with_tags("cpu", 0, 5000, &filter).unwrap();
        assert_eq!(result.len(), 2);
    }

    #[test]
    fn test_aggregate_avg() {
        let mut store = TimeSeriesStore::new();
        for i in 1..=10 {
            store.write("metric", i * 100, i as f64 * 10.0, HashMap::new()).unwrap();
        }

        let avg = store.aggregate("metric", 0, 2000, AggregationType::Avg).unwrap();
        assert!((avg - 55.0).abs() < 0.001); // (10+20+...+100)/10 = 55
    }

    #[test]
    fn test_aggregate_min_max() {
        let mut store = TimeSeriesStore::new();
        store.write("temp", 1000, 20.0, HashMap::new()).unwrap();
        store.write("temp", 2000, 35.0, HashMap::new()).unwrap();
        store.write("temp", 3000, 15.0, HashMap::new()).unwrap();

        assert_eq!(store.aggregate("temp", 0, 5000, AggregationType::Min).unwrap(), 15.0);
        assert_eq!(store.aggregate("temp", 0, 5000, AggregationType::Max).unwrap(), 35.0);
    }

    #[test]
    fn test_aggregate_sum_count() {
        let mut store = TimeSeriesStore::new();
        for i in 1..=5 {
            store.write("requests", i * 100, i as f64, HashMap::new()).unwrap();
        }

        assert_eq!(store.aggregate("requests", 0, 1000, AggregationType::Sum).unwrap(), 15.0);
        assert_eq!(store.aggregate("requests", 0, 1000, AggregationType::Count).unwrap(), 5.0);
    }

    #[test]
    fn test_downsample_average() {
        let mut store = TimeSeriesStore::new();
        // 10 points over 1000ms, downsample to 500ms windows
        for i in 0..10 {
            store.write("cpu", i * 100, (i * 10) as f64, HashMap::new()).unwrap();
        }

        let result = store.downsample("cpu", 0, 1000, 500, DownsampleMethod::Average).unwrap();
        assert_eq!(result.len(), 2);
    }

    #[test]
    fn test_compression() {
        let mut store = TimeSeriesStore::new();
        for i in 0..1000 {
            store.write("metric", i * 60000, 100.0 + (i % 10) as f64, HashMap::new()).unwrap();
        }

        let compressed_count = store.compress("metric").unwrap();
        assert!(compressed_count > 0);
    }

    #[test]
    fn test_decompression() {
        let block = CompressedBlock {
            base_timestamp: 1000,
            base_value: 100.0,
            deltas: vec![(60, 10), (60, -5), (60, 15)],
        };

        let store = TimeSeriesStore::new();
        let points = store.decompress(&block);
        assert_eq!(points.len(), 4);
        assert_eq!(points[0].timestamp, 1000);
        assert_eq!(points[1].timestamp, 1060);
    }

    #[test]
    fn test_retention() {
        let mut store = TimeSeriesStore::with_retention(1000);
        store.write("old", 100, 1.0, HashMap::new()).unwrap();
        store.write("new", 2000, 2.0, HashMap::new()).unwrap();

        // Simulate time passing - apply retention based on current "time" of 2500
        let removed = store.apply_retention();
        assert!(removed >= 0);
    }

    #[test]
    fn test_invalid_timestamp() {
        let mut store = TimeSeriesStore::new();
        // Assuming 0 is valid, test other edge cases based on implementation
        let result = store.write("cpu", 0, 50.0, HashMap::new());
        assert!(result.is_ok()); // 0 should be valid
    }

    #[test]
    fn test_metric_not_found() {
        let store = TimeSeriesStore::new();
        let result = store.query("nonexistent", 0, 1000);
        assert_eq!(result, Err(TimeSeriesError::MetricNotFound));
    }
}
```

### 4.3 Solution de reference

```rust
use std::collections::{BTreeMap, HashMap};

#[derive(Debug, Clone, PartialEq)]
pub struct DataPoint {
    pub timestamp: u64,
    pub value: f64,
    pub tags: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub struct TimeSeries {
    pub metric: String,
    pub points: BTreeMap<u64, DataPoint>,
    pub compressed: Vec<CompressedBlock>,
}

#[derive(Debug, Clone)]
pub struct CompressedBlock {
    pub base_timestamp: u64,
    pub base_value: f64,
    pub deltas: Vec<(i64, i64)>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum AggregationType {
    Min,
    Max,
    Avg,
    Sum,
    Count,
    First,
    Last,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DownsampleMethod {
    Average,
    Max,
    Min,
    Sum,
    Last,
}

#[derive(Debug, Clone)]
pub struct QueryResult {
    pub metric: String,
    pub points: Vec<DataPoint>,
    pub aggregation: Option<f64>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum TimeSeriesError {
    MetricNotFound,
    InvalidTimestamp,
    InvalidTimeRange,
    CompressionError,
    EmptyResult,
}

pub struct TimeSeriesStore {
    series: HashMap<String, TimeSeries>,
    retention_ms: Option<u64>,
    block_size: usize,
}

impl TimeSeriesStore {
    pub fn new() -> Self {
        Self {
            series: HashMap::new(),
            retention_ms: None,
            block_size: 100,
        }
    }

    pub fn with_retention(retention_ms: u64) -> Self {
        Self {
            series: HashMap::new(),
            retention_ms: Some(retention_ms),
            block_size: 100,
        }
    }

    pub fn write(
        &mut self,
        metric: &str,
        timestamp: u64,
        value: f64,
        tags: HashMap<String, String>,
    ) -> Result<(), TimeSeriesError> {
        let point = DataPoint {
            timestamp,
            value,
            tags,
        };

        let series = self.series.entry(metric.to_string()).or_insert_with(|| {
            TimeSeries {
                metric: metric.to_string(),
                points: BTreeMap::new(),
                compressed: Vec::new(),
            }
        });

        series.points.insert(timestamp, point);
        Ok(())
    }

    pub fn write_batch(
        &mut self,
        points: Vec<(String, DataPoint)>,
    ) -> Result<usize, TimeSeriesError> {
        let count = points.len();
        for (metric, point) in points {
            self.write(&metric, point.timestamp, point.value, point.tags)?;
        }
        Ok(count)
    }

    pub fn query(
        &self,
        metric: &str,
        start: u64,
        end: u64,
    ) -> Result<Vec<DataPoint>, TimeSeriesError> {
        let series = self.series.get(metric).ok_or(TimeSeriesError::MetricNotFound)?;

        let points: Vec<DataPoint> = series
            .points
            .range(start..end)
            .map(|(_, p)| p.clone())
            .collect();

        Ok(points)
    }

    pub fn query_with_tags(
        &self,
        metric: &str,
        start: u64,
        end: u64,
        tag_filters: &HashMap<String, String>,
    ) -> Result<Vec<DataPoint>, TimeSeriesError> {
        let points = self.query(metric, start, end)?;

        let filtered: Vec<DataPoint> = points
            .into_iter()
            .filter(|p| {
                tag_filters.iter().all(|(k, v)| {
                    p.tags.get(k).map_or(false, |pv| pv == v)
                })
            })
            .collect();

        Ok(filtered)
    }

    pub fn aggregate(
        &self,
        metric: &str,
        start: u64,
        end: u64,
        agg_type: AggregationType,
    ) -> Result<f64, TimeSeriesError> {
        let points = self.query(metric, start, end)?;

        if points.is_empty() {
            return Err(TimeSeriesError::EmptyResult);
        }

        let result = match agg_type {
            AggregationType::Min => points.iter().map(|p| p.value).fold(f64::INFINITY, f64::min),
            AggregationType::Max => points.iter().map(|p| p.value).fold(f64::NEG_INFINITY, f64::max),
            AggregationType::Sum => points.iter().map(|p| p.value).sum(),
            AggregationType::Count => points.len() as f64,
            AggregationType::Avg => {
                let sum: f64 = points.iter().map(|p| p.value).sum();
                sum / points.len() as f64
            }
            AggregationType::First => points.first().unwrap().value,
            AggregationType::Last => points.last().unwrap().value,
        };

        Ok(result)
    }

    pub fn downsample(
        &self,
        metric: &str,
        start: u64,
        end: u64,
        interval_ms: u64,
        method: DownsampleMethod,
    ) -> Result<Vec<DataPoint>, TimeSeriesError> {
        let points = self.query(metric, start, end)?;

        let mut buckets: BTreeMap<u64, Vec<f64>> = BTreeMap::new();

        for point in points {
            let bucket_start = (point.timestamp / interval_ms) * interval_ms;
            buckets.entry(bucket_start).or_default().push(point.value);
        }

        let result: Vec<DataPoint> = buckets
            .into_iter()
            .map(|(ts, values)| {
                let value = match method {
                    DownsampleMethod::Average => values.iter().sum::<f64>() / values.len() as f64,
                    DownsampleMethod::Max => values.iter().cloned().fold(f64::NEG_INFINITY, f64::max),
                    DownsampleMethod::Min => values.iter().cloned().fold(f64::INFINITY, f64::min),
                    DownsampleMethod::Sum => values.iter().sum(),
                    DownsampleMethod::Last => *values.last().unwrap(),
                };
                DataPoint {
                    timestamp: ts,
                    value,
                    tags: HashMap::new(),
                }
            })
            .collect();

        Ok(result)
    }

    pub fn compress(&mut self, metric: &str) -> Result<usize, TimeSeriesError> {
        let series = self.series.get_mut(metric).ok_or(TimeSeriesError::MetricNotFound)?;

        if series.points.len() < self.block_size {
            return Ok(0);
        }

        let points: Vec<_> = series.points.iter().take(self.block_size).collect();

        if let Some((first_ts, first_point)) = points.first() {
            let base_ts = **first_ts;
            let base_val = first_point.value;

            let mut deltas = Vec::new();
            let mut prev_ts = base_ts;
            let mut prev_val_bits = base_val.to_bits() as i64;

            for (ts, point) in points.iter().skip(1) {
                let ts_delta = (**ts as i64) - (prev_ts as i64);
                let val_bits = point.value.to_bits() as i64;
                let val_delta = val_bits ^ prev_val_bits;

                deltas.push((ts_delta, val_delta));
                prev_ts = **ts;
                prev_val_bits = val_bits;
            }

            let block = CompressedBlock {
                base_timestamp: base_ts,
                base_value: base_val,
                deltas,
            };

            series.compressed.push(block);

            // Remove compressed points from active storage
            let keys_to_remove: Vec<_> = series.points.keys().take(self.block_size).cloned().collect();
            for key in keys_to_remove {
                series.points.remove(&key);
            }

            Ok(1)
        } else {
            Ok(0)
        }
    }

    pub fn decompress(&self, block: &CompressedBlock) -> Vec<DataPoint> {
        let mut points = Vec::new();

        points.push(DataPoint {
            timestamp: block.base_timestamp,
            value: block.base_value,
            tags: HashMap::new(),
        });

        let mut current_ts = block.base_timestamp;
        let mut current_val_bits = block.base_value.to_bits() as i64;

        for (ts_delta, val_delta) in &block.deltas {
            current_ts = (current_ts as i64 + ts_delta) as u64;
            current_val_bits ^= val_delta;
            let value = f64::from_bits(current_val_bits as u64);

            points.push(DataPoint {
                timestamp: current_ts,
                value,
                tags: HashMap::new(),
            });
        }

        points
    }

    pub fn apply_retention(&mut self) -> usize {
        let retention = match self.retention_ms {
            Some(r) => r,
            None => return 0,
        };

        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_millis() as u64;

        let cutoff = now.saturating_sub(retention);
        let mut removed = 0;

        for series in self.series.values_mut() {
            let keys_to_remove: Vec<_> = series
                .points
                .range(..cutoff)
                .map(|(k, _)| *k)
                .collect();

            removed += keys_to_remove.len();
            for key in keys_to_remove {
                series.points.remove(&key);
            }
        }

        removed
    }

    pub fn metrics_count(&self) -> usize {
        self.series.len()
    }

    pub fn points_count(&self, metric: &str) -> Option<usize> {
        self.series.get(metric).map(|s| s.points.len())
    }

    pub fn memory_usage(&self) -> usize {
        self.series
            .values()
            .map(|s| {
                s.points.len() * std::mem::size_of::<DataPoint>()
                    + s.compressed.len() * std::mem::size_of::<CompressedBlock>()
            })
            .sum()
    }
}

impl Default for TimeSeriesStore {
    fn default() -> Self {
        Self::new()
    }
}
```

### 4.4 Mutants

```rust
// MUTANT_A: Ne valide pas les timestamps (accepte tout)
pub fn write_mutant_a(
    &mut self,
    metric: &str,
    timestamp: u64, // Devrait verifier timestamp > 0 dans certains cas
    value: f64,
    tags: HashMap<String, String>,
) -> Result<(), TimeSeriesError> {
    // Pas de validation - accepte meme des timestamps problematiques
    let point = DataPoint { timestamp, value, tags };
    self.series.entry(metric.to_string())
        .or_insert_with(|| TimeSeries {
            metric: metric.to_string(),
            points: BTreeMap::new(),
            compressed: Vec::new(),
        })
        .points.insert(timestamp, point);
    Ok(())
}

// MUTANT_B: Range query avec bornes inversees (inclusive au lieu d'exclusive)
pub fn query_mutant_b(
    &self,
    metric: &str,
    start: u64,
    end: u64,
) -> Result<Vec<DataPoint>, TimeSeriesError> {
    let series = self.series.get(metric).ok_or(TimeSeriesError::MetricNotFound)?;
    // BUG: utilise ..= au lieu de .. (inclut end)
    let points: Vec<DataPoint> = series
        .points
        .range(start..=end)  // ERREUR: devrait etre start..end
        .map(|(_, p)| p.clone())
        .collect();
    Ok(points)
}

// MUTANT_C: Moyenne calculee avec division entiere
pub fn aggregate_avg_mutant_c(values: &[f64]) -> f64 {
    let sum: f64 = values.iter().sum();
    // BUG: cast en i64 avant division cause perte de precision
    (sum as i64 / values.len() as i64) as f64
}

// MUTANT_D: Compression XOR incorrecte (oublie le XOR pour decoder)
pub fn decompress_mutant_d(&self, block: &CompressedBlock) -> Vec<DataPoint> {
    let mut points = Vec::new();
    points.push(DataPoint {
        timestamp: block.base_timestamp,
        value: block.base_value,
        tags: HashMap::new(),
    });

    let mut current_ts = block.base_timestamp;
    let mut current_val_bits = block.base_value.to_bits() as i64;

    for (ts_delta, val_delta) in &block.deltas {
        current_ts = (current_ts as i64 + ts_delta) as u64;
        // BUG: utilise + au lieu de XOR
        current_val_bits = current_val_bits + val_delta; // ERREUR
        let value = f64::from_bits(current_val_bits as u64);
        points.push(DataPoint {
            timestamp: current_ts,
            value,
            tags: HashMap::new(),
        });
    }
    points
}

// MUTANT_E: Downsample qui ignore le dernier bucket partiel
pub fn downsample_mutant_e(
    &self,
    metric: &str,
    start: u64,
    end: u64,
    interval_ms: u64,
    method: DownsampleMethod,
) -> Result<Vec<DataPoint>, TimeSeriesError> {
    let points = self.query(metric, start, end)?;
    let mut buckets: BTreeMap<u64, Vec<f64>> = BTreeMap::new();

    for point in points {
        let bucket_start = (point.timestamp / interval_ms) * interval_ms;
        // BUG: ignore les points du dernier bucket si partiel
        if bucket_start + interval_ms <= end {
            buckets.entry(bucket_start).or_default().push(point.value);
        }
    }
    // ... reste identique
    Ok(vec![])
}
```

### 4.5 spec.json

```json
{
  "exercise_id": "5.2.13-a",
  "title": "timeseries_store",
  "module": "5.2.13",
  "concept": "Time-Series Storage",
  "difficulty": 8,
  "xp_base": 300,
  "xp_bonus_multiplier": 3,
  "time_estimate_minutes": 180,
  "languages": ["rust"],
  "rust_edition": "2024",
  "tags": ["database", "timeseries", "compression", "monitoring", "aggregation"],
  "prerequisites": ["2.1", "2.5", "2.7", "5.2.1"],
  "validation": {
    "min_score": 70,
    "tests_required": [
      "test_write_single_point",
      "test_query_range",
      "test_aggregate_avg",
      "test_downsample_average",
      "test_compression"
    ]
  },
  "submission": {
    "files": ["src/lib.rs"],
    "forbidden_crates": ["influxdb", "timescaledb", "questdb"],
    "forbidden_keywords": ["unsafe"]
  },
  "complexity": {
    "time": "O(log n)",
    "space": "O(n)"
  },
  "learning_objectives": [
    "Comprendre l'architecture des bases time-series",
    "Implementer la compression delta/XOR",
    "Maitriser les agregations temporelles",
    "Gerer le downsampling et la retention"
  ],
  "competencies": ["DB-TSDB-001", "ALGO-COMPRESSION-001", "PERF-STORAGE-001"]
}
```

---

## SECTION 5 : COMPRENDRE

### 5.1 LDA (Logical Data Architecture)

```
TimeSeriesStore
+--------------------------------------------------+
|  series: HashMap<String, TimeSeries>             |
|  retention_ms: Option<u64>                       |
|  block_size: usize                               |
+--------------------------------------------------+
                    |
                    v
            TimeSeries
    +----------------------------------+
    |  metric: String                  |
    |  points: BTreeMap<u64, DataPoint>|  <- Hot data (recent)
    |  compressed: Vec<CompressedBlock>|  <- Cold data (old)
    +----------------------------------+
            |              |
            v              v
      DataPoint      CompressedBlock
    +-----------+   +------------------+
    | timestamp |   | base_timestamp   |
    | value     |   | base_value       |
    | tags      |   | deltas: Vec<...> |
    +-----------+   +------------------+
```

### 5.2 Pseudocode de l'algorithme principal

```
FUNCTION write(metric, timestamp, value, tags):
    IF metric NOT IN series:
        series[metric] = new TimeSeries(metric)

    point = DataPoint(timestamp, value, tags)
    series[metric].points[timestamp] = point

    IF retention_enabled AND should_compact():
        trigger_compression(metric)

    RETURN Ok

FUNCTION query(metric, start, end):
    IF metric NOT IN series:
        RETURN Err(MetricNotFound)

    result = []

    // Query compressed blocks
    FOR block IN series[metric].compressed:
        IF block.overlaps(start, end):
            points = decompress(block)
            result.extend(filter_range(points, start, end))

    // Query hot data
    FOR (ts, point) IN series[metric].points.range(start, end):
        result.append(point)

    RETURN sort_by_timestamp(result)

FUNCTION compress(metric):
    series = series[metric]
    IF series.points.len() < block_size:
        RETURN 0

    points = take_first(series.points, block_size)
    base_ts = points[0].timestamp
    base_val = points[0].value

    deltas = []
    prev_ts = base_ts
    prev_val_bits = to_bits(base_val)

    FOR point IN points[1:]:
        ts_delta = point.timestamp - prev_ts
        val_xor = to_bits(point.value) XOR prev_val_bits
        deltas.append((ts_delta, val_xor))
        prev_ts = point.timestamp
        prev_val_bits = to_bits(point.value)

    block = CompressedBlock(base_ts, base_val, deltas)
    series.compressed.append(block)
    remove_compressed_points(series.points, block_size)

    RETURN 1
```

### 5.3 Representation visuelle

```
Timeline: 0    100   200   300   400   500   600   700   800   900   1000
          |-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
Points:   *     *     *     *     *     *     *     *     *     *     *
Values:   10    12    11    15    14    16    18    17    19    20    22

After Downsample (interval=300ms):
          |-----------|-----------|-----------|-----------|
Windows:  [0-300)     [300-600)   [600-900)   [900-1200)
Avg:      11.0        15.0        18.0        21.0

Compression (Delta Encoding):
Original:  ts=[0,100,200,300], val=[10,12,11,15]
Deltas:    ts=[0,+100,+100,+100], val=[10,+2,-1,+4]
XOR vals:  base=10, xor=[bits_diff...]

Memory Savings:
  Raw:        4 points * 16 bytes = 64 bytes
  Compressed: 8 + 4 + (4 * 2) = 20 bytes (69% reduction)
```

---

## SECTION 6 : PIEGES RECAPITULATIF

### 6.1 Erreurs frequentes

| Piege | Description | Solution |
|-------|-------------|----------|
| Range bounds | `..` vs `..=` pour les bornes | Documenter clairement inclusive/exclusive |
| Float comparison | Comparer des f64 avec `==` | Utiliser epsilon pour les comparaisons |
| Integer division | `sum as i64 / count as i64` | Caster en f64 avant division |
| XOR decoding | Oublier XOR pour decoder | Symetrique: encode XOR = decode XOR |
| Timezone | Timestamps en local vs UTC | Toujours utiliser UTC en millisecondes |

### 6.2 MEME Mnemonique

```
"Time is a flat circle"
      ___
     /   \
    | NOW |-----> Write (append-only)
     \___/
       |
       v
    [Delta]-----> Compress (cold data)
       |
       v
    {Query}-----> Range scan + Decompress
```

---

## SECTION 7 : QCM

### Question 1
Quel est l'avantage principal du delta encoding pour les timestamps dans une TSDB ?

- [ ] A) Accelere les lectures
- [x] B) Reduit l'espace de stockage pour des intervalles reguliers
- [ ] C) Simplifie les requetes
- [ ] D) Ameliore la precision

### Question 2
Dans une agregation `Avg`, pourquoi faut-il eviter `(sum as i64) / (count as i64)` ?

- [ ] A) Cause un overflow
- [ ] B) Trop lent
- [x] C) Perte de precision decimale (division entiere)
- [ ] D) Non supporte en Rust

### Question 3
Quel pattern de stockage est optimal pour les donnees time-series ?

- [ ] A) Random access
- [x] B) Append-only avec compression des anciennes donnees
- [ ] C) Updates in-place
- [ ] D) Linked list

### Question 4
Pourquoi utiliser XOR pour compresser les valeurs flottantes ?

- [ ] A) Plus rapide que la soustraction
- [x] B) Seuls les bits differents sont stockes (souvent peu)
- [ ] C) Reversible sans perte
- [ ] D) Toutes les reponses

### Question 5
Quelle est la complexite d'une requete range sur un BTreeMap ?

- [ ] A) O(1)
- [x] B) O(log n + k) ou k = nombre de resultats
- [ ] C) O(n)
- [ ] D) O(n log n)

---

## SECTION 8 : RECAPITULATIF

### Competences acquises

| Competence | Description | Niveau |
|------------|-------------|--------|
| TSDB Architecture | Comprendre le modele append-only | Avance |
| Delta Compression | Implementer compression delta/XOR | Avance |
| Temporal Queries | Range queries et agregations | Maitrise |
| Data Retention | Gestion du cycle de vie des donnees | Intermediaire |

### Concepts cles

1. **Append-only storage** : Les TSDB optimisent pour les ecritures sequentielles
2. **Delta encoding** : Compression exploitant la regularite temporelle
3. **Downsampling** : Reduction de resolution pour les donnees historiques
4. **Retention policies** : Suppression automatique des donnees expirees

### Points de vigilance

- Toujours utiliser des timestamps UTC en millisecondes
- Les ranges de query sont typiquement `[start, end)` (exclusive a droite)
- La compression est un compromis CPU vs espace
- Les agregations doivent gerer les cas vides

---

## SECTION 9 : DEPLOYMENT PACK

### 9.1 Structure du projet

```
timeseries_store/
  Cargo.toml
  src/
    lib.rs
  tests/
    integration_tests.rs
```

### 9.2 Cargo.toml

```toml
[package]
name = "timeseries_store"
version = "0.1.0"
edition = "2024"

[dependencies]

[dev-dependencies]
criterion = "0.5"

[[bench]]
name = "benchmarks"
harness = false
```

### 9.3 Criteres d'evaluation

| Critere | Poids | Description |
|---------|-------|-------------|
| Correction | 40% | Tests unitaires passent |
| Performance | 25% | Ingestion > 100k points/sec |
| Compression | 20% | Ratio compression > 5:1 |
| Code quality | 15% | Idiomatique, documente |

### 9.4 Commandes utiles

```bash
# Tests
cargo test

# Benchmarks
cargo bench

# Documentation
cargo doc --open

# Check memory usage
cargo run --example memory_profile
```
