# Exercice 2.4.5-synth : avengers_parallel_lib

**Module :**
2.4.5 â€” Patterns de Programmation Parallele Avances

**Concepts :**
a â€” Parallel for (distribution d'iterations)
d â€” Parallel prefix (scan operation)
g â€” Data decomposition (partitionnement)
h â€” Critical sections (exclusion mutuelle)
c â€” Future/Promise (resultats asynchrones)

**Difficulte :**
â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜†â˜† (8/10)

**Type :**
complet

**Tiers :**
3 â€” Synthese (tous concepts aâ†’c)

**Langage :**
Rust Edition 2024

**Prerequis :**
- 2.4.1 Threads et spawn basique
- 2.4.2 Arc/Mutex et synchronisation
- 2.4.3 Channels et message passing
- 2.4.4 Atomics et memory ordering

**Domaines :**
Struct, Algo, CPU, Process

**Duree estimee :**
180 min

**XP Base :**
500

**Complexite :**
T8 O(n/p) Ã— S6 O(n)

---

## SECTION 1 : PROTOTYPE & CONSIGNE

### 1.1 Obligations

**Fichier a rendre :** `src/lib.rs`

**Fonctions autorisees :**
- `std::thread::*`
- `std::sync::{Arc, Mutex, Barrier, Condvar, atomic::*}`
- `std::collections::VecDeque`

**Fonctions interdites :**
- `rayon::*` (bibliotheque externe)
- `crossbeam::*` (bibliotheque externe)
- `tokio::*` (runtime async externe)
- `unsafe` blocs (sauf pour les atomics)

---

### 1.2 Consigne

#### 2.4.1 AVENGERS ASSEMBLE â€” L'Analogie Heroique

**L'INVASION CHITAURI : COMPRENDRE LE PARALLELISME**

Imagine la Bataille de New York dans Avengers (2012). Des milliers d'aliens Chitauri envahissent Manhattan. Nick Fury a une decision cruciale : comment deployer les Avengers ?

```
                    â˜ï¸ PORTAIL CHITAURI â˜ï¸
                         â•‘â•‘â•‘â•‘â•‘â•‘â•‘
                    â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
              [1000 ENNEMIS A NEUTRALISER]
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
    â•”â•â•â•â•â•â•â•â•â•â•—    â•”â•â•â•â•â•â•â•â•â•â•—    â•”â•â•â•â•â•â•â•â•â•â•—
    â•‘ IRON    â•‘    â•‘  THOR   â•‘    â•‘ CAPTAIN â•‘
    â•‘  MAN    â•‘    â•‘         â•‘    â•‘ AMERICA â•‘
    â•šâ•â•â•â•â•¤â•â•â•â•â•    â•šâ•â•â•â•â•¤â•â•â•â•â•    â•šâ•â•â•â•â•¤â•â•â•â•â•
         â”‚              â”‚              â”‚
    [Zone Est]    [Zone Centre]  [Zone Ouest]
    250 kills      250 kills      250 kills
         â”‚              â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
              â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
              â•‘    NICK FURY      â•‘
              â•‘  (LE REDUCER)     â•‘
              â•‘ Total: 750 kills  â•‘
              â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**MAPREDUCE = OPERATION AVENGERS**

```rust
// Chaque heros combat SEPAREMENT dans sa zone
fn hero_combat(zone: &[Chitauri]) -> KillCount {
    zone.iter().filter(|c| c.neutralize()).count()  // MAP local
}

// Nick Fury COMBINE tous les rapports
fn fury_debrief(reports: Vec<KillCount>) -> TotalVictory {
    reports.iter().sum()  // REDUCE global
}
```

**Pourquoi ca marche ?**
- Chaque Avenger est INDEPENDANT (pas besoin de se coordonner pendant le combat)
- Les zones sont DISJOINTES (pas de collision de territoire)
- Le combine est ASSOCIATIF (750 = 250 + 250 + 250, peu importe l'ordre)

---

**WORK STEALING = LE PROTOCOLE HAWKEYE-IRON MAN**

Probleme : Hawkeye a fini ses 100 cibles, mais Hulk est encore submerge par 500 ennemis.

```
    AVANT WORK STEALING:

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   HAWKEYE    â•‘      â•‘    HULK      â•‘
    â•‘   [IDLE]     â•‘      â•‘ [OVERWHELMED]â•‘
    â•‘   0 left     â•‘      â•‘  500 left    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           â”‚                     â”‚
           â”‚    "Hey Hulk,      â”‚
           â”‚    je prends 200   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                                 â”‚
    APRES WORK STEALING:        â–¼

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   HAWKEYE    â•‘      â•‘    HULK      â•‘
    â•‘ [WORKING]    â•‘      â•‘  [WORKING]   â•‘
    â•‘  200 left    â•‘      â•‘  300 left    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

```rust
// La file de travail de chaque heros
struct HeroWorkQueue {
    tasks: VecDeque<Chitauri>,  // Double-ended queue
}

impl HeroWorkQueue {
    // Hawkeye VOLE par l'arriere (moins de contention)
    fn steal(&self) -> Option<Chitauri> {
        self.tasks.pop_back()  // Vol depuis l'autre bout!
    }

    // Hulk travaille par l'avant
    fn work(&mut self) -> Option<Chitauri> {
        self.tasks.pop_front()  // Travail normal
    }
}
```

**Pourquoi voler par l'arriere ?**
- Hulk pop_front, Hawkeye pop_back = PAS DE COLLISION
- C'est le secret des work-stealing schedulers (Cilk, Rayon, Go)

---

**PARALLEL SCAN = L'ARMURE MARK 50 NANO-ASSEMBLY**

Iron Man assemble son armure piece par piece, mais chaque piece depend des precedentes :

```
    Nano-particules:  [1]  [2]  [3]  [4]  [5]
                       â”‚    â”‚    â”‚    â”‚    â”‚
    Assemblage:       [1] [1+2][1+2+3][...]
                       â–¼    â–¼    â–¼    â–¼    â–¼
    Resultat:         [1]  [3]  [6] [10] [15]
```

**Phase 1 : UP-SWEEP (Reduction par paires)**
```
    Niveau 0:  1    2    3    4    5    6    7    8
               â””â”€â”¬â”€â”˜    â””â”€â”¬â”€â”˜    â””â”€â”¬â”€â”˜    â””â”€â”¬â”€â”˜
    Niveau 1:    3        7       11       15
                 â””â”€â”€â”€â”¬â”€â”€â”€â”˜        â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    Niveau 2:        10               26
                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    Niveau 3:                36 (total)
```

**Phase 2 : DOWN-SWEEP (Distribution des prefixes)**
```
    Niveau 3:                0 (identite)
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    Niveau 2:        0               10
                 â”Œâ”€â”€â”€â”´â”€â”€â”€â”       â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    Niveau 1:    0       3       10      21
               â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”
    Niveau 0:  0   1   3   6   10  15  21  28
```

---

**FUTURE/PROMISE = LE CONTRAT AVENGERS-SHIELD**

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   NICK FURY     â”‚          â”‚    IRON MAN     â”‚
    â”‚   (Consumer)    â”‚          â”‚   (Producer)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
             â”‚  "Tony, j'ai besoin du     â”‚
             â”‚   scan du portail"         â”‚
             â”‚                            â”‚
             â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
             â””â”€â”€â–ºâ”‚   PROMISE    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚  (Le contrat)â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    FUTURE    â”‚
                 â”‚ (Le resultat â”‚
                 â”‚   a venir)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚              â”‚
         â–¼              â–¼              â–¼
    [Pending]      [Ready(data)]  [Failed(err)]
```

```rust
// Fury cree le contrat
let (promise, future) = Promise::<ScanResult>::new();

// Tony travaille en arriere-plan
thread::spawn(move || {
    let scan = scan_portal();  // Operation longue
    promise.set_value(scan);   // Tient sa promesse
});

// Fury peut faire autre chose, puis attendre
let result = future.get();  // Bloque jusqu'a Ready
```

---

#### 2.4.2 Enonce Academique

**BIBLIOTHEQUE DE PATTERNS DE PROGRAMMATION PARALLELE**

Implementer une bibliotheque complete de patterns de programmation parallele comprenant :

1. **parallel_for** : Distribution d'iterations sur N threads avec scheduling configurable (Static, Dynamic/Work-Stealing, Guided)

2. **parallel_map** : Transformation parallele d'un vecteur avec preservation de l'ordre

3. **parallel_reduce** : Agregation parallele avec element neutre et fonction associative

4. **parallel_scan** : Algorithme de Blelloch pour le prefix parallel (inclusive et exclusive)

5. **Future/Promise** : Primitives de synchronisation pour resultats asynchrones

6. **AsyncExecutor** : Pool de threads avec soumission de taches et work-stealing

**Contraintes mathematiques :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  parallel_for:   T(n,p) = O(n/p) avec p threads             â”‚
â”‚  parallel_map:   T(n,p) = O(n/p), S(n) = O(n)              â”‚
â”‚  parallel_reduce: T(n,p) = O(n/p + log(p))                 â”‚
â”‚  parallel_scan:  T(n,p) = O(n/p + log(n)), Work = O(n)     â”‚
â”‚  work_stealing:  Expected T = O(T_1/p + T_inf)             â”‚
â”‚                  ou T_1 = travail total, T_inf = chemin    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ta mission :**

Implementer les fonctions suivantes :

1. `parallel_for(start, end, config, f)` â€” Execute f(i) pour i in [start, end) en parallele
2. `parallel_map(input, config, f)` â€” Retourne Vec ou chaque element est f(input[i])
3. `parallel_reduce(input, identity, config, combine)` â€” Reduit avec associativite
4. `parallel_scan(input, identity, scan_type, config, combine)` â€” Prefix sum parallele
5. `Promise::new()` et `Future::get()` â€” Synchronisation asynchrone
6. `AsyncExecutor::new(n)` et `spawn(task)` â€” Pool de threads

**Entrees :**
- `start, end: usize` â€” Bornes de l'iteration
- `config: &ParallelConfig` â€” Configuration (num_threads, chunk_size, schedule)
- `f: Fn` â€” Fonction a appliquer (doit etre Sync + Send)
- `identity: T` â€” Element neutre pour reduce/scan
- `combine: Fn(T, T) -> T` â€” Fonction de combinaison associative

**Sorties :**
- `parallel_map` retourne `Vec<U>` de meme taille que l'entree
- `parallel_reduce` retourne `T` le resultat agrege
- `parallel_scan` retourne `Vec<T>` des prefixes
- `Future::get()` retourne `Result<T, String>`

**Contraintes :**
- Thread-safety obligatoire (pas de data races)
- Gestion des edge cases (vecteur vide, single element)
- Work-stealing pour Schedule::Dynamic
- Algorithme de Blelloch work-efficient pour scan

**Exemples :**

| Appel | Retour | Explication |
|-------|--------|-------------|
| `parallel_reduce(&[1,2,3,4], 0, cfg, \|a,b\| a+b)` | `10` | 1+2+3+4 = 10 |
| `parallel_scan(&[1,2,3], 0, Inclusive, cfg, \|a,b\| a+b)` | `[1,3,6]` | Prefixes cumulatifs |
| `parallel_scan(&[1,2,3], 0, Exclusive, cfg, \|a,b\| a+b)` | `[0,1,3]` | Sans element courant |
| `parallel_map(&[1,2,3], cfg, \|x\| x*2)` | `[2,4,6]` | Transformation |

---

### 1.3 Prototype

```rust
// Configuration
pub struct ParallelConfig {
    pub num_threads: usize,
    pub chunk_size: Option<usize>,
    pub schedule: Schedule,
}

#[derive(Clone, Copy)]
pub enum Schedule {
    Static,   // Division egale pre-calculee
    Dynamic,  // Work stealing
    Guided,   // Chunks decroissants
}

#[derive(Clone, Copy)]
pub enum ScanType {
    Inclusive,  // result[i] = sum(input[0..=i])
    Exclusive,  // result[i] = sum(input[0..i])
}

// Core functions
pub fn parallel_for<F>(start: usize, end: usize, config: &ParallelConfig, f: F)
where
    F: Fn(usize) + Sync + Send;

pub fn parallel_map<T, U, F>(input: &[T], config: &ParallelConfig, f: F) -> Vec<U>
where
    T: Sync,
    U: Send + Default + Clone,
    F: Fn(&T) -> U + Sync + Send;

pub fn parallel_reduce<T, C>(
    input: &[T],
    identity: T,
    config: &ParallelConfig,
    combine: C,
) -> T
where
    T: Clone + Send + Sync,
    C: Fn(T, T) -> T + Sync + Send;

pub fn parallel_scan<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    config: &ParallelConfig,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send;

// Future/Promise
pub struct Promise<T> { /* ... */ }
pub struct Future<T> { /* ... */ }

impl<T> Promise<T> {
    pub fn new() -> (Promise<T>, Future<T>);
    pub fn set_value(self, value: T);
    pub fn set_error(self, error: String);
}

impl<T: Clone> Future<T> {
    pub fn get(&self) -> Result<T, String>;
    pub fn is_ready(&self) -> bool;
}

// Async Executor
pub struct AsyncExecutor { /* ... */ }

impl AsyncExecutor {
    pub fn new(num_threads: usize) -> Self;
    pub fn spawn<F, T>(&self, task: F) -> Future<T>
    where
        F: FnOnce() -> T + Send + 'static,
        T: Send + Clone + 'static;
    pub fn shutdown(&self);
}
```

---

## SECTION 2 : LE SAVIEZ-VOUS ?

### 2.1 Origine du Parallel Scan

L'algorithme de parallel prefix (scan) a ete invente par **Guy Blelloch** a Carnegie Mellon en 1990. Son insight genial : on peut calculer TOUS les prefixes en O(n) de travail total et O(log n) de profondeur parallele.

**Fun fact :** Le parallel scan est utilise dans les GPU modernes pour :
- Le tri par radix (les milliards de pixels de vos jeux)
- La compression stream (Netflix, YouTube)
- L'allocation memoire GPU

### 2.2 Work Stealing : L'Histoire de Cilk

Le work stealing a ete invente au MIT par **Charles Leiserson** pour le langage Cilk (1994). La beaute : avec P processeurs, un programme Cilk tourne en temps T_1/P + O(T_inf) en esperance, ou T_1 est le travail sequentiel et T_inf le chemin critique.

**Anecdote :** Intel a rachete Cilk Arts et l'a integre dans leurs compilateurs. Le scheduler de Go (goroutines) utilise aussi du work stealing.

---

## SECTION 2.5 : DANS LA VRAIE VIE

| Metier | Utilisation | Exemple Concret |
|--------|-------------|-----------------|
| **Game Developer** | parallel_for pour le rendu | Calcul de lighting sur 1M de vertices en 16ms |
| **Data Engineer** | MapReduce distribue | Spark jobs sur 10TB de logs |
| **Quant Developer** | parallel_reduce pour pricing | Monte Carlo sur 1M de paths |
| **ML Engineer** | parallel_scan pour attention | Transformer self-attention O(n^2) |
| **Systems Programmer** | Work stealing scheduler | Runtime Go, Tokio, Rayon |

---

## SECTION 3 : EXEMPLE D'UTILISATION

### 3.0 Session bash

```bash
$ ls
src/  Cargo.toml  tests/

$ cargo test
running 12 tests
test test_parallel_for ... ok
test test_parallel_for_empty ... ok
test test_parallel_map ... ok
test test_parallel_reduce ... ok
test test_parallel_reduce_empty ... ok
test test_parallel_scan_inclusive ... ok
test test_parallel_scan_exclusive ... ok
test test_future_promise ... ok
test test_future_error ... ok
test test_async_executor ... ok
test test_speedup ... ok
test test_chunk_range ... ok

test result: ok. 12 passed; 0 failed

$ cargo bench
parallel_map/1M         time: [12.3 ms 12.5 ms 12.7 ms]
sequential_map/1M       time: [89.2 ms 90.1 ms 91.0 ms]
Speedup: 7.2x on 8 cores
```

---

### 3.1 BONUS EXPERT (OPTIONNEL)

**Difficulte Bonus :**
ğŸ’€ (12/10)

**Recompense :**
XP x4

**Time Complexity attendue :**
O(n/p + log(n)) pour scan

**Space Complexity attendue :**
O(n) pour scan, O(1) auxiliaire pour reduce

**Domaines Bonus :**
`CPU, Algo, DP`

#### 3.1.1 Consigne Bonus

**LE PROJET ULTRON : WORK-EFFICIENT PARALLEL SCAN**

Tony Stark doit implementer l'algorithme de Blelloch pour assembler les nano-particules de son armure en temps optimal. L'algorithme naif fait O(n log n) de travail, mais Blelloch fait O(n).

**Ta mission :**

Implementer `parallel_scan_blelloch` qui :
1. Fait exactement O(2n - 2) operations (work-optimal)
2. A une profondeur de O(2 log n) (span-optimal)
3. Supporte les deux modes (Inclusive et Exclusive)

**Contraintes :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Work total: W(n) = 2n - 2 - log(n) operations exactement   â”‚
â”‚  Span (profondeur): S(n) = 2 * log(n)                       â”‚
â”‚  Parallelisme: W/S = O(n / log(n))                          â”‚
â”‚  Memoire auxiliaire: O(1) (in-place sur copie)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemples :**

| Input | Identity | Type | Output |
|-------|----------|------|--------|
| `[3,1,7,0,4,1,6,3]` | `0` | Exclusive | `[0,3,4,11,11,15,16,22]` |
| `[3,1,7,0,4,1,6,3]` | `0` | Inclusive | `[3,4,11,11,15,16,22,25]` |

#### 3.1.2 Prototype Bonus

```rust
pub fn parallel_scan_blelloch<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send;
```

#### 3.1.3 Ce qui change par rapport a l'exercice de base

| Aspect | Base | Bonus |
|--------|------|-------|
| Algorithme | Simple (potentiellement O(n log n) work) | Blelloch work-efficient |
| Work | O(n log n) acceptable | Exactement O(2n - 2) |
| Implementation | Libre | Up-sweep + Down-sweep obligatoires |
| In-place | Non requis | Sur copie du vecteur |

---

## SECTION 4 : ZONE CORRECTION

### 4.1 Moulinette

| Test | Description | Points | Trap |
|------|-------------|--------|------|
| `parallel_for_basic` | 1000 iterations comptees | 10 | Non |
| `parallel_for_empty` | Range vide = 0 iterations | 5 | Oui |
| `parallel_map_order` | Ordre preserve | 15 | Oui |
| `parallel_map_types` | Transformation de types | 10 | Non |
| `parallel_reduce_sum` | Somme de 1..1000 | 15 | Non |
| `parallel_reduce_empty` | Retourne identity | 5 | Oui |
| `parallel_scan_inclusive` | [1,2,3] -> [1,3,6] | 15 | Non |
| `parallel_scan_exclusive` | [1,2,3] -> [0,1,3] | 15 | Non |
| `future_promise_value` | Set + Get value | 10 | Non |
| `future_promise_error` | Set + Get error | 10 | Oui |
| `executor_spawn` | Spawn + wait | 15 | Non |
| `executor_multiple` | 100 taches paralleles | 15 | Non |
| `no_data_race` | ThreadSanitizer clean | 20 | Oui |
| `speedup_test` | Speedup > 2x sur 4 cores | 10 | Non |

**Score minimum :** 70/150

---

### 4.2 main.rs de test

```rust
use parallel_lib::*;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::Instant;

fn main() {
    println!("=== Tests ParallelLib ===\n");

    // Test parallel_for
    let counter = AtomicUsize::new(0);
    let config = ParallelConfig::default();

    parallel_for(0, 1000, &config, |_| {
        counter.fetch_add(1, Ordering::Relaxed);
    });

    let count = counter.load(Ordering::Relaxed);
    println!("parallel_for(0, 1000): {} iterations", count);
    assert_eq!(count, 1000, "FAIL: Expected 1000 iterations");
    println!("[OK] parallel_for\n");

    // Test parallel_map
    let input: Vec<i32> = (0..100).collect();
    let output = parallel_map(&input, &config, |x| x * 2);

    let correct = output.iter().enumerate().all(|(i, &v)| v == (i as i32) * 2);
    println!("parallel_map: {} elements, order preserved: {}", output.len(), correct);
    assert!(correct, "FAIL: Order not preserved or wrong values");
    println!("[OK] parallel_map\n");

    // Test parallel_reduce
    let input: Vec<i32> = (1..=1000).collect();
    let sum = parallel_reduce(&input, 0, &config, |a, b| a + b);

    println!("parallel_reduce(1..=1000): {}", sum);
    assert_eq!(sum, 500500, "FAIL: Expected 500500 (n*(n+1)/2)");
    println!("[OK] parallel_reduce\n");

    // Test parallel_scan inclusive
    let input = vec![1, 2, 3, 4, 5];
    let scan = parallel_scan(&input, 0, ScanType::Inclusive, &config, |a, b| a + b);

    println!("parallel_scan inclusive [1,2,3,4,5]: {:?}", scan);
    assert_eq!(scan, vec![1, 3, 6, 10, 15], "FAIL: Wrong inclusive scan");
    println!("[OK] parallel_scan inclusive\n");

    // Test parallel_scan exclusive
    let scan_ex = parallel_scan(&input, 0, ScanType::Exclusive, &config, |a, b| a + b);

    println!("parallel_scan exclusive [1,2,3,4,5]: {:?}", scan_ex);
    assert_eq!(scan_ex, vec![0, 1, 3, 6, 10], "FAIL: Wrong exclusive scan");
    println!("[OK] parallel_scan exclusive\n");

    // Test Future/Promise
    let (promise, future) = Promise::<i32>::new();

    std::thread::spawn(move || {
        std::thread::sleep(std::time::Duration::from_millis(10));
        promise.set_value(42);
    });

    let result = future.get().unwrap();
    println!("Future/Promise: got {}", result);
    assert_eq!(result, 42, "FAIL: Expected 42");
    println!("[OK] Future/Promise\n");

    // Test AsyncExecutor
    let executor = AsyncExecutor::new(4);

    let futures: Vec<_> = (0..100)
        .map(|i| executor.spawn(move || i * 2))
        .collect();

    let results: Vec<_> = futures.iter()
        .map(|f| f.get().unwrap())
        .collect();

    let correct_exec = results.iter().enumerate().all(|(i, &v)| v == i * 2);
    println!("AsyncExecutor: {} tasks completed correctly: {}", results.len(), correct_exec);
    assert!(correct_exec, "FAIL: Executor returned wrong values");

    executor.shutdown();
    println!("[OK] AsyncExecutor\n");

    // Speedup test
    let large_input: Vec<f64> = (0..1_000_000).map(|x| x as f64).collect();

    let start = Instant::now();
    let _: Vec<f64> = large_input.iter().map(|x| x.sqrt()).collect();
    let seq_time = start.elapsed();

    let start = Instant::now();
    let _ = parallel_map(&large_input, &config, |x| x.sqrt());
    let par_time = start.elapsed();

    let speedup = seq_time.as_secs_f64() / par_time.as_secs_f64();
    println!("Sequential: {:?}", seq_time);
    println!("Parallel:   {:?}", par_time);
    println!("Speedup:    {:.2}x", speedup);

    println!("\n=== All tests passed! ===");
}
```

---

### 4.3 Solution de reference

```rust
use std::sync::{Arc, Mutex, Condvar, Barrier};
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::thread;
use std::collections::VecDeque;

// ============================================================
// Configuration
// ============================================================

pub struct ParallelConfig {
    pub num_threads: usize,
    pub chunk_size: Option<usize>,
    pub schedule: Schedule,
}

#[derive(Clone, Copy, PartialEq)]
pub enum Schedule {
    Static,
    Dynamic,
    Guided,
}

impl Default for ParallelConfig {
    fn default() -> Self {
        ParallelConfig {
            num_threads: thread::available_parallelism()
                .map(|p| p.get())
                .unwrap_or(4),
            chunk_size: None,
            schedule: Schedule::Static,
        }
    }
}

#[derive(Clone, Copy, PartialEq)]
pub enum ScanType {
    Inclusive,
    Exclusive,
}

// ============================================================
// Parallel For
// ============================================================

pub fn parallel_for<F>(start: usize, end: usize, config: &ParallelConfig, f: F)
where
    F: Fn(usize) + Sync + Send,
{
    if start >= end {
        return;
    }

    let range_size = end - start;
    let num_threads = config.num_threads.min(range_size);
    let chunk_size = config.chunk_size
        .unwrap_or((range_size + num_threads - 1) / num_threads);

    match config.schedule {
        Schedule::Static => {
            thread::scope(|s| {
                for t in 0..num_threads {
                    let f = &f;
                    s.spawn(move || {
                        let thread_start = start + t * chunk_size;
                        let thread_end = (thread_start + chunk_size).min(end);
                        for i in thread_start..thread_end {
                            f(i);
                        }
                    });
                }
            });
        }
        Schedule::Dynamic => {
            let next_index = Arc::new(AtomicUsize::new(start));
            let chunk = chunk_size.max(1);

            thread::scope(|s| {
                for _ in 0..num_threads {
                    let next = Arc::clone(&next_index);
                    let f = &f;
                    s.spawn(move || {
                        loop {
                            let idx = next.fetch_add(chunk, Ordering::Relaxed);
                            if idx >= end {
                                break;
                            }
                            let local_end = (idx + chunk).min(end);
                            for i in idx..local_end {
                                f(i);
                            }
                        }
                    });
                }
            });
        }
        Schedule::Guided => {
            let remaining = Arc::new(AtomicUsize::new(range_size));
            let next_start = Arc::new(AtomicUsize::new(start));

            thread::scope(|s| {
                for _ in 0..num_threads {
                    let rem = Arc::clone(&remaining);
                    let next = Arc::clone(&next_start);
                    let f = &f;
                    s.spawn(move || {
                        loop {
                            let r = rem.load(Ordering::Acquire);
                            if r == 0 {
                                break;
                            }
                            let chunk = (r / num_threads).max(1);
                            let actual_chunk = rem.fetch_sub(chunk.min(r), Ordering::AcqRel)
                                .min(chunk);
                            if actual_chunk == 0 {
                                break;
                            }
                            let idx = next.fetch_add(actual_chunk, Ordering::Relaxed);
                            let local_end = (idx + actual_chunk).min(end);
                            for i in idx..local_end {
                                f(i);
                            }
                        }
                    });
                }
            });
        }
    }
}

// ============================================================
// Parallel Map
// ============================================================

pub fn parallel_map<T, U, F>(input: &[T], config: &ParallelConfig, f: F) -> Vec<U>
where
    T: Sync,
    U: Send + Default + Clone,
    F: Fn(&T) -> U + Sync + Send,
{
    if input.is_empty() {
        return vec![];
    }

    let mut output: Vec<U> = vec![U::default(); input.len()];
    let num_threads = config.num_threads.min(input.len());
    let chunk_size = (input.len() + num_threads - 1) / num_threads;

    thread::scope(|s| {
        let chunks: Vec<_> = output.chunks_mut(chunk_size).collect();

        for (thread_id, chunk) in chunks.into_iter().enumerate() {
            let start = thread_id * chunk_size;
            let f = &f;
            let input = &input;

            s.spawn(move || {
                for (i, out) in chunk.iter_mut().enumerate() {
                    *out = f(&input[start + i]);
                }
            });
        }
    });

    output
}

// ============================================================
// Parallel Reduce
// ============================================================

pub fn parallel_reduce<T, C>(
    input: &[T],
    identity: T,
    config: &ParallelConfig,
    combine: C,
) -> T
where
    T: Clone + Send + Sync,
    C: Fn(T, T) -> T + Sync + Send,
{
    if input.is_empty() {
        return identity;
    }

    let num_threads = config.num_threads.min(input.len());
    let chunk_size = (input.len() + num_threads - 1) / num_threads;

    let partial_results: Vec<T> = thread::scope(|s| {
        let handles: Vec<_> = (0..num_threads)
            .filter_map(|t| {
                let start = t * chunk_size;
                if start >= input.len() {
                    return None;
                }
                let end = (start + chunk_size).min(input.len());
                let chunk = &input[start..end];
                let id = identity.clone();
                let combine = &combine;

                Some(s.spawn(move || {
                    chunk.iter().cloned().fold(id, |acc, x| combine(acc, x))
                }))
            })
            .collect();

        handles.into_iter()
            .map(|h| h.join().unwrap())
            .collect()
    });

    partial_results.into_iter()
        .fold(identity, |acc, x| combine(acc, x))
}

// ============================================================
// Parallel Scan (Blelloch-style)
// ============================================================

pub fn parallel_scan<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    config: &ParallelConfig,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send,
{
    if input.is_empty() {
        return vec![];
    }

    if input.len() == 1 {
        return match scan_type {
            ScanType::Inclusive => vec![input[0].clone()],
            ScanType::Exclusive => vec![identity],
        };
    }

    let n = input.len();
    let num_threads = config.num_threads.min(n);
    let chunk_size = (n + num_threads - 1) / num_threads;

    // Phase 1: Local scans within each chunk
    let local_scans: Vec<Vec<T>> = thread::scope(|s| {
        let handles: Vec<_> = (0..num_threads)
            .filter_map(|t| {
                let start = t * chunk_size;
                if start >= n {
                    return None;
                }
                let end = (start + chunk_size).min(n);
                let chunk = &input[start..end];
                let id = identity.clone();
                let combine = &combine;

                Some(s.spawn(move || {
                    let mut scan = Vec::with_capacity(chunk.len());
                    let mut acc = id;
                    for item in chunk {
                        acc = combine(&acc, item);
                        scan.push(acc.clone());
                    }
                    scan
                }))
            })
            .collect();

        handles.into_iter()
            .map(|h| h.join().unwrap())
            .collect()
    });

    // Phase 2: Compute chunk totals (last element of each local scan)
    let chunk_totals: Vec<T> = local_scans.iter()
        .map(|scan| scan.last().unwrap().clone())
        .collect();

    // Phase 3: Sequential prefix of totals (small, ok to be sequential)
    let mut total_prefixes = vec![identity.clone()];
    let mut acc = identity.clone();
    for total in &chunk_totals[..chunk_totals.len().saturating_sub(1)] {
        acc = combine(&acc, total);
        total_prefixes.push(acc.clone());
    }

    // Phase 4: Add chunk prefix to each local scan
    let mut result = Vec::with_capacity(n);

    for (chunk_idx, local_scan) in local_scans.into_iter().enumerate() {
        let prefix = &total_prefixes[chunk_idx];
        for (i, val) in local_scan.into_iter().enumerate() {
            let adjusted = if i == 0 && chunk_idx == 0 {
                val
            } else {
                combine(prefix, &val)
            };
            result.push(adjusted);
        }
    }

    // Adjust for scan type
    match scan_type {
        ScanType::Inclusive => result,
        ScanType::Exclusive => {
            let mut exclusive = vec![identity];
            exclusive.extend(result.into_iter().take(n - 1));
            exclusive
        }
    }
}

// ============================================================
// Future / Promise
// ============================================================

pub enum FutureState<T> {
    Pending,
    Ready(T),
    Failed(String),
}

pub struct Promise<T> {
    state: Arc<(Mutex<FutureState<T>>, Condvar)>,
}

pub struct Future<T> {
    state: Arc<(Mutex<FutureState<T>>, Condvar)>,
}

impl<T> Promise<T> {
    pub fn new() -> (Promise<T>, Future<T>) {
        let state = Arc::new((Mutex::new(FutureState::Pending), Condvar::new()));
        (
            Promise { state: Arc::clone(&state) },
            Future { state },
        )
    }

    pub fn set_value(self, value: T) {
        let (lock, cvar) = &*self.state;
        let mut state = lock.lock().unwrap();
        *state = FutureState::Ready(value);
        cvar.notify_all();
    }

    pub fn set_error(self, error: String) {
        let (lock, cvar) = &*self.state;
        let mut state = lock.lock().unwrap();
        *state = FutureState::Failed(error);
        cvar.notify_all();
    }
}

impl<T: Clone> Future<T> {
    pub fn get(&self) -> Result<T, String> {
        let (lock, cvar) = &*self.state;
        let mut state = lock.lock().unwrap();

        while matches!(*state, FutureState::Pending) {
            state = cvar.wait(state).unwrap();
        }

        match &*state {
            FutureState::Ready(v) => Ok(v.clone()),
            FutureState::Failed(e) => Err(e.clone()),
            FutureState::Pending => unreachable!(),
        }
    }

    pub fn is_ready(&self) -> bool {
        let (lock, _) = &*self.state;
        let state = lock.lock().unwrap();
        !matches!(*state, FutureState::Pending)
    }
}

// ============================================================
// Async Executor
// ============================================================

pub struct AsyncExecutor {
    task_queue: Arc<Mutex<VecDeque<Box<dyn FnOnce() + Send>>>>,
    condvar: Arc<Condvar>,
    shutdown: Arc<AtomicBool>,
    workers: Mutex<Vec<thread::JoinHandle<()>>>,
}

impl AsyncExecutor {
    pub fn new(num_threads: usize) -> Self {
        let task_queue = Arc::new(Mutex::new(VecDeque::new()));
        let condvar = Arc::new(Condvar::new());
        let shutdown = Arc::new(AtomicBool::new(false));
        let mut workers = Vec::with_capacity(num_threads);

        for _ in 0..num_threads {
            let queue = Arc::clone(&task_queue);
            let cvar = Arc::clone(&condvar);
            let shut = Arc::clone(&shutdown);

            workers.push(thread::spawn(move || {
                loop {
                    let task = {
                        let mut queue = queue.lock().unwrap();
                        while queue.is_empty() && !shut.load(Ordering::Relaxed) {
                            queue = cvar.wait(queue).unwrap();
                        }
                        if shut.load(Ordering::Relaxed) && queue.is_empty() {
                            break;
                        }
                        queue.pop_front()
                    };

                    if let Some(task) = task {
                        task();
                    }
                }
            }));
        }

        AsyncExecutor {
            task_queue,
            condvar,
            shutdown,
            workers: Mutex::new(workers),
        }
    }

    pub fn spawn<F, T>(&self, task: F) -> Future<T>
    where
        F: FnOnce() -> T + Send + 'static,
        T: Send + Clone + 'static,
    {
        let (promise, future) = Promise::new();

        let boxed_task: Box<dyn FnOnce() + Send> = Box::new(move || {
            let result = task();
            promise.set_value(result);
        });

        {
            let mut queue = self.task_queue.lock().unwrap();
            queue.push_back(boxed_task);
        }
        self.condvar.notify_one();

        future
    }

    pub fn shutdown(&self) {
        self.shutdown.store(true, Ordering::SeqCst);
        self.condvar.notify_all();

        // Wait for workers to finish
        let mut workers = self.workers.lock().unwrap();
        for worker in workers.drain(..) {
            let _ = worker.join();
        }
    }
}

// ============================================================
// Utilities
// ============================================================

pub fn num_cpus() -> usize {
    thread::available_parallelism()
        .map(|p| p.get())
        .unwrap_or(1)
}

pub fn chunk_range(start: usize, end: usize, num_chunks: usize) -> Vec<(usize, usize)> {
    if start >= end || num_chunks == 0 {
        return vec![];
    }

    let total = end - start;
    let chunk_size = (total + num_chunks - 1) / num_chunks;

    (0..num_chunks)
        .map(|i| {
            let chunk_start = start + i * chunk_size;
            let chunk_end = (chunk_start + chunk_size).min(end);
            (chunk_start, chunk_end)
        })
        .filter(|(s, e)| s < e)
        .collect()
}
```

---

### 4.4 Solutions alternatives acceptees

```rust
// Alternative 1: parallel_reduce avec Barrier au lieu de join
pub fn parallel_reduce_barrier<T, C>(
    input: &[T],
    identity: T,
    config: &ParallelConfig,
    combine: C,
) -> T
where
    T: Clone + Send + Sync,
    C: Fn(T, T) -> T + Sync + Send,
{
    if input.is_empty() {
        return identity;
    }

    let num_threads = config.num_threads.min(input.len());
    let chunk_size = (input.len() + num_threads - 1) / num_threads;
    let results = Arc::new(Mutex::new(vec![identity.clone(); num_threads]));
    let barrier = Arc::new(Barrier::new(num_threads));

    thread::scope(|s| {
        for t in 0..num_threads {
            let results = Arc::clone(&results);
            let barrier = Arc::clone(&barrier);
            let combine = &combine;
            let id = identity.clone();

            s.spawn(move || {
                let start = t * chunk_size;
                if start >= input.len() {
                    barrier.wait();
                    return;
                }
                let end = (start + chunk_size).min(input.len());

                let local = input[start..end]
                    .iter()
                    .cloned()
                    .fold(id, |acc, x| combine(acc, x));

                results.lock().unwrap()[t] = local;
                barrier.wait();
            });
        }
    });

    let results = Arc::try_unwrap(results).unwrap().into_inner().unwrap();
    results.into_iter().fold(identity, |acc, x| combine(acc, x))
}

// Alternative 2: parallel_scan iteratif simple
pub fn parallel_scan_simple<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    combine: F,
) -> Vec<T>
where
    T: Clone,
    F: Fn(&T, &T) -> T,
{
    if input.is_empty() {
        return vec![];
    }

    let mut result = Vec::with_capacity(input.len());
    let mut acc = identity.clone();

    match scan_type {
        ScanType::Inclusive => {
            for item in input {
                acc = combine(&acc, item);
                result.push(acc.clone());
            }
        }
        ScanType::Exclusive => {
            for item in input {
                result.push(acc.clone());
                acc = combine(&acc, item);
            }
        }
    }

    result
}
```

---

### 4.5 Solutions refusees (avec explications)

```rust
// REFUSE 1: Data race - pas de synchronisation
pub fn parallel_for_unsafe<F>(start: usize, end: usize, f: F)
where
    F: Fn(usize) + Send,  // Manque Sync!
{
    // DANGER: f n'est pas Sync, data race possible
    thread::scope(|s| {
        for i in start..end {
            s.spawn(|| f(i));  // ERROR: Closure captures f by move each time
        }
    });
}
// Pourquoi c'est refuse: La fonction f doit etre &f (reference partagee)
// et donc avoir le trait Sync pour etre utilisee par plusieurs threads

// REFUSE 2: Deadlock potentiel dans Future
pub fn future_get_wrong<T: Clone>(future: &Future<T>) -> Result<T, String> {
    let (lock, cvar) = &*future.state;
    let state = lock.lock().unwrap();  // Lock acquis

    // DEADLOCK: Si un autre thread veut set_value, il ne peut pas
    // car on tient le lock indefiniment
    loop {
        match &*state {
            FutureState::Ready(v) => return Ok(v.clone()),
            FutureState::Failed(e) => return Err(e.clone()),
            FutureState::Pending => std::thread::yield_now(),  // Busy wait!
        }
    }
}
// Pourquoi c'est refuse: Busy waiting + potentiel deadlock
// Il faut utiliser cvar.wait() pour liberer le lock en attendant

// REFUSE 3: parallel_reduce non-associatif
pub fn parallel_reduce_wrong<T, C>(input: &[T], identity: T, combine: C) -> T
where
    T: Clone + Send,
    C: Fn(T, T) -> T + Send,
{
    // Probleme: On assume que combine est associatif mais on ne peut pas
    // garantir l'ordre de combinaison, donc pour des operations
    // non-associatives (comme la soustraction), ca donne des resultats faux

    // Par exemple: reduce([1,2,3,4], 0, |a,b| a-b)
    // Sequentiel: ((((0-1)-2)-3)-4) = -10
    // Parallele: (0-1) combined with (2-3) = -1 combined with -1 = 0 FAUX!

    input.iter().cloned().fold(identity, |a, b| combine(a, b))
}
// Pourquoi c'est refuse: Ne documente pas que combine DOIT etre associatif
```

---

### 4.6 Solution bonus de reference (COMPLETE)

```rust
/// Blelloch work-efficient parallel scan
/// Work: O(2n - 2), Span: O(2 log n)
pub fn parallel_scan_blelloch<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send,
{
    if input.is_empty() {
        return vec![];
    }

    let n = input.len();

    // Pad to power of 2
    let padded_len = n.next_power_of_two();
    let mut data: Vec<T> = input.iter().cloned().collect();
    data.resize(padded_len, identity.clone());

    let levels = (padded_len as f64).log2() as usize;

    // ============================================================
    // Phase 1: Up-sweep (Reduce phase)
    // ============================================================
    // At each level d, combine pairs at distance 2^d
    for d in 0..levels {
        let stride = 1 << (d + 1);  // 2, 4, 8, ...
        let num_ops = padded_len / stride;

        thread::scope(|s| {
            // Split data into mutable chunks for parallel access
            let data_ptr = data.as_mut_ptr();

            for i in 0..num_ops {
                let left_idx = (i + 1) * stride - 1 - (1 << d);
                let right_idx = (i + 1) * stride - 1;
                let combine = &combine;

                // Safety: indices are disjoint at each level
                s.spawn(move || {
                    unsafe {
                        let left = &*data_ptr.add(left_idx);
                        let right = &mut *data_ptr.add(right_idx);
                        *right = combine(left, right);
                    }
                });
            }
        });
    }

    // ============================================================
    // Phase 2: Down-sweep (Distribution phase)
    // ============================================================
    // Set root to identity
    data[padded_len - 1] = identity.clone();

    // Traverse back down
    for d in (0..levels).rev() {
        let stride = 1 << (d + 1);
        let num_ops = padded_len / stride;

        thread::scope(|s| {
            let data_ptr = data.as_mut_ptr();

            for i in 0..num_ops {
                let left_idx = (i + 1) * stride - 1 - (1 << d);
                let right_idx = (i + 1) * stride - 1;
                let combine = &combine;

                s.spawn(move || {
                    unsafe {
                        let left = &*data_ptr.add(left_idx);
                        let right = &*data_ptr.add(right_idx);
                        let new_left = right.clone();
                        let new_right = combine(left, right);
                        *data_ptr.add(left_idx) = new_left;
                        *data_ptr.add(right_idx) = new_right;
                    }
                });
            }
        });
    }

    // Truncate back to original size
    data.truncate(n);

    // Adjust for scan type
    match scan_type {
        ScanType::Exclusive => data,
        ScanType::Inclusive => {
            // Exclusive scan + input = inclusive scan
            data.iter()
                .zip(input.iter())
                .map(|(exc, inp)| combine(exc, inp))
                .collect()
        }
    }
}
```

---

### 4.7 Solutions alternatives bonus (COMPLETES)

```rust
// Alternative bonus: Version recursive de Blelloch
pub fn parallel_scan_recursive<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send + Clone,
{
    fn up_sweep<T, F>(data: &mut [T], combine: &F)
    where
        T: Clone + Send + Sync,
        F: Fn(&T, &T) -> T + Sync + Send,
    {
        let n = data.len();
        if n <= 1 {
            return;
        }

        // Pairwise combine
        let half = n / 2;
        thread::scope(|s| {
            let (left, right) = data.split_at_mut(half);
            s.spawn(|| up_sweep(left, combine));
            s.spawn(|| up_sweep(right, combine));
        });

        // Combine at this level
        data[n - 1] = combine(&data[half - 1], &data[n - 1]);
    }

    fn down_sweep<T, F>(data: &mut [T], identity: &T, combine: &F)
    where
        T: Clone + Send + Sync,
        F: Fn(&T, &T) -> T + Sync + Send,
    {
        let n = data.len();
        if n <= 1 {
            if n == 1 {
                data[0] = identity.clone();
            }
            return;
        }

        let half = n / 2;
        let temp = data[n - 1].clone();
        data[n - 1] = combine(&data[half - 1], &data[n - 1]);
        data[half - 1] = temp;

        thread::scope(|s| {
            let (left, right) = data.split_at_mut(half);
            s.spawn(|| down_sweep(left, identity, combine));
            s.spawn(|| down_sweep(right, &left[half - 1], combine));
        });
    }

    if input.is_empty() {
        return vec![];
    }

    let n = input.len();
    let padded = n.next_power_of_two();
    let mut data: Vec<T> = input.iter().cloned().collect();
    data.resize(padded, identity.clone());

    up_sweep(&mut data, &combine);
    data[padded - 1] = identity.clone();
    down_sweep(&mut data, &identity, &combine);

    data.truncate(n);

    match scan_type {
        ScanType::Exclusive => data,
        ScanType::Inclusive => {
            data.iter()
                .zip(input.iter())
                .map(|(exc, inp)| combine(exc, inp))
                .collect()
        }
    }
}
```

---

### 4.8 Solutions refusees bonus (COMPLETES)

```rust
// REFUSE BONUS 1: O(n log n) work - pas work-efficient
pub fn parallel_scan_naive<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send,
{
    // Algorithme de Hillis-Steele: simple mais O(n log n) work
    let n = input.len();
    let mut current: Vec<T> = input.to_vec();
    let mut next: Vec<T> = vec![identity.clone(); n];

    let levels = (n as f64).log2().ceil() as usize;

    for d in 0..levels {
        let stride = 1 << d;

        thread::scope(|s| {
            for i in 0..n {
                let combine = &combine;
                let current_ref = &current;
                let result_ptr = next.as_mut_ptr();

                s.spawn(move || {
                    let val = if i >= stride {
                        combine(&current_ref[i - stride], &current_ref[i])
                    } else {
                        current_ref[i].clone()
                    };
                    unsafe {
                        *result_ptr.add(i) = val;
                    }
                });
            }
        });

        std::mem::swap(&mut current, &mut next);
    }

    // Ce code fait O(n log n) operations au lieu de O(n)
    // C'est 4x plus lent pour n=1M
    current
}
// Pourquoi c'est refuse: Le bonus demande explicitement O(2n-2) work
// Cet algorithme fait O(n log n) work, ce qui est inacceptable

// REFUSE BONUS 2: Race condition dans down-sweep
pub fn parallel_scan_race<T, F>(
    input: &[T],
    identity: T,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send,
{
    let mut data = input.to_vec();
    let n = data.len();

    // Down-sweep avec race condition
    for d in (0..((n as f64).log2() as usize)).rev() {
        let stride = 1 << (d + 1);

        thread::scope(|s| {
            for i in (stride - 1..n).step_by(stride) {
                let left_idx = i - (1 << d);
                // RACE: On lit et ecrit aux memes indices en parallele!
                let data_ptr = data.as_mut_ptr();
                let combine = &combine;

                s.spawn(move || {
                    unsafe {
                        // Thread 1 lit left pendant que Thread 2 ecrit left
                        // = UNDEFINED BEHAVIOR
                        let temp = (*data_ptr.add(left_idx)).clone();
                        *data_ptr.add(left_idx) = (*data_ptr.add(i)).clone();
                        *data_ptr.add(i) = combine(&temp, &*data_ptr.add(i));
                    }
                });
            }
        });
    }

    data
}
// Pourquoi c'est refuse: Race condition evidente dans le down-sweep
// Les indices left_idx peuvent etre lus/ecrits par plusieurs threads
```

---

### 4.9 spec.json

```json
{
  "name": "avengers_parallel_lib",
  "language": "rust",
  "rust_edition": "2024",
  "type": "complet",
  "tier": 3,
  "tier_info": "Synthese (tous concepts a->c)",
  "tags": ["parallel", "concurrency", "threads", "futures", "phase2", "advanced"],
  "passing_score": 70,

  "functions": [
    {
      "name": "parallel_for",
      "prototype": "pub fn parallel_for<F>(start: usize, end: usize, config: &ParallelConfig, f: F) where F: Fn(usize) + Sync + Send",
      "return_type": "()",
      "parameters": [
        {"name": "start", "type": "usize"},
        {"name": "end", "type": "usize"},
        {"name": "config", "type": "&ParallelConfig"},
        {"name": "f", "type": "F"}
      ]
    },
    {
      "name": "parallel_map",
      "prototype": "pub fn parallel_map<T, U, F>(input: &[T], config: &ParallelConfig, f: F) -> Vec<U>",
      "return_type": "Vec<U>",
      "parameters": [
        {"name": "input", "type": "&[T]"},
        {"name": "config", "type": "&ParallelConfig"},
        {"name": "f", "type": "F"}
      ]
    },
    {
      "name": "parallel_reduce",
      "prototype": "pub fn parallel_reduce<T, C>(input: &[T], identity: T, config: &ParallelConfig, combine: C) -> T",
      "return_type": "T",
      "parameters": [
        {"name": "input", "type": "&[T]"},
        {"name": "identity", "type": "T"},
        {"name": "config", "type": "&ParallelConfig"},
        {"name": "combine", "type": "C"}
      ]
    },
    {
      "name": "parallel_scan",
      "prototype": "pub fn parallel_scan<T, F>(input: &[T], identity: T, scan_type: ScanType, config: &ParallelConfig, combine: F) -> Vec<T>",
      "return_type": "Vec<T>",
      "parameters": [
        {"name": "input", "type": "&[T]"},
        {"name": "identity", "type": "T"},
        {"name": "scan_type", "type": "ScanType"},
        {"name": "config", "type": "&ParallelConfig"},
        {"name": "combine", "type": "F"}
      ]
    }
  ],

  "driver": {
    "reference": "// See section 4.3 for full implementation",
    "reference_file": "references/ref_solution.rs",

    "edge_cases": [
      {
        "name": "parallel_for_empty",
        "description": "Empty range should execute zero times",
        "test_code": "let counter = AtomicUsize::new(0); parallel_for(5, 5, &config, |_| { counter.fetch_add(1, Ordering::Relaxed); }); assert_eq!(counter.load(Ordering::Relaxed), 0);",
        "is_trap": true,
        "trap_explanation": "start >= end means zero iterations"
      },
      {
        "name": "parallel_reduce_empty",
        "description": "Empty input returns identity",
        "test_code": "let empty: Vec<i32> = vec![]; let result = parallel_reduce(&empty, 42, &config, |a, b| a + b); assert_eq!(result, 42);",
        "is_trap": true,
        "trap_explanation": "Empty reduce must return identity element"
      },
      {
        "name": "parallel_scan_single",
        "description": "Single element scan",
        "test_code": "let result = parallel_scan(&[5], 0, ScanType::Inclusive, &config, |a, b| a + b); assert_eq!(result, vec![5]);",
        "is_trap": true,
        "trap_explanation": "Single element inclusive scan is just the element"
      },
      {
        "name": "parallel_scan_exclusive_single",
        "description": "Single element exclusive scan",
        "test_code": "let result = parallel_scan(&[5], 0, ScanType::Exclusive, &config, |a, b| a + b); assert_eq!(result, vec![0]);",
        "is_trap": true,
        "trap_explanation": "Single element exclusive scan is identity"
      },
      {
        "name": "future_error_handling",
        "description": "Future should propagate errors",
        "test_code": "let (promise, future) = Promise::<i32>::new(); promise.set_error(\"test error\".to_string()); assert!(future.get().is_err());",
        "is_trap": true,
        "trap_explanation": "Errors must be propagated through Future"
      },
      {
        "name": "parallel_map_order",
        "description": "Map must preserve order",
        "test_code": "let input: Vec<i32> = (0..1000).collect(); let output = parallel_map(&input, &config, |x| x * 2); assert!(output.iter().enumerate().all(|(i, &v)| v == (i as i32) * 2));",
        "is_trap": true,
        "trap_explanation": "Parallel map MUST preserve element order"
      },
      {
        "name": "parallel_reduce_associative",
        "description": "Reduce with associative operation",
        "test_code": "let input: Vec<i32> = (1..=1000).collect(); let sum = parallel_reduce(&input, 0, &config, |a, b| a + b); assert_eq!(sum, 500500);",
        "is_trap": false
      },
      {
        "name": "executor_concurrent",
        "description": "Executor handles concurrent tasks",
        "test_code": "let executor = AsyncExecutor::new(4); let futures: Vec<_> = (0..100).map(|i| executor.spawn(move || i * 2)).collect(); let results: Vec<_> = futures.iter().map(|f| f.get().unwrap()).collect(); assert!(results.iter().enumerate().all(|(i, &v)| v == i * 2)); executor.shutdown();",
        "is_trap": false
      }
    ],

    "fuzzing": {
      "enabled": true,
      "iterations": 500,
      "generators": [
        {
          "type": "array_int",
          "param_index": 0,
          "params": {
            "min_len": 0,
            "max_len": 10000,
            "min_val": -1000000,
            "max_val": 1000000
          }
        }
      ]
    }
  },

  "norm": {
    "allowed_crates": ["std"],
    "forbidden_crates": ["rayon", "crossbeam", "tokio", "async-std"],
    "check_thread_safety": true,
    "check_memory": true,
    "run_miri": true,
    "blocking": true
  },

  "bonus": {
    "name": "parallel_scan_blelloch",
    "tier": "EXPERT",
    "multiplier": 4,
    "constraints": {
      "work_complexity": "O(2n - 2)",
      "span_complexity": "O(2 log n)",
      "algorithm": "Blelloch up-sweep/down-sweep"
    }
  }
}
```

---

### 4.10 Solutions Mutantes (minimum 5)

```rust
/* Mutant A (Boundary) : Off-by-one dans parallel_for */
pub fn parallel_for_mutant_a<F>(start: usize, end: usize, config: &ParallelConfig, f: F)
where
    F: Fn(usize) + Sync + Send,
{
    if start > end {  // BUG: > au lieu de >=
        return;
    }

    let range_size = end - start;
    let num_threads = config.num_threads.min(range_size);
    let chunk_size = config.chunk_size
        .unwrap_or((range_size + num_threads - 1) / num_threads);

    thread::scope(|s| {
        for t in 0..num_threads {
            let f = &f;
            s.spawn(move || {
                let thread_start = start + t * chunk_size;
                let thread_end = (thread_start + chunk_size).min(end);
                for i in thread_start..=thread_end {  // BUG: ..= au lieu de ..
                    f(i);
                }
            });
        }
    });
}
// Pourquoi c'est faux: Deux bugs - la condition start > end laisse passer
// start == end (devrait etre 0 iterations), et ..= inclut end qui est hors range
// Ce qui etait pense: Confusion sur les bornes inclusives/exclusives

/* Mutant B (Safety) : Pas de verification empty dans reduce */
pub fn parallel_reduce_mutant_b<T, C>(
    input: &[T],
    identity: T,
    config: &ParallelConfig,
    combine: C,
) -> T
where
    T: Clone + Send + Sync,
    C: Fn(T, T) -> T + Sync + Send,
{
    // BUG: Pas de verification input.is_empty()
    // Si input est vide, chunk_size = 0/num_threads = 0, division par zero potentielle

    let num_threads = config.num_threads.min(input.len());  // 0 si vide
    let chunk_size = (input.len() + num_threads - 1) / num_threads;  // PANIC!

    let partial_results: Vec<T> = thread::scope(|s| {
        let handles: Vec<_> = (0..num_threads)
            .filter_map(|t| {
                let start = t * chunk_size;
                if start >= input.len() {
                    return None;
                }
                let end = (start + chunk_size).min(input.len());
                let chunk = &input[start..end];
                let id = identity.clone();
                let combine = &combine;

                Some(s.spawn(move || {
                    chunk.iter().cloned().fold(id, |acc, x| combine(acc, x))
                }))
            })
            .collect();

        handles.into_iter()
            .map(|h| h.join().unwrap())
            .collect()
    });

    partial_results.into_iter()
        .fold(identity, |acc, x| combine(acc, x))
}
// Pourquoi c'est faux: Division par zero quand input est vide
// Ce qui etait pense: "input.len() sera 0 donc ca marchera" - non!

/* Mutant C (Resource) : Oubli de notify workers dans shutdown */
pub struct AsyncExecutorMutantC {
    task_queue: Arc<Mutex<VecDeque<Box<dyn FnOnce() + Send>>>>,
    condvar: Arc<Condvar>,
    shutdown: Arc<AtomicBool>,
    workers: Vec<thread::JoinHandle<()>>,
}

impl AsyncExecutorMutantC {
    pub fn shutdown(&mut self) {
        self.shutdown.store(true, Ordering::SeqCst);
        // BUG: Oubli de self.condvar.notify_all();
        // Les workers sont bloques sur cvar.wait() pour toujours!

        for worker in self.workers.drain(..) {
            let _ = worker.join();  // DEADLOCK: Ca bloque indefiniment
        }
    }
}
// Pourquoi c'est faux: Les worker threads sont bloques dans cvar.wait()
// Sans notify_all(), ils ne se reveillent jamais et join() bloque
// Ce qui etait pense: "shutdown flag suffit" - non, il faut reveiller les threads

/* Mutant D (Logic) : Mauvais calcul des prefixes dans scan */
pub fn parallel_scan_mutant_d<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    config: &ParallelConfig,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send,
{
    if input.is_empty() {
        return vec![];
    }

    let n = input.len();
    let num_threads = config.num_threads.min(n);
    let chunk_size = (n + num_threads - 1) / num_threads;

    // Phase 1: Local scans (correct)
    let local_scans: Vec<Vec<T>> = thread::scope(|s| {
        // ... meme code que reference ...
        vec![vec![]]  // Placeholder
    });

    // Phase 2: Chunk totals (correct)
    let chunk_totals: Vec<T> = local_scans.iter()
        .map(|scan| scan.last().unwrap().clone())
        .collect();

    // Phase 3: BUG - Mauvais calcul des prefixes
    let mut total_prefixes = vec![identity.clone()];
    for i in 1..chunk_totals.len() {
        // BUG: On utilise chunk_totals[i] au lieu de chunk_totals[i-1]
        let prefix = combine(&total_prefixes[i-1], &chunk_totals[i]);
        total_prefixes.push(prefix);
    }
    // Resultat: Les prefixes sont decales d'un chunk!

    // Phase 4: ... reste du code ...
    vec![]  // Placeholder
}
// Pourquoi c'est faux: Le prefix du chunk i devrait etre la somme des chunks 0..i-1
// Mais on calcule la somme des chunks 1..i, donc tout est decale
// Ce qui etait pense: "Le prefix est la somme des precedents" - oui mais pas celui-ci!

/* Mutant E (Return) : Future.get() retourne toujours Ok meme sur erreur */
impl<T: Clone> Future<T> {
    pub fn get_mutant_e(&self) -> Result<T, String> {
        let (lock, cvar) = &*self.state;
        let mut state = lock.lock().unwrap();

        while matches!(*state, FutureState::Pending) {
            state = cvar.wait(state).unwrap();
        }

        match &*state {
            FutureState::Ready(v) => Ok(v.clone()),
            FutureState::Failed(e) => {
                // BUG: On print l'erreur mais on retourne Ok avec valeur par defaut
                eprintln!("Warning: Future failed with {}", e);
                Ok(Default::default())  // BUG! Devrait etre Err(e.clone())
            }
            FutureState::Pending => unreachable!(),
        }
    }
}
// Pourquoi c'est faux: Les erreurs sont silencieusement ignorees
// Le code appelant pense que tout va bien alors que ca a echoue
// Ce qui etait pense: "Log l'erreur et continue" - non, il faut propager!
```

---

## SECTION 5 : COMPRENDRE

### 5.1 Ce que cet exercice enseigne

1. **Data Parallelism vs Task Parallelism** : La difference entre distribuer des donnees (parallel_map) et distribuer des taches (AsyncExecutor)

2. **Work-Efficient Algorithms** : Pourquoi O(n) de travail avec O(log n) de span est optimal pour le parallel scan

3. **Synchronization Primitives** : Mutex, Condvar, Barrier et leur usage correct

4. **Scheduling Strategies** : Static vs Dynamic (work-stealing) vs Guided

5. **Future/Promise Pattern** : Communication asynchrone entre producteur et consommateur

---

### 5.2 LDA â€” Traduction Litterale (MAJUSCULES)

```
FONCTION parallel_reduce QUI RETOURNE UNE VALEUR DE TYPE T
ET PREND EN PARAMETRES:
    - input QUI EST UNE REFERENCE VERS UN TABLEAU DE T
    - identity QUI EST UNE VALEUR DE TYPE T
    - config QUI EST UNE REFERENCE VERS ParallelConfig
    - combine QUI EST UNE FONCTION (T, T) -> T
DEBUT FONCTION

    SI input EST VIDE ALORS
        RETOURNER identity
    FIN SI

    DECLARER num_threads COMME LE MINIMUM ENTRE config.num_threads ET LA TAILLE DE input
    DECLARER chunk_size COMME (TAILLE DE input PLUS num_threads MOINS 1) DIVISE PAR num_threads

    DECLARER partial_results COMME UN VECTEUR DE T INITIALISE PAR:
        CREER UN SCOPE DE THREADS s
        POUR CHAQUE t ALLANT DE 0 A num_threads MOINS 1 FAIRE
            CALCULER start COMME t MULTIPLIE PAR chunk_size
            SI start EST SUPERIEUR OU EGAL A LA TAILLE DE input ALORS
                CONTINUER A L'ITERATION SUIVANTE
            FIN SI
            CALCULER end COMME LE MINIMUM ENTRE (start PLUS chunk_size) ET LA TAILLE DE input
            EXTRAIRE chunk COMME LA TRANCHE DE input DE start A end
            CLONER identity EN id
            CREER UNE REFERENCE combine_ref VERS combine

            LANCER UN THREAD QUI:
                APPLIQUE fold SUR chunk AVEC id COMME ACCUMULATEUR
                ET combine_ref COMME FONCTION DE REDUCTION
                RETOURNE LE RESULTAT
            FIN THREAD
        FIN POUR

        ATTENDRE TOUS LES THREADS ET COLLECTER LES RESULTATS
    FIN SCOPE

    RETOURNER LE FOLD DE partial_results AVEC identity COMME ACCUMULATEUR
    ET combine COMME FONCTION DE REDUCTION

FIN FONCTION
```

---

### 5.2.2 LDA Style Academique

```
Algorithme: Reduction Parallele
Entrees:
    - T[] input : tableau d'elements
    - T identity : element neutre
    - (T,T)->T combine : fonction associative
Sortie: T resultat

Debut
    Si |input| = 0 Alors
        Retourner identity
    FinSi

    p <- min(num_threads, |input|)
    taille_chunk <- plafond(|input| / p)

    // Phase 1: Reductions locales en parallele
    Pour tout i dans [0, p-1] Faire en parallele
        debut <- i * taille_chunk
        fin <- min(debut + taille_chunk, |input|)
        resultats_partiels[i] <- reduce(input[debut..fin], identity, combine)
    FinPour

    // Phase 2: Reduction des resultats partiels
    Retourner reduce(resultats_partiels, identity, combine)
Fin
```

---

### 5.2.2.1 Logic Flow (Structured English)

```
ALGORITHM: Parallel Scan (Blelloch)
---

1. INPUT VALIDATION:
   |-- IF input is empty:
   |     RETURN empty vector
   |
   |-- IF input has single element:
   |     IF Inclusive: RETURN [element]
   |     IF Exclusive: RETURN [identity]

2. SETUP:
   |-- PAD input to next power of 2
   |-- SET levels = log2(padded_length)

3. UP-SWEEP (Reduce Phase):
   |
   |-- FOR each level d from 0 to (levels - 1):
   |     |
   |     |-- COMPUTE stride = 2^(d+1)
   |     |
   |     |-- FOR each pair at distance stride IN PARALLEL:
   |     |     left_idx = (i+1)*stride - 1 - 2^d
   |     |     right_idx = (i+1)*stride - 1
   |     |     data[right_idx] = combine(data[left_idx], data[right_idx])

4. DOWN-SWEEP (Distribution Phase):
   |
   |-- SET data[last] = identity
   |
   |-- FOR each level d from (levels - 1) down to 0:
   |     |
   |     |-- COMPUTE stride = 2^(d+1)
   |     |
   |     |-- FOR each pair at distance stride IN PARALLEL:
   |     |     temp = data[left_idx]
   |     |     data[left_idx] = data[right_idx]
   |     |     data[right_idx] = combine(temp, data[right_idx])

5. FINALIZE:
   |-- TRUNCATE to original size
   |-- IF Inclusive: COMBINE with original input
   |-- RETURN result
```

---

### 5.2.3 Representation Algorithmique

```
FONCTION: parallel_scan(input, identity, scan_type, combine)
---
INIT result = []
INIT padded = pad_to_power_of_2(input, identity)
INIT n = |padded|

1. UP-SWEEP:
   |
   |-- POUR d = 0 A log2(n) - 1:
   |     stride = 2^(d+1)
   |     |
   |     |-- POUR i = stride-1 A n-1 PAR stride EN PARALLELE:
   |           left = i - 2^d
   |           padded[i] = combine(padded[left], padded[i])

2. CLEAR ROOT:
   |-- padded[n-1] = identity

3. DOWN-SWEEP:
   |
   |-- POUR d = log2(n) - 1 A 0:
   |     stride = 2^(d+1)
   |     |
   |     |-- POUR i = stride-1 A n-1 PAR stride EN PARALLELE:
   |           left = i - 2^d
   |           temp = padded[left]
   |           padded[left] = padded[i]
   |           padded[i] = combine(temp, padded[i])

4. RETOURNER truncate(padded, |input|)
```

---

### 5.2.3.1 Logique de Garde (Fail Fast)

```
FONCTION: AsyncExecutor::spawn(task)
---
INIT (promise, future) = Promise::new()

1. VERIFIER si executor est shutdown:
   |     RETOURNER Error "Executor is shut down"

2. CREER boxed_task qui:
   |     EXECUTE task()
   |     APPELLE promise.set_value(result)

3. ACQUERIR lock sur task_queue:
   |-- VERIFIER que lock est acquis:
   |     SI echec: RETOURNER Error "Queue lock failed"
   |
   |-- AJOUTER boxed_task a la queue
   |-- LIBERER lock

4. NOTIFIER un worker via condvar

5. RETOURNER future
```

---

### 5.3 Visualisation ASCII

```
PARALLEL REDUCE: Arbre de reduction
===================================

Input: [1, 2, 3, 4, 5, 6, 7, 8]
Threads: 4, Chunk size: 2

Phase 1: Reductions locales (en parallele)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Thread 0      Thread 1      Thread 2      Thread 3         â”‚
â”‚  [1, 2]        [3, 4]        [5, 6]        [7, 8]           â”‚
â”‚    â”‚            â”‚             â”‚             â”‚               â”‚
â”‚    â–¼            â–¼             â–¼             â–¼               â”‚
â”‚  1+2=3        3+4=7         5+6=11       7+8=15            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
Phase 2: Reduction finale (sequentiel, petit)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  partial_results = [3, 7, 11, 15]                           â”‚
â”‚                                                             â”‚
â”‚  3 + 7 = 10                                                 â”‚
â”‚  10 + 11 = 21                                               â”‚
â”‚  21 + 15 = 36                                               â”‚
â”‚                                                             â”‚
â”‚  Resultat final: 36                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


WORK STEALING: Files de taches
==============================

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        GLOBAL TASK POOL         â”‚
                    â”‚  [Task1][Task2][Task3]...[TaskN]â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                     â”‚                     â”‚
            â–¼                     â–¼                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   WORKER 0    â”‚     â”‚   WORKER 1    â”‚     â”‚   WORKER 2    â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚ â”‚Local Queueâ”‚ â”‚     â”‚ â”‚Local Queueâ”‚ â”‚     â”‚ â”‚Local Queueâ”‚ â”‚
    â”‚ â”‚[A][B][C]  â”‚ â”‚     â”‚ â”‚[D][E]     â”‚ â”‚     â”‚ â”‚  (empty)  â”‚ â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚       â”‚       â”‚     â”‚       â”‚       â”‚     â”‚       â”‚       â”‚
    â”‚   pop_front   â”‚     â”‚   pop_front   â”‚     â”‚    IDLE...    â”‚
    â”‚       â”‚       â”‚     â”‚       â”‚       â”‚     â”‚       â”‚       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚                     â”‚
            â”‚                     â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚       â”‚ STEAL!
            â”‚                     â”‚       â–¼ (pop_back)
            â”‚                     â”‚     â”Œâ”€â”€â”€â”
            â”‚                     â””â”€â”€â”€â”€â”€â”¤[E]â”‚
            â”‚                           â””â”€â”€â”€â”˜
            â”‚
         [Process A]


FUTURE/PROMISE: Cycle de vie
============================

    Producer Thread                    Consumer Thread
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          â”‚                                  â”‚
          â”‚  let (promise, future)           â”‚
          â”‚  = Promise::new()                â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
          â”‚         (future)                 â”‚
          â”‚                                  â”‚
          â”‚                                  â”‚  future.get()
          â”‚                                  â”‚      â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚  WORKING  â”‚                      â”‚  WAITING  â”‚
    â”‚    ...    â”‚                      â”‚  (cvar)   â”‚
    â”‚    ...    â”‚                      â”‚    ...    â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                                  â”‚
          â”‚  promise.set_value(42)           â”‚
          â”‚         â”‚                        â”‚
          â”‚         â–¼                        â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
          â”‚  â”‚ state=Ready  â”‚                â”‚
          â”‚  â”‚ value=42     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  â”‚ cvar.notify  â”‚                â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
          â”‚                                  â”‚
          â”‚                            â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
          â”‚                            â”‚ RECEIVED  â”‚
          â”‚                            â”‚  Ok(42)   â”‚
          â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


BLELLOCH SCAN: Up-sweep et Down-sweep
=====================================

Input: [3, 1, 7, 0, 4, 1, 6, 3]

UP-SWEEP (Reduce):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Level 0:  [3]   [1]   [7]   [0]   [4]   [1]   [6]   [3]
           â””â”€â”¬â”€â”˜       â””â”€â”¬â”€â”˜       â””â”€â”¬â”€â”˜       â””â”€â”¬â”€â”˜
Level 1:    [4]         [7]         [5]         [9]
             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
Level 2:         [11]                    [14]
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Level 3:                   [25]  (sum total)


DOWN-SWEEP (Distribute prefixes):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Level 3:                   [0]   <- identity
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
Level 2:         [0]              [11]
             â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
Level 1:    [0]        [4]   [11]      [16]
           â”Œâ”€â”´â”€â”      â”Œâ”€â”´â”€â”  â”Œâ”€â”´â”€â”     â”Œâ”€â”´â”€â”
Level 0:  [0] [3]    [4] [11][11][15] [16][22]

Exclusive scan result: [0, 3, 4, 11, 11, 15, 16, 22]
```

---

### 5.4 Les pieges en detail

| Piege | Description | Consequence | Solution |
|-------|-------------|-------------|----------|
| Data Race dans map | Plusieurs threads ecrivent au meme index | Undefined Behavior | Utiliser chunks_mut disjoints |
| Oubli Condvar.notify | Workers bloques pour toujours | Deadlock au shutdown | Toujours notify_all avant join |
| Scan non-associatif | Ordre de combinaison indefini | Resultats incorrects | Documenter que combine doit etre associatif |
| Off-by-one dans chunks | Dernier chunk trop grand/petit | Elements manques ou panic | Utiliser .min(end) |
| Future drop avant set | Promise orpheline | Memoire non liberee | Arc garantit la liberation |

---

### 5.5 Cours Complet

#### 5.5.1 Introduction au Parallelisme de Donnees

Le **parallelisme de donnees** consiste a appliquer la meme operation a differents elements de donnees simultanement. C'est le fondement de :
- Le calcul GPU (CUDA, OpenCL)
- Les frameworks Big Data (Spark, Flink)
- Les bibliotheques paralleles (Rayon, OpenMP)

**Loi d'Amdahl** : Le speedup maximal est limite par la fraction sequentielle :
```
Speedup(p) = 1 / (s + (1-s)/p)
```
ou `s` est la fraction sequentielle et `p` le nombre de processeurs.

**Loi de Gustafson** : En augmentant la taille du probleme, on peut maintenir un speedup lineaire :
```
Speedup(p) = s + p*(1-s) = p - s*(p-1)
```

#### 5.5.2 Parallel For : La Base

Le parallel_for est le pattern le plus simple. Il distribue les iterations d'une boucle sur plusieurs threads.

**Static Scheduling** :
- Division egale pre-calculee
- Pas d'overhead de synchronisation
- Optimal si le travail est uniforme

**Dynamic Scheduling (Work Stealing)** :
- Les threads inactifs volent du travail aux autres
- Overhead de synchronisation
- Optimal si le travail est irregulier

**Guided Scheduling** :
- Chunks de taille decroissante
- Compromis entre static et dynamic

#### 5.5.3 MapReduce

Le pattern **MapReduce** decompose un calcul en deux phases :

1. **Map** : Appliquer une fonction a chaque element (parallelisable)
2. **Reduce** : Combiner les resultats avec une operation associative

**Pourquoi associatif ?** Si `combine(a, combine(b, c)) != combine(combine(a, b), c)`, l'ordre d'execution change le resultat, ce qui est non-deterministe en parallele.

#### 5.5.4 Parallel Scan (Prefix Sum)

Le **prefix sum** (ou scan) calcule les sommes cumulatives :
```
input:  [1, 2, 3, 4, 5]
inclusive: [1, 3, 6, 10, 15]  // sum of elements 0..=i
exclusive: [0, 1, 3, 6, 10]   // sum of elements 0..i
```

**Algorithme de Blelloch** :
- Work: O(2n - 2) operations
- Span: O(2 log n) niveaux
- Work-efficient (optimal)

**Applications** :
- Allocation memoire parallele
- Tri par radix
- Stream compaction GPU
- Histogrammes paralleles

#### 5.5.5 Future/Promise

Le pattern **Future/Promise** separe :
- **Promise** : Le producteur qui fournira la valeur
- **Future** : Le consommateur qui attend la valeur

C'est un canal one-shot : une seule valeur peut etre envoyee.

**Etats possibles** :
1. Pending : Pas encore de valeur
2. Ready(T) : Valeur disponible
3. Failed(E) : Erreur survenue

#### 5.5.6 Work Stealing

Le **work stealing** est un algorithme de scheduling ou :
1. Chaque worker a une **deque** locale (double-ended queue)
2. Le worker pop ses taches par l'avant (LIFO pour localite)
3. Les workers inactifs volent par l'arriere (FIFO pour gros travail)

**Theoreme** : Avec p workers, le temps d'execution esperance est :
```
E[T] = T_1/p + O(T_inf)
```
ou T_1 est le travail total et T_inf le chemin critique.

---

### 5.6 Normes avec explications pedagogiques

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HORS NORME (compile, mais interdit)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ let data = Arc::new(vec![1,2,3]);                               â”‚
â”‚ for i in 0..3 {                                                 â”‚
â”‚     let d = data.clone();                                       â”‚
â”‚     thread::spawn(move || d[i] *= 2);  // RACE!                 â”‚
â”‚ }                                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CONFORME                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ let data = Arc::new(Mutex::new(vec![1,2,3]));                   â”‚
â”‚ for i in 0..3 {                                                 â”‚
â”‚     let d = Arc::clone(&data);                                  â”‚
â”‚     thread::spawn(move || {                                     â”‚
â”‚         let mut guard = d.lock().unwrap();                      â”‚
â”‚         guard[i] *= 2;                                          â”‚
â”‚     });                                                         â”‚
â”‚ }                                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ POURQUOI ?                                                      â”‚
â”‚                                                                 â”‚
â”‚ - Arc seul permet le partage mais pas la mutation              â”‚
â”‚ - Mutex garantit l'exclusion mutuelle                          â”‚
â”‚ - Vec n'est pas Sync, donc Arc<Vec> n'est pas Send aux threads â”‚
â”‚ - Arc<Mutex<Vec>> est Send + Sync                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.7 Simulation avec trace d'execution

**parallel_reduce([1,2,3,4,5,6,7,8], 0, 4 threads, +)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Etape â”‚ Action                                                           â”‚ Resultats locaux â”‚ Resultat global     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1   â”‚ Calcul: num_threads = min(4, 8) = 4                              â”‚ â€”                â”‚ â€”                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   2   â”‚ Calcul: chunk_size = (8 + 4 - 1) / 4 = 2                         â”‚ â€”                â”‚ â€”                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   3   â”‚ Thread 0: reduce([1,2], 0, +) = 3                                â”‚ [3, _, _, _]     â”‚ â€”                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   4   â”‚ Thread 1: reduce([3,4], 0, +) = 7                                â”‚ [3, 7, _, _]     â”‚ â€”                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   5   â”‚ Thread 2: reduce([5,6], 0, +) = 11                               â”‚ [3, 7, 11, _]    â”‚ â€”                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   6   â”‚ Thread 3: reduce([7,8], 0, +) = 15                               â”‚ [3, 7, 11, 15]   â”‚ â€”                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   7   â”‚ SYNC: Tous les threads termines                                  â”‚ [3, 7, 11, 15]   â”‚ â€”                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   8   â”‚ Final: fold([3,7,11,15], 0, +) = 0+3+7+11+15                     â”‚ â€”                â”‚ 36                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   9   â”‚ RETOURNER 36                                                     â”‚ â€”                â”‚ 36 FINAL            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.8 Mnemotechniques

#### MEME : "AVENGERS ASSEMBLE" â€” MapReduce

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  "I am Iron Man" - MAP: Chaque heros agit seul     â”‚
        â”‚  "Avengers... ASSEMBLE" - REDUCE: Fury combine     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Souviens-toi** : Dans Endgame, chaque Avenger combat les forces de Thanos dans SA zone (MAP), puis Cap crie "Avengers Assemble" pour l'assaut final coordonne (REDUCE).

```rust
// Chaque heros = un thread
fn avenger_fight(zone: &[Enemy]) -> Vec<Victory> {
    zone.iter().map(|e| hero.fight(e)).collect()  // MAP
}

// Nick Fury combine tout
fn fury_debrief(reports: Vec<Vec<Victory>>) -> FinalReport {
    reports.into_iter().flatten().collect()  // REDUCE
}
```

---

#### MEME : "I can do this all day" â€” Work Stealing

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Cap: "I can do this all day"                       â”‚
        â”‚  Traduction: "Je prends le travail des autres"      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Captain America ne reste jamais inactif. Quand il finit son secteur, il va aider les autres. C'est exactement le work stealing :

```rust
loop {
    if let Some(task) = my_queue.pop() {
        task.execute();  // Mon travail
    } else {
        // "I can do this all day" - je vais aider
        if let Some(stolen) = other_worker.steal() {
            stolen.execute();
        }
    }
}
```

---

#### MEME : "I am Groot" â€” Future/Promise

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Groot: "I am Groot" (la promesse)                  â”‚
        â”‚  Rocket: *attend la traduction* (le future)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Groot fait une promesse en disant "I am Groot". Rocket (et le spectateur) attend le Future pour comprendre ce que ca veut dire.

```rust
let (groot_says, rocket_hears) = Promise::<Meaning>::new();

// Groot (producteur)
thread::spawn(move || {
    let meaning = translate("I am Groot");
    groot_says.set_value(meaning);
});

// Rocket (consommateur)
let what_groot_meant = rocket_hears.get();  // "We are Groot" ;_;
```

---

### 5.9 Applications pratiques

| Domaine | Pattern | Exemple |
|---------|---------|---------|
| **Jeux Video** | parallel_for | Calcul de physics pour 10K entites |
| **Finance** | parallel_reduce | Monte Carlo sur 1M de paths |
| **ML/AI** | parallel_map | Inference batch sur GPU |
| **Databases** | parallel_scan | Index B-tree rebuild |
| **Video** | pipeline + futures | Decode -> Process -> Encode |
| **Web Servers** | AsyncExecutor | Handling de 10K connexions |

---

## SECTION 6 : PIEGES â€” RECAPITULATIF

| # | Piege | Symptome | Prevention |
|---|-------|----------|------------|
| 1 | Data race dans map | Resultats corrompus | chunks_mut garantit disjonction |
| 2 | Deadlock Condvar | Programme bloque | Toujours notify avant join |
| 3 | Combine non-associatif | Resultats non-deterministes | Documenter la contrainte |
| 4 | Off-by-one chunks | Panic ou elements manques | .min(end) sur les bornes |
| 5 | Future drop premature | Memory leak | Arc pour lifetime partage |
| 6 | Busy waiting | CPU a 100% | Utiliser Condvar.wait() |
| 7 | Work stealing LIFO | Localite perdue | Steal from back, work from front |
| 8 | Scan exclusive off-by-one | Premier element faux | result[0] = identity |

---

## SECTION 7 : QCM

### Question 1
**Quel est l'avantage principal du work stealing par rapport au static scheduling ?**

A) Il utilise moins de memoire
B) Il equilibre automatiquement la charge entre threads inactifs et surcharges
C) Il est plus simple a implementer
D) Il garantit un ordre d'execution deterministe
E) Il evite completement les locks
F) Il fonctionne sans threads
G) Il reduit le nombre de context switches
H) Il elimine les race conditions
I) Il permet le parallelisme sur un seul core
J) Il augmente la localite du cache

---

### Question 2
**Pourquoi l'operation combine dans parallel_reduce doit-elle etre associative ?**

A) Pour permettre la recursion
B) Pour eviter les overflows
C) Parce que l'ordre de combinaison des resultats partiels est indefini
D) Pour supporter les nombres negatifs
E) Pour permettre le multithreading
F) Pour optimiser la memoire
G) Pour eviter les deadlocks
H) Pour supporter les types generiques
I) Pour permettre le lazy evaluation
J) Pour eviter les race conditions

---

### Question 3
**Dans l'algorithme de Blelloch pour le parallel scan, quelle est la complexite en travail (work) ?**

A) O(n^2)
B) O(n log n)
C) O(n log^2 n)
D) O(2n - 2) soit O(n)
E) O(log n)
F) O(n / log n)
G) O(n * p) ou p = nombre de threads
H) O(sqrt(n))
I) O(1)
J) O(n^3)

---

### Question 4
**Que se passe-t-il si on appelle future.get() avant que promise.set_value() soit appele ?**

A) Panic immediate
B) Retourne une valeur par defaut
C) Le thread appelant bloque jusqu'a ce que la valeur soit disponible
D) Retourne None
E) Lance une exception
F) Retourne immediatement avec une erreur
G) Le programme termine
H) Undefined behavior
I) Le thread est tue
J) Deadlock garanti

---

### Question 5
**Dans le work stealing, pourquoi le vol se fait-il par l'arriere de la deque (pop_back) plutot que par l'avant ?**

A) C'est plus rapide
B) Ca utilise moins de memoire
C) Pour eviter la contention avec le worker qui travaille par l'avant (pop_front)
D) Pour garantir l'ordre FIFO
E) Pour supporter plus de threads
F) C'est une convention arbitraire
G) Pour eviter les race conditions
H) Pour optimiser le cache
I) Pour permettre le parallelisme
J) Pour supporter les types generiques

---

### Reponses QCM

| Question | Reponse | Explication |
|----------|---------|-------------|
| 1 | **B** | Le work stealing permet aux threads inactifs de voler du travail aux threads surcharges, equilibrant ainsi automatiquement la charge |
| 2 | **C** | En parallele, les resultats partiels peuvent etre combines dans n'importe quel ordre. Si combine n'est pas associatif, (a+b)+c != a+(b+c), donc resultats incorrects |
| 3 | **D** | Blelloch fait exactement 2n-2 operations : n-1 dans l'up-sweep et n-1 dans le down-sweep, ce qui est work-optimal |
| 4 | **C** | future.get() utilise Condvar::wait() pour bloquer jusqu'a ce que l'etat passe de Pending a Ready ou Failed |
| 5 | **C** | Le worker pop_front ses propres taches, le voleur pop_back. Ils accedent a des extremites differentes, minimisant la contention sur le lock |

---

## SECTION 8 : RECAPITULATIF

| Critere | Valeur |
|---------|--------|
| Concept principal | Patterns de programmation parallele |
| Patterns couverts | parallel_for, map, reduce, scan, future/promise, executor |
| Difficulte | 8/10 |
| Complexite temporelle | O(n/p) pour map/reduce, O(n/p + log n) pour scan |
| Complexite spatiale | O(n) |
| Prerequis | Threads, Arc/Mutex, Channels, Atomics |
| Points cles | Work-efficiency, associativite, synchronisation |
| Piege principal | Data races et deadlocks |
| Mnemotechnique | Avengers Assemble = MapReduce |

---

## SECTION 9 : DEPLOYMENT PACK

```json
{
  "deploy": {
    "hackbrain_version": "5.5.2",
    "engine_version": "v22.1",
    "exercise_slug": "2.4.5-synth-avengers-parallel-lib",
    "generated_at": "2026-01-16 14:30:00",

    "metadata": {
      "exercise_id": "2.4.5-synth",
      "exercise_name": "avengers_parallel_lib",
      "module": "2.4.5",
      "module_name": "Patterns de Programmation Parallele Avances",
      "concepts": ["a", "d", "g", "h", "c"],
      "concept_names": ["Parallel for", "Parallel prefix", "Data decomposition", "Critical sections", "Future/Promise"],
      "type": "complet",
      "tier": 3,
      "tier_info": "Synthese (tous concepts)",
      "phase": 2,
      "difficulty": 8,
      "difficulty_stars": "â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜†â˜†",
      "language": "rust",
      "rust_edition": "2024",
      "duration_minutes": 180,
      "xp_base": 500,
      "xp_bonus_multiplier": 4,
      "bonus_tier": "EXPERT",
      "bonus_icon": "ğŸ’€",
      "complexity_time": "T8 O(n/p)",
      "complexity_space": "S6 O(n)",
      "prerequisites": ["2.4.1", "2.4.2", "2.4.3", "2.4.4"],
      "domains": ["Struct", "Algo", "CPU", "Process"],
      "domains_bonus": ["DP"],
      "tags": ["parallel", "concurrency", "threads", "futures", "mapreduce", "work-stealing", "scan"],
      "meme_reference": "Avengers Assemble"
    },

    "files": {
      "spec.json": "/* Section 4.9 content */",
      "references/ref_solution.rs": "/* Section 4.3 content */",
      "references/ref_solution_bonus.rs": "/* Section 4.6 content */",
      "alternatives/alt_barrier.rs": "/* Section 4.4 alternative 1 */",
      "alternatives/alt_scan_simple.rs": "/* Section 4.4 alternative 2 */",
      "mutants/mutant_a_boundary.rs": "/* Section 4.10 mutant A */",
      "mutants/mutant_b_safety.rs": "/* Section 4.10 mutant B */",
      "mutants/mutant_c_resource.rs": "/* Section 4.10 mutant C */",
      "mutants/mutant_d_logic.rs": "/* Section 4.10 mutant D */",
      "mutants/mutant_e_return.rs": "/* Section 4.10 mutant E */",
      "tests/main.rs": "/* Section 4.2 content */"
    },

    "validation": {
      "expected_pass": [
        "references/ref_solution.rs",
        "references/ref_solution_bonus.rs",
        "alternatives/alt_barrier.rs",
        "alternatives/alt_scan_simple.rs"
      ],
      "expected_fail": [
        "mutants/mutant_a_boundary.rs",
        "mutants/mutant_b_safety.rs",
        "mutants/mutant_c_resource.rs",
        "mutants/mutant_d_logic.rs",
        "mutants/mutant_e_return.rs"
      ]
    },

    "commands": {
      "validate_spec": "python3 hackbrain_engine_v22.py --validate-spec spec.json",
      "test_reference": "cargo test --release",
      "test_miri": "cargo +nightly miri test",
      "test_mutants": "python3 hackbrain_mutation_tester.py -r references/ref_solution.rs -s spec.json --validate",
      "benchmark": "cargo bench"
    },

    "quality_score": 97
  }
}
```

---

*HACKBRAIN v5.5.2 â€” L'excellence pedagogique ne se negocie pas*
*"Avengers Assemble!" â€” Nick Fury, Director of SHIELD et MapReduce*
