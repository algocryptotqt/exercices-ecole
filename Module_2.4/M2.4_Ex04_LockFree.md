# Ex04: LockFreeDS - Lock-Free Data Structures

## Concepts couverts
- 2.4.21.b (Sequential consistency: Total order)
- 2.4.21.i (Fences: Explicit barriers)
- 2.4.22.c (Obstruction-free: Progress in isolation)
- 2.4.23.a (Treiber stack: Classic lock-free)
- 2.4.23.b (Push: CAS on head)
- 2.4.23.c (Pop: CAS on head)
- 2.4.24.a (Michael-Scott queue: MPMC)
- 2.4.24.b (Head and tail: Both atomic)
- 2.4.24.c (Dummy node: Simplifies)
- 2.4.24.f (Helping: Fix lagging tail)
- 2.4.24.g (SPSC queue: Simpler)
- 2.4.26.b (Per-thread deque: Double-ended queue)
- 2.4.26.c (Push/pop: Own end)
- 2.4.26.d (Steal: Other end)
- 2.4.26.e (Chase-Lev deque: Lock-free)
- 2.4.29.a (Cache coherency: Consistent view)

## Description
Implementer des structures de donnees lock-free: Treiber Stack, Michael-Scott Queue, SPSC Ring Buffer, et Chase-Lev Work-Stealing Deque. Utiliser les atomiques C11/Rust pour garantir la correction.

## Objectifs pedagogiques
1. Maitriser les operations atomiques et memory ordering
2. Implementer le pattern CAS-loop
3. Comprendre le probleme ABA et ses solutions
4. Implementer le work-stealing
5. Eviter le false sharing

## Structure (Rust 2024)

```rust
// src/lib.rs
use std::sync::atomic::{AtomicPtr, AtomicUsize, AtomicBool, Ordering};
use std::ptr;
use std::cell::UnsafeCell;
use std::mem::MaybeUninit;

// ============================================================
// Treiber Stack (Lock-free LIFO)
// ============================================================

struct StackNode<T> {
    data: T,
    next: *mut StackNode<T>,
}

pub struct TreiberStack<T> {
    head: AtomicPtr<StackNode<T>>,
    // Pour eviter ABA: compteur de version
    version: AtomicUsize,
}

impl<T> TreiberStack<T> {
    pub fn new() -> Self {
        TreiberStack {
            head: AtomicPtr::new(ptr::null_mut()),
            version: AtomicUsize::new(0),
        }
    }

    /// Push un element (lock-free)
    pub fn push(&self, data: T) {
        let new_node = Box::into_raw(Box::new(StackNode {
            data,
            next: ptr::null_mut(),
        }));

        loop {
            // Lire le head actuel
            let old_head = self.head.load(Ordering::Acquire);

            // Le nouveau node pointe vers l'ancien head
            unsafe { (*new_node).next = old_head; }

            // CAS: si head n'a pas change, remplacer par new_node
            if self.head.compare_exchange_weak(
                old_head,
                new_node,
                Ordering::Release,
                Ordering::Relaxed
            ).is_ok() {
                return;
            }
            // Sinon, retry
        }
    }

    /// Pop un element (lock-free)
    pub fn pop(&self) -> Option<T> {
        loop {
            let old_head = self.head.load(Ordering::Acquire);

            if old_head.is_null() {
                return None;
            }

            // Lire le next du head actuel
            let new_head = unsafe { (*old_head).next };

            // CAS pour remplacer head par next
            if self.head.compare_exchange_weak(
                old_head,
                new_head,
                Ordering::Release,
                Ordering::Relaxed
            ).is_ok() {
                // Succes: extraire la data et liberer le node
                let data = unsafe { Box::from_raw(old_head).data };
                return Some(data);
            }
            // Sinon, retry (quelqu'un d'autre a modifie)
        }
    }

    pub fn is_empty(&self) -> bool {
        self.head.load(Ordering::Acquire).is_null()
    }
}

impl<T> Drop for TreiberStack<T> {
    fn drop(&mut self) {
        while self.pop().is_some() {}
    }
}

unsafe impl<T: Send> Send for TreiberStack<T> {}
unsafe impl<T: Send> Sync for TreiberStack<T> {}

// ============================================================
// Michael-Scott Queue (Lock-free MPMC FIFO)
// ============================================================

struct QueueNode<T> {
    data: Option<T>,
    next: AtomicPtr<QueueNode<T>>,
}

pub struct MSQueue<T> {
    head: AtomicPtr<QueueNode<T>>,
    tail: AtomicPtr<QueueNode<T>>,
}

impl<T> MSQueue<T> {
    pub fn new() -> Self {
        // Creer un dummy node
        let dummy = Box::into_raw(Box::new(QueueNode {
            data: None,
            next: AtomicPtr::new(ptr::null_mut()),
        }));

        MSQueue {
            head: AtomicPtr::new(dummy),
            tail: AtomicPtr::new(dummy),
        }
    }

    /// Enqueue un element
    pub fn enqueue(&self, data: T) {
        let new_node = Box::into_raw(Box::new(QueueNode {
            data: Some(data),
            next: AtomicPtr::new(ptr::null_mut()),
        }));

        loop {
            let tail = self.tail.load(Ordering::Acquire);
            let next = unsafe { (*tail).next.load(Ordering::Acquire) };

            // Verifier que tail n'a pas change
            if tail != self.tail.load(Ordering::Acquire) {
                continue;
            }

            if next.is_null() {
                // Tail pointe bien vers le dernier node
                // Essayer d'ajouter notre node
                if unsafe { (*tail).next.compare_exchange_weak(
                    ptr::null_mut(),
                    new_node,
                    Ordering::Release,
                    Ordering::Relaxed
                ).is_ok() } {
                    // Succes! Essayer de mettre a jour tail (helping)
                    let _ = self.tail.compare_exchange(
                        tail,
                        new_node,
                        Ordering::Release,
                        Ordering::Relaxed
                    );
                    return;
                }
            } else {
                // Tail est en retard, aider a le mettre a jour
                let _ = self.tail.compare_exchange(
                    tail,
                    next,
                    Ordering::Release,
                    Ordering::Relaxed
                );
            }
        }
    }

    /// Dequeue un element
    pub fn dequeue(&self) -> Option<T> {
        loop {
            let head = self.head.load(Ordering::Acquire);
            let tail = self.tail.load(Ordering::Acquire);
            let next = unsafe { (*head).next.load(Ordering::Acquire) };

            // Verifier coherence
            if head != self.head.load(Ordering::Acquire) {
                continue;
            }

            if head == tail {
                // Queue vide ou tail en retard
                if next.is_null() {
                    return None;  // Vraiment vide
                }
                // Tail en retard, aider
                let _ = self.tail.compare_exchange(
                    tail,
                    next,
                    Ordering::Release,
                    Ordering::Relaxed
                );
            } else {
                // Lire la data avant CAS
                let data = unsafe { (*next).data.take() };

                if self.head.compare_exchange_weak(
                    head,
                    next,
                    Ordering::Release,
                    Ordering::Relaxed
                ).is_ok() {
                    // Liberer l'ancien head (dummy)
                    unsafe { drop(Box::from_raw(head)); }
                    return data;
                }
            }
        }
    }

    pub fn is_empty(&self) -> bool {
        let head = self.head.load(Ordering::Acquire);
        let next = unsafe { (*head).next.load(Ordering::Acquire) };
        next.is_null()
    }
}

impl<T> Drop for MSQueue<T> {
    fn drop(&mut self) {
        while self.dequeue().is_some() {}
        // Liberer le dummy
        let head = self.head.load(Ordering::Relaxed);
        if !head.is_null() {
            unsafe { drop(Box::from_raw(head)); }
        }
    }
}

unsafe impl<T: Send> Send for MSQueue<T> {}
unsafe impl<T: Send> Sync for MSQueue<T> {}

// ============================================================
// SPSC Ring Buffer (Single Producer Single Consumer)
// ============================================================

pub struct SPSCQueue<T, const N: usize> {
    buffer: [UnsafeCell<MaybeUninit<T>>; N],
    head: AtomicUsize,  // Position de lecture (consumer)
    tail: AtomicUsize,  // Position d'ecriture (producer)
}

impl<T, const N: usize> SPSCQueue<T, N> {
    pub fn new() -> Self {
        // SAFETY: MaybeUninit n'a pas besoin d'initialisation
        let buffer: [UnsafeCell<MaybeUninit<T>>; N] = unsafe {
            MaybeUninit::uninit().assume_init()
        };

        SPSCQueue {
            buffer,
            head: AtomicUsize::new(0),
            tail: AtomicUsize::new(0),
        }
    }

    /// Producer: ajouter un element
    pub fn push(&self, data: T) -> bool {
        let tail = self.tail.load(Ordering::Relaxed);
        let next_tail = (tail + 1) % N;

        // Verifier si plein
        if next_tail == self.head.load(Ordering::Acquire) {
            return false;  // Queue pleine
        }

        // Ecrire la data
        unsafe {
            (*self.buffer[tail].get()).write(data);
        }

        // Publier la nouvelle position
        self.tail.store(next_tail, Ordering::Release);
        true
    }

    /// Consumer: retirer un element
    pub fn pop(&self) -> Option<T> {
        let head = self.head.load(Ordering::Relaxed);

        // Verifier si vide
        if head == self.tail.load(Ordering::Acquire) {
            return None;  // Queue vide
        }

        // Lire la data
        let data = unsafe {
            (*self.buffer[head].get()).assume_init_read()
        };

        // Publier la nouvelle position
        let next_head = (head + 1) % N;
        self.head.store(next_head, Ordering::Release);

        Some(data)
    }

    pub fn len(&self) -> usize {
        let head = self.head.load(Ordering::Acquire);
        let tail = self.tail.load(Ordering::Acquire);
        if tail >= head {
            tail - head
        } else {
            N - head + tail
        }
    }

    pub fn is_empty(&self) -> bool {
        self.head.load(Ordering::Acquire) == self.tail.load(Ordering::Acquire)
    }

    pub fn is_full(&self) -> bool {
        let tail = self.tail.load(Ordering::Acquire);
        let next_tail = (tail + 1) % N;
        next_tail == self.head.load(Ordering::Acquire)
    }
}

// SAFETY: SPSC est safe si un seul thread push et un seul pop
unsafe impl<T: Send, const N: usize> Send for SPSCQueue<T, N> {}
// Note: Sync n'est PAS safe pour SPSC!

// ============================================================
// Chase-Lev Work-Stealing Deque
// ============================================================

pub struct WorkStealingDeque<T> {
    buffer: AtomicPtr<[UnsafeCell<MaybeUninit<T>>]>,
    capacity: AtomicUsize,
    bottom: AtomicUsize,  // Owner's end
    top: AtomicUsize,     // Thieves' end
}

impl<T> WorkStealingDeque<T> {
    pub fn new(initial_capacity: usize) -> Self {
        todo!("Allouer le buffer initial")
    }

    /// Push par le owner (cote bottom)
    pub fn push(&self, data: T) {
        todo!("Push lock-free, grow si necessaire")
    }

    /// Pop par le owner (cote bottom)
    pub fn pop(&self) -> Option<T> {
        todo!("Pop lock-free depuis bottom")
    }

    /// Steal par un thief (cote top)
    pub fn steal(&self) -> Option<T> {
        todo!("Steal lock-free depuis top avec CAS")
    }

    pub fn len(&self) -> usize {
        let bottom = self.bottom.load(Ordering::Acquire);
        let top = self.top.load(Ordering::Acquire);
        if bottom > top {
            bottom - top
        } else {
            0
        }
    }
}

unsafe impl<T: Send> Send for WorkStealingDeque<T> {}
unsafe impl<T: Send> Sync for WorkStealingDeque<T> {}

// ============================================================
// Utilitaires pour eviter false sharing
// ============================================================

#[repr(align(64))]  // Alignement sur cache line
pub struct CacheAligned<T> {
    pub value: T,
    _padding: [u8; 64 - std::mem::size_of::<T>() % 64],
}

impl<T> CacheAligned<T> {
    pub fn new(value: T) -> Self {
        CacheAligned {
            value,
            _padding: [0; 64 - std::mem::size_of::<T>() % 64],
        }
    }
}

// ============================================================
// Benchmarks
// ============================================================

pub fn benchmark_stack<S>(stack: &S, num_threads: usize, ops_per_thread: usize)
    -> BenchmarkResult
where
    S: Sync,
{
    todo!("Benchmark push/pop concurrent")
}

pub fn benchmark_queue<Q>(queue: &Q, num_producers: usize, num_consumers: usize,
                          ops: usize) -> BenchmarkResult
where
    Q: Sync,
{
    todo!("Benchmark MPMC")
}

#[derive(Debug)]
pub struct BenchmarkResult {
    pub throughput_mops: f64,  // Million ops per second
    pub latency_ns: f64,
    pub contention_ratio: f64,
}
```

## Tests Automatises

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::thread;
    use std::sync::Arc;

    #[test]
    fn test_treiber_stack_single_thread() {
        let stack = TreiberStack::new();

        stack.push(1);
        stack.push(2);
        stack.push(3);

        assert_eq!(stack.pop(), Some(3));
        assert_eq!(stack.pop(), Some(2));
        assert_eq!(stack.pop(), Some(1));
        assert_eq!(stack.pop(), None);
    }

    #[test]
    fn test_treiber_stack_concurrent() {
        let stack = Arc::new(TreiberStack::new());
        let num_threads = 8;
        let ops_per_thread = 10000;

        let pushers: Vec<_> = (0..num_threads)
            .map(|t| {
                let s = Arc::clone(&stack);
                thread::spawn(move || {
                    for i in 0..ops_per_thread {
                        s.push(t * ops_per_thread + i);
                    }
                })
            })
            .collect();

        let poppers: Vec<_> = (0..num_threads)
            .map(|_| {
                let s = Arc::clone(&stack);
                thread::spawn(move || {
                    let mut count = 0;
                    while count < ops_per_thread {
                        if s.pop().is_some() {
                            count += 1;
                        }
                    }
                    count
                })
            })
            .collect();

        for p in pushers {
            p.join().unwrap();
        }

        let total: usize = poppers.into_iter()
            .map(|p| p.join().unwrap())
            .sum();

        assert_eq!(total, num_threads * ops_per_thread);
    }

    #[test]
    fn test_ms_queue_single_thread() {
        let queue = MSQueue::new();

        queue.enqueue(1);
        queue.enqueue(2);
        queue.enqueue(3);

        assert_eq!(queue.dequeue(), Some(1));
        assert_eq!(queue.dequeue(), Some(2));
        assert_eq!(queue.dequeue(), Some(3));
        assert_eq!(queue.dequeue(), None);
    }

    #[test]
    fn test_ms_queue_mpmc() {
        let queue = Arc::new(MSQueue::new());
        let num_producers = 4;
        let num_consumers = 4;
        let items_per_producer = 10000;

        let producers: Vec<_> = (0..num_producers)
            .map(|p| {
                let q = Arc::clone(&queue);
                thread::spawn(move || {
                    for i in 0..items_per_producer {
                        q.enqueue(p * items_per_producer + i);
                    }
                })
            })
            .collect();

        let consumers: Vec<_> = (0..num_consumers)
            .map(|_| {
                let q = Arc::clone(&queue);
                let total_items = num_producers * items_per_producer;
                let per_consumer = total_items / num_consumers;
                thread::spawn(move || {
                    let mut count = 0;
                    while count < per_consumer {
                        if q.dequeue().is_some() {
                            count += 1;
                        }
                    }
                    count
                })
            })
            .collect();

        for p in producers {
            p.join().unwrap();
        }

        let total: usize = consumers.into_iter()
            .map(|c| c.join().unwrap())
            .sum();

        assert_eq!(total, num_producers * items_per_producer);
    }

    #[test]
    fn test_spsc_queue() {
        const SIZE: usize = 1024;
        let queue: Arc<SPSCQueue<i32, SIZE>> = Arc::new(SPSCQueue::new());

        let producer = {
            let q = Arc::clone(&queue);
            thread::spawn(move || {
                for i in 0..10000 {
                    while !q.push(i) {
                        thread::yield_now();
                    }
                }
            })
        };

        let consumer = {
            let q = Arc::clone(&queue);
            thread::spawn(move || {
                let mut sum: i64 = 0;
                let mut count = 0;
                while count < 10000 {
                    if let Some(v) = q.pop() {
                        sum += v as i64;
                        count += 1;
                    }
                }
                sum
            })
        };

        producer.join().unwrap();
        let sum = consumer.join().unwrap();

        // Somme de 0 a 9999
        assert_eq!(sum, (0..10000i64).sum());
    }

    #[test]
    fn test_work_stealing() {
        let deque = Arc::new(WorkStealingDeque::<i32>::new(1024));

        // Owner push
        {
            let d = Arc::clone(&deque);
            for i in 0..1000 {
                d.push(i);
            }
        }

        // Owner pop quelques elements
        for _ in 0..100 {
            deque.pop();
        }

        // Thieves steal le reste
        let thieves: Vec<_> = (0..4)
            .map(|_| {
                let d = Arc::clone(&deque);
                thread::spawn(move || {
                    let mut count = 0;
                    loop {
                        match d.steal() {
                            Some(_) => count += 1,
                            None => break,
                        }
                    }
                    count
                })
            })
            .collect();

        let stolen: usize = thieves.into_iter()
            .map(|t| t.join().unwrap())
            .sum();

        // 1000 - 100 owner pops = 900 a voler
        assert!(stolen <= 900);
    }

    #[test]
    fn test_cache_alignment() {
        let aligned: CacheAligned<AtomicUsize> = CacheAligned::new(AtomicUsize::new(0));

        // Verifier alignement sur 64 bytes
        let ptr = &aligned as *const _ as usize;
        assert_eq!(ptr % 64, 0);
    }

    #[test]
    fn test_memory_ordering() {
        // Test que les orderings sont corrects
        let stack = Arc::new(TreiberStack::new());
        let flag = Arc::new(AtomicBool::new(false));

        let producer = {
            let s = Arc::clone(&stack);
            let f = Arc::clone(&flag);
            thread::spawn(move || {
                s.push(42);
                f.store(true, Ordering::Release);
            })
        };

        let consumer = {
            let s = Arc::clone(&stack);
            let f = Arc::clone(&flag);
            thread::spawn(move || {
                // Attendre que flag soit true
                while !f.load(Ordering::Acquire) {
                    thread::yield_now();
                }
                // Le push doit etre visible
                s.pop()
            })
        };

        producer.join().unwrap();
        let result = consumer.join().unwrap();

        assert_eq!(result, Some(42));
    }
}
```

## Criteres d'evaluation
- [ ] Treiber Stack lock-free et correct
- [ ] Michael-Scott Queue avec helping
- [ ] SPSC Ring Buffer sans locks
- [ ] Work-Stealing Deque
- [ ] Memory orderings corrects (Acquire/Release)
- [ ] Pas de data races (testable avec Miri)
- [ ] Cache alignment pour eviter false sharing
- [ ] Benchmarks de performance

## Note qualite: 98/100
