# Ex05: ParallelLib - Parallel Programming Patterns

## Concepts couverts
- 2.4.27.a (Parallel for: Distribute iterations)
- 2.4.27.d (Parallel prefix: Scan operation)
- 2.4.27.g (Data decomposition: Break data)
- 2.4.28.b (#pragma omp parallel: Parallel region)
- 2.4.28.c (#pragma omp for: Parallel loop)
- 2.4.28.d (#pragma omp parallel for: Combined)
- 2.4.28.h (critical: Mutual exclusion)
- 2.4.30.a (Asynchronous: Non-blocking)
- 2.4.30.b (Callback: Called on completion)
- 2.4.30.c (Future/Promise: Async result)
- 2.4.30.f (C++ coroutines: co_await, co_yield)

## Description
Implementer une bibliotheque de patterns de programmation parallele: parallel_for, map, reduce, scan, pipeline, et un executor async avec futures/promises. Compatible avec une API style OpenMP.

## Objectifs pedagogiques
1. Implementer les patterns paralleles fondamentaux
2. Comprendre le data parallelism vs task parallelism
3. Maitriser les reductions paralleles
4. Implementer le parallel prefix (scan)
5. Creer un systeme async avec futures

## Structure (Rust 2024)

```rust
// src/lib.rs
use std::sync::{Arc, Mutex, Barrier};
use std::thread;
use std::marker::PhantomData;

// ============================================================
// Parallel For
// ============================================================

pub struct ParallelConfig {
    pub num_threads: usize,
    pub chunk_size: Option<usize>,  // None = auto
    pub schedule: Schedule,
}

#[derive(Clone, Copy)]
pub enum Schedule {
    Static,     // Division egale
    Dynamic,    // Work stealing
    Guided,     // Chunks decroissants
}

impl Default for ParallelConfig {
    fn default() -> Self {
        ParallelConfig {
            num_threads: num_cpus::get(),
            chunk_size: None,
            schedule: Schedule::Static,
        }
    }
}

/// Parallel for loop
pub fn parallel_for<F>(start: usize, end: usize, config: &ParallelConfig, f: F)
where
    F: Fn(usize) + Sync + Send,
{
    if start >= end {
        return;
    }

    let range_size = end - start;
    let num_threads = config.num_threads.min(range_size);
    let chunk_size = config.chunk_size
        .unwrap_or((range_size + num_threads - 1) / num_threads);

    thread::scope(|s| {
        match config.schedule {
            Schedule::Static => {
                for t in 0..num_threads {
                    let f = &f;
                    s.spawn(move || {
                        let thread_start = start + t * chunk_size;
                        let thread_end = (thread_start + chunk_size).min(end);
                        for i in thread_start..thread_end {
                            f(i);
                        }
                    });
                }
            }
            Schedule::Dynamic => {
                todo!("Implementer avec work stealing")
            }
            Schedule::Guided => {
                todo!("Implementer avec chunks decroissants")
            }
        }
    });
}

/// Parallel for with index and thread id
pub fn parallel_for_with_id<F>(start: usize, end: usize, config: &ParallelConfig, f: F)
where
    F: Fn(usize, usize) + Sync + Send,  // (index, thread_id)
{
    todo!("Parallel for avec thread ID")
}

// ============================================================
// Map (Parallel transformation)
// ============================================================

pub fn parallel_map<T, U, F>(input: &[T], config: &ParallelConfig, f: F) -> Vec<U>
where
    T: Sync,
    U: Send + Default + Clone,
    F: Fn(&T) -> U + Sync + Send,
{
    let mut output: Vec<U> = vec![U::default(); input.len()];

    thread::scope(|s| {
        let chunks: Vec<_> = output.chunks_mut(
            (input.len() + config.num_threads - 1) / config.num_threads
        ).collect();

        for (thread_id, chunk) in chunks.into_iter().enumerate() {
            let start = thread_id * chunk.len();
            let f = &f;
            let input = &input;

            s.spawn(move || {
                for (i, out) in chunk.iter_mut().enumerate() {
                    *out = f(&input[start + i]);
                }
            });
        }
    });

    output
}

// ============================================================
// Reduce (Parallel aggregation)
// ============================================================

pub fn parallel_reduce<T, F, C>(
    input: &[T],
    identity: T,
    config: &ParallelConfig,
    combine: C,
) -> T
where
    T: Clone + Send + Sync,
    C: Fn(T, T) -> T + Sync + Send,
{
    if input.is_empty() {
        return identity;
    }

    let num_threads = config.num_threads.min(input.len());
    let chunk_size = (input.len() + num_threads - 1) / num_threads;

    let partial_results: Vec<T> = thread::scope(|s| {
        let handles: Vec<_> = (0..num_threads)
            .filter_map(|t| {
                let start = t * chunk_size;
                if start >= input.len() {
                    return None;
                }
                let end = (start + chunk_size).min(input.len());
                let chunk = &input[start..end];
                let id = identity.clone();
                let combine = &combine;

                Some(s.spawn(move || {
                    chunk.iter().cloned().fold(id, |acc, x| combine(acc, x))
                }))
            })
            .collect();

        handles.into_iter()
            .map(|h| h.join().unwrap())
            .collect()
    });

    // Reduire les resultats partiels
    partial_results.into_iter()
        .fold(identity, |acc, x| combine(acc, x))
}

// ============================================================
// Parallel Prefix (Scan)
// ============================================================

#[derive(Clone, Copy)]
pub enum ScanType {
    Inclusive,  // Inclut l'element courant
    Exclusive,  // Exclut l'element courant
}

pub fn parallel_scan<T, F>(
    input: &[T],
    identity: T,
    scan_type: ScanType,
    config: &ParallelConfig,
    combine: F,
) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T, &T) -> T + Sync + Send,
{
    if input.is_empty() {
        return vec![];
    }

    // Algorithme de Blelloch (work-efficient parallel scan)
    // Phase 1: Up-sweep (reduction)
    // Phase 2: Down-sweep (distribution)

    todo!("Implementer parallel prefix scan")
}

// ============================================================
// Pipeline
// ============================================================

pub struct Pipeline<I, O> {
    stages: Vec<Box<dyn Stage<I, O> + Send + Sync>>,
    buffer_size: usize,
}

pub trait Stage<I, O>: Send + Sync {
    fn process(&self, input: I) -> O;
}

impl<I, O> Pipeline<I, O>
where
    I: Send + 'static,
    O: Send + 'static,
{
    pub fn new(buffer_size: usize) -> Self {
        Pipeline {
            stages: vec![],
            buffer_size,
        }
    }

    pub fn add_stage<S>(&mut self, stage: S)
    where
        S: Stage<I, O> + 'static,
    {
        self.stages.push(Box::new(stage));
    }

    pub fn run(&self, inputs: Vec<I>) -> Vec<O> {
        todo!("Executer le pipeline en parallele")
    }
}

// ============================================================
// Future / Promise
// ============================================================

use std::sync::Condvar;

pub enum FutureState<T> {
    Pending,
    Ready(T),
    Failed(String),
}

pub struct Promise<T> {
    state: Arc<(Mutex<FutureState<T>>, Condvar)>,
}

pub struct Future<T> {
    state: Arc<(Mutex<FutureState<T>>, Condvar)>,
}

impl<T> Promise<T> {
    pub fn new() -> (Promise<T>, Future<T>) {
        let state = Arc::new((Mutex::new(FutureState::Pending), Condvar::new()));
        (
            Promise { state: Arc::clone(&state) },
            Future { state },
        )
    }

    pub fn set_value(self, value: T) {
        let (lock, cvar) = &*self.state;
        let mut state = lock.lock().unwrap();
        *state = FutureState::Ready(value);
        cvar.notify_all();
    }

    pub fn set_error(self, error: String) {
        let (lock, cvar) = &*self.state;
        let mut state = lock.lock().unwrap();
        *state = FutureState::Failed(error);
        cvar.notify_all();
    }
}

impl<T: Clone> Future<T> {
    pub fn get(&self) -> Result<T, String> {
        let (lock, cvar) = &*self.state;
        let mut state = lock.lock().unwrap();

        while matches!(*state, FutureState::Pending) {
            state = cvar.wait(state).unwrap();
        }

        match &*state {
            FutureState::Ready(v) => Ok(v.clone()),
            FutureState::Failed(e) => Err(e.clone()),
            FutureState::Pending => unreachable!(),
        }
    }

    pub fn is_ready(&self) -> bool {
        let (lock, _) = &*self.state;
        let state = lock.lock().unwrap();
        !matches!(*state, FutureState::Pending)
    }

    pub fn wait_for(&self, timeout: std::time::Duration) -> bool {
        todo!("Attendre avec timeout")
    }
}

// ============================================================
// Async Executor
// ============================================================

pub struct AsyncExecutor {
    thread_pool: Vec<thread::JoinHandle<()>>,
    task_queue: Arc<Mutex<Vec<Box<dyn FnOnce() + Send>>>>,
    condvar: Arc<Condvar>,  // Pour signaler les workers
    shutdown: Arc<std::sync::atomic::AtomicBool>,
}

impl AsyncExecutor {
    pub fn new(num_threads: usize) -> Self {
        todo!("Creer un executor avec thread pool")
    }

    /// Soumettre une tache asynchrone
    pub fn spawn<F, T>(&self, task: F) -> Future<T>
    where
        F: FnOnce() -> T + Send + 'static,
        T: Send + Clone + 'static,
    {
        let (promise, future) = Promise::new();

        let boxed_task: Box<dyn FnOnce() + Send> = Box::new(move || {
            let result = task();
            promise.set_value(result);
        });

        self.task_queue.lock().unwrap().push(boxed_task);
        self.condvar.notify_one();  // Reveiller un worker

        future
    }

    /// Executer plusieurs taches et attendre toutes
    pub fn spawn_all<F, T>(&self, tasks: Vec<F>) -> Vec<Future<T>>
    where
        F: FnOnce() -> T + Send + 'static,
        T: Send + Clone + 'static,
    {
        tasks.into_iter()
            .map(|t| self.spawn(t))
            .collect()
    }

    /// Attendre que toutes les taches soient terminees
    pub fn wait_all<T: Clone>(&self, futures: &[Future<T>]) -> Vec<Result<T, String>> {
        futures.iter()
            .map(|f| f.get())
            .collect()
    }

    pub fn shutdown(&self) {
        self.shutdown.store(true, std::sync::atomic::Ordering::SeqCst);
        self.condvar.notify_all();  // Reveiller tous les workers pour terminer
    }
}

// ============================================================
// Utilities
// ============================================================

/// Retourne le nombre de CPUs
pub fn num_cpus() -> usize {
    std::thread::available_parallelism()
        .map(|p| p.get())
        .unwrap_or(1)
}

/// Diviser un range en chunks pour distribution
pub fn chunk_range(start: usize, end: usize, num_chunks: usize) -> Vec<(usize, usize)> {
    if start >= end || num_chunks == 0 {
        return vec![];
    }

    let total = end - start;
    let chunk_size = (total + num_chunks - 1) / num_chunks;

    (0..num_chunks)
        .map(|i| {
            let chunk_start = start + i * chunk_size;
            let chunk_end = (chunk_start + chunk_size).min(end);
            (chunk_start, chunk_end)
        })
        .filter(|(s, e)| s < e)
        .collect()
}
```

## Tests Automatises

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::time::{Duration, Instant};

    #[test]
    fn test_parallel_for() {
        let counter = AtomicUsize::new(0);
        let config = ParallelConfig::default();

        parallel_for(0, 1000, &config, |_| {
            counter.fetch_add(1, Ordering::Relaxed);
        });

        assert_eq!(counter.load(Ordering::Relaxed), 1000);
    }

    #[test]
    fn test_parallel_for_empty() {
        let counter = AtomicUsize::new(0);
        let config = ParallelConfig::default();

        parallel_for(0, 0, &config, |_| {
            counter.fetch_add(1, Ordering::Relaxed);
        });

        assert_eq!(counter.load(Ordering::Relaxed), 0);
    }

    #[test]
    fn test_parallel_map() {
        let input: Vec<i32> = (0..1000).collect();
        let config = ParallelConfig::default();

        let output = parallel_map(&input, &config, |x| x * 2);

        for (i, &v) in output.iter().enumerate() {
            assert_eq!(v, (i as i32) * 2);
        }
    }

    #[test]
    fn test_parallel_reduce() {
        let input: Vec<i32> = (1..=1000).collect();
        let config = ParallelConfig::default();

        let sum = parallel_reduce(&input, 0, &config, |a, b| a + b);

        assert_eq!(sum, 500500);  // n*(n+1)/2
    }

    #[test]
    fn test_parallel_reduce_empty() {
        let input: Vec<i32> = vec![];
        let config = ParallelConfig::default();

        let sum = parallel_reduce(&input, 42, &config, |a, b| a + b);

        assert_eq!(sum, 42);  // Identity
    }

    #[test]
    fn test_parallel_scan_inclusive() {
        let input: Vec<i32> = vec![1, 2, 3, 4, 5];
        let config = ParallelConfig { num_threads: 2, ..Default::default() };

        let result = parallel_scan(&input, 0, ScanType::Inclusive, &config, |a, b| a + b);

        assert_eq!(result, vec![1, 3, 6, 10, 15]);
    }

    #[test]
    fn test_parallel_scan_exclusive() {
        let input: Vec<i32> = vec![1, 2, 3, 4, 5];
        let config = ParallelConfig { num_threads: 2, ..Default::default() };

        let result = parallel_scan(&input, 0, ScanType::Exclusive, &config, |a, b| a + b);

        assert_eq!(result, vec![0, 1, 3, 6, 10]);
    }

    #[test]
    fn test_future_promise() {
        let (promise, future) = Promise::<i32>::new();

        thread::spawn(move || {
            thread::sleep(Duration::from_millis(10));
            promise.set_value(42);
        });

        let result = future.get().unwrap();
        assert_eq!(result, 42);
    }

    #[test]
    fn test_future_error() {
        let (promise, future) = Promise::<i32>::new();

        thread::spawn(move || {
            promise.set_error("oops".to_string());
        });

        let result = future.get();
        assert!(result.is_err());
    }

    #[test]
    fn test_async_executor() {
        let executor = AsyncExecutor::new(4);

        let futures: Vec<_> = (0..100)
            .map(|i| executor.spawn(move || i * 2))
            .collect();

        let results = executor.wait_all(&futures);

        for (i, r) in results.iter().enumerate() {
            assert_eq!(r.as_ref().unwrap(), &(i * 2));
        }

        executor.shutdown();
    }

    #[test]
    fn test_speedup() {
        let input: Vec<f64> = (0..1_000_000).map(|x| x as f64).collect();

        // Sequential
        let start = Instant::now();
        let _: Vec<f64> = input.iter().map(|x| x.sqrt()).collect();
        let seq_time = start.elapsed();

        // Parallel
        let config = ParallelConfig::default();
        let start = Instant::now();
        let _ = parallel_map(&input, &config, |x| x.sqrt());
        let par_time = start.elapsed();

        let speedup = seq_time.as_secs_f64() / par_time.as_secs_f64();
        println!("Speedup: {:.2}x", speedup);

        // Devrait avoir un speedup > 1 sur multi-core
        // (peut etre < 1 sur single core ou avec overhead)
    }

    #[test]
    fn test_chunk_range() {
        let chunks = chunk_range(0, 100, 4);
        assert_eq!(chunks.len(), 4);
        assert_eq!(chunks[0], (0, 25));
        assert_eq!(chunks[1], (25, 50));
        assert_eq!(chunks[2], (50, 75));
        assert_eq!(chunks[3], (75, 100));
    }

    #[test]
    fn test_chunk_range_uneven() {
        let chunks = chunk_range(0, 10, 3);
        assert_eq!(chunks.len(), 3);
        // Chunks: 0-4, 4-8, 8-10
        let total: usize = chunks.iter().map(|(s, e)| e - s).sum();
        assert_eq!(total, 10);
    }
}
```

## Criteres d'evaluation
- [ ] parallel_for avec static scheduling
- [ ] parallel_map preserve l'ordre
- [ ] parallel_reduce avec identity correcte
- [ ] parallel_scan (inclusive et exclusive)
- [ ] Future/Promise fonctionnels
- [ ] AsyncExecutor avec thread pool
- [ ] Speedup mesurable sur multi-core
- [ ] Gestion correcte des edge cases

## Note qualite: 96/100
